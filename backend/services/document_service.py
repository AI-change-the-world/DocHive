import json
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any, AsyncGenerator, BinaryIO, Dict, List, Optional

from loguru import logger
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from typing_extensions import deprecated

from models.database_models import (
    ClassTemplate,
    ClassTemplateConfigs,
    Document,
    DocumentType,
    DocumentTypeField,
    TemplateDocumentMapping,
)
from schemas.api_schemas import DocumentCreate, DocumentUpdate, SSEEvent
from utils.llm_client import LLMClient
from utils.parser import DocumentParser
from utils.storage import StorageClient

EXTRACT_FIELES_PROMPT = """
ä½ æ˜¯ä¸€åä¿¡æ¯æŠ½å–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–æŒ‡å®šå­—æ®µçš„ä¿¡æ¯ï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºã€‚

ã€å­—æ®µå®šä¹‰ã€‘
{{field_definitions}}

æ¯ä¸ªå­—æ®µåŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š
- field_nameï¼šå­—æ®µåï¼ˆä½œä¸º JSON çš„é”®ï¼‰
- descriptionï¼šå­—æ®µå«ä¹‰æˆ–æå–è¯´æ˜
- field_typeï¼šå­—æ®µç±»å‹ï¼ˆå¯ä¸º text / date / arrayï¼‰

ã€è¾“å‡ºè¦æ±‚ã€‘
1. è¾“å‡ºä¸€ä¸ªå®Œæ•´ JSONï¼Œé”®åä¸ field_name å¯¹åº”ã€‚
2. å¦‚æœæŸä¸ªå­—æ®µæ— æ³•ç¡®å®šå†…å®¹ï¼Œè¯·è¿”å› nullã€‚
3. å„å­—æ®µå¤„ç†è§„èŒƒï¼š
   - textï¼šæå–æ–‡ä¸­å¯¹åº”çš„æ–‡å­—å†…å®¹ã€‚
   - dateï¼šè¯†åˆ«å¹¶è½¬æ¢ä¸º YYYY-MM-DD æ ¼å¼ã€‚
   - arrayï¼šæå–å¤šä¸ªç›¸å…³é¡¹ï¼Œä»¥å­—ç¬¦ä¸²æ•°ç»„å½¢å¼è¿”å›ã€‚
4. ä¸è¦ç”Ÿæˆå¤šä½™è§£é‡Šæˆ–è¯´æ˜ï¼Œåªè¾“å‡º JSONã€‚

ã€ç¤ºä¾‹è¾“å‡ºã€‘
```
{
  "æ ‡é¢˜": "å…³äºæ¨è¿›æ•°å­—æ”¿åŠ¡å»ºè®¾çš„è‹¥å¹²æ„è§",
  "å‘æ–‡å•ä½": "å›½åŠ¡é™¢åŠå…¬å…",
  "å‘æ–‡å­—å·": "å›½åŠå‘ã€”2023ã€•12å·",
  "å‘å¸ƒæ—¥æœŸ": "2023-05-12"
}

ã€å¾…æå–æ–‡æ¡£å†…å®¹ã€‘
{{document_content}}
"""


CODE_EXTRACTION_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ†æåŠ©ç†ï¼Œç”¨äºä»æ–‡æ¡£ä¸­æå–ä¸šåŠ¡ç¼–ç ä¿¡æ¯ã€‚è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹ä¸šåŠ¡ç¼–ç é…ç½®ï¼š

JSON é…ç½®ï¼š
{{JSON_CONFIG}}

è¯´æ˜ï¼š
1. level è¡¨ç¤ºç¼–ç å±‚çº§ï¼Œ1 è¡¨ç¤ºä¸€çº§ç¼–ç ï¼Œ2 è¡¨ç¤ºäºŒçº§ç¼–ç ã€‚
2. name æ˜¯ç¼–ç å­—æ®µåç§°ï¼Œdescription æ˜¯å¯¹è¯¥å­—æ®µçš„ç®€çŸ­æè¿°ã€‚
3. code æ˜¯ç¼–ç æ ‡è¯†ã€‚
4. extraction_promptï¼ˆå¦‚æœæœ‰ï¼‰æä¾›äº†å¯èƒ½å€¼æˆ–åŒ¹é…æç¤ºã€‚
5. å¦‚æœ extraction_prompt ä¸º nullï¼Œè¯·æ ¹æ®æ–‡æœ¬å†…å®¹ç›´æ¥æå–å¯¹åº”å€¼ã€‚

è¯·ä½ ç”Ÿæˆä¸€ä¸ªä¼˜åŒ–åçš„æå–ç¼–ç çš„æŒ‡ä»¤æ¨¡æ¿ï¼ˆpromptï¼‰ï¼Œè¦æ±‚ï¼š
- èƒ½æ˜ç¡®å‘Šè¯‰æ¨¡å‹è¦æå–å“ªäº›å­—æ®µã€‚
- å¯¹æ¯ä¸ªå­—æ®µæä¾›æå–è§„åˆ™æˆ–æç¤ºã€‚
- è¾“å‡ºæ ¼å¼ä¸º JSON åˆ—è¡¨ï¼Œç¤ºä¾‹ï¼š
[
  {"code":"YEAR", "value":"2025", "level":1},
  {"code":"REGION", "value":"JS", "level":2}
]
- é‡åˆ°æ— æ³•æå–çš„å­—æ®µå¯ä»¥è¿”å› nullã€‚
- ä¸è¦æ·»åŠ å¤šä½™è§£é‡Šï¼Œç›´æ¥ç”Ÿæˆå¯ä»¥ç›´æ¥ç”¨äºè°ƒç”¨æ¨¡å‹çš„ promptã€‚

"""

TYPE_CLASSIFICATION_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªæ”¿åºœå…¬æ–‡æ™ºèƒ½åˆ†ç±»åŠ©æ‰‹ï¼Œè¯·æ ¹æ®æ–‡æ¡£å†…å®¹åˆ¤æ–­å…¶æ‰€å±çš„æ–‡æ¡£ç±»å‹ã€‚

ä»¥ä¸‹æ˜¯æ–‡æ¡£ç±»å‹å®šä¹‰è¡¨ï¼ˆtype_codeã€type_nameã€descriptionï¼‰ï¼š

{{type_code}}

è¯·é˜…è¯»ä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œåˆ¤æ–­è¯¥æ–‡æ¡£æœ€ç¬¦åˆçš„ç±»å‹ï¼Œå¹¶è¾“å‡ºç»“æœã€‚

è¦æ±‚ï¼š
1. åªèƒ½é€‰æ‹©ä¸€ä¸ªæœ€åˆé€‚çš„ç±»å‹ã€‚
2. è¾“å‡ºæ ¼å¼ä¸º JSONï¼š
{
  "type_code": "XXX",
  "type_name": "XXX",
  "reason": "ç®€è¦è¯´æ˜åˆ¤æ–­ä¾æ®"
}

ç¤ºä¾‹è¾“å…¥ï¼š
ã€Šå…³äºå°å‘ã€ˆå¸‚ç§‘æŠ€åˆ›æ–°å‘å±•è§„åˆ’ï¼ˆ2025-2030ï¼‰ã€‰çš„é€šçŸ¥ã€‹

ç¤ºä¾‹è¾“å‡ºï¼š
{
  "type_code": "GH",
  "type_name": "è§„åˆ’æ–¹æ¡ˆ",
  "reason": "æ–‡ä¸­åŒ…å«â€œå‘å±•è§„åˆ’â€ï¼Œå±äºè®¡åˆ’ç±»æ–‡ä»¶"
}

ç°åœ¨è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æ¡£çš„ç±»å‹ï¼š

{{doc}}
"""


class DocumentService:
    """æ–‡æ¡£æœåŠ¡å±‚"""

    @staticmethod
    async def upload_file_stream(
        db: AsyncSession,
        llm_client: LLMClient,
        file_data: BinaryIO,
        filename: str,
        document_data: DocumentCreate,
        user_id: int,
    ) -> AsyncGenerator[str, Any]:
        """
        ä¸Šä¼ å¹¶è§£ææ–‡æ¡£ï¼ˆæµå¼å¤„ç†ï¼‰
        """

        event = SSEEvent(
            event="process document content", data=None, id=None, done=False
        )

        file_extension = Path(filename).suffix
        object_name = f"{uuid.uuid4()}{file_extension}"

        # 1ï¸âƒ£ è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆåªè¯»ä¸€æ¬¡ï¼‰
        file_bytes = file_data.read()
        if hasattr(file_data, "seek"):
            file_data.seek(0)

        # 2ï¸âƒ£ TODO æ¨¡æ‹Ÿä¸Šä¼ ï¼ˆæš‚æ—¶æ²¡æœ‰å®ç°ä¸Šä¼ åˆ°s3çš„é€»è¾‘ï¼‰
        file_path = f"{object_name}"
        event.data = "[info] ä¸Šä¼ æ–‡ä»¶æˆåŠŸ"
        yield event.model_dump_json(ensure_ascii=False)

        # 3ï¸âƒ£ è§£ææ–‡æœ¬å†…å®¹
        doc = await DocumentParser.parse_file(file_bytes, file_extension)

        # 4ï¸âƒ£ è·å–æ¨¡æ¿
        template_id = document_data.template_id
        result = await db.execute(
            select(ClassTemplate).where(ClassTemplate.id == template_id)
        )
        template = result.scalar_one_or_none()

        if not template:
            event.done = True
            event.data = "[error] æ¨¡æ¿ä¸å­˜åœ¨"
            yield event.model_dump_json(ensure_ascii=False)
            return

        # 5ï¸âƒ£ è·å–æ–‡æ¡£ç±»å‹
        doc_type_result = await db.execute(
            select(DocumentType).where(DocumentType.template_id == template_id)
        )
        doc_types = doc_type_result.scalars().all()

        if not doc_types:
            event.done = True
            event.data = "[error] æ–‡æ¡£ç±»å‹ä¸å­˜åœ¨"
            yield event.model_dump_json(ensure_ascii=False)
            return

        # 6ï¸âƒ£ è·å–æ¨¡æ¿çš„å±‚çº§å®šä¹‰
        template_json_list: List[Dict[str, Any]] = getattr(template, "levels") or []

        # 7ï¸âƒ£ æ£€æŸ¥å¹¶ç”Ÿæˆç¼–ç æå–æç¤º
        class_template_config_result = await db.execute(
            select(ClassTemplateConfigs).where(
                ClassTemplateConfigs.template_id == template_id,
                ClassTemplateConfigs.config_name == "code_extraction_prompt",
            )
        )
        class_template_config = class_template_config_result.scalar_one_or_none()

        type_level = -1
        new_list = []
        for i in template_json_list:
            if i.get("is_doc_type", False):
                type_level = i.get("level", -1)
                continue
            new_list.append(i)

        if class_template_config:
            code_prompt = class_template_config.config_value
            event.data = "[info] ä½¿ç”¨è‡ªå®šä¹‰çš„ç¼–ç æå–æç¤º"
            yield event.model_dump_json(ensure_ascii=False)
        else:
            event.data = "[info] é‡æ–°æ„é€ ç¼–ç æå–æç¤º"
            yield event.model_dump_json(ensure_ascii=False)

            prompt = CODE_EXTRACTION_PROMPT.replace(
                "{{JSON_CONFIG}}", json.dumps(new_list, ensure_ascii=False)
            )
            code_prompt = await llm_client.chat_completion(prompt, db=db)

            # ä¿å­˜é…ç½®
            new_config = ClassTemplateConfigs(
                template_id=template_id,
                config_name="code_extraction_prompt",
                config_value=code_prompt,
            )
            db.add(new_config)
            await db.commit()

        # 8ï¸âƒ£ æå–ç¼–ç ç»“æœ
        # æ„é€ ä¸€ä¸ªåˆé€‚çš„æç¤ºæ¶ˆæ¯
        prompt_message = (
            str(code_prompt) + "\n\nä»¥ä¸‹ä¸ºæ–‡æ¡£å†…å®¹ï¼Œè¯·å¸®æˆ‘æå–ï¼š" + str(doc)
        )
        code_json_result = await llm_client.extract_json_response(
            prompt_message,
            db=db,
        )
        # ç¡®ä¿code_jsonæ˜¯ä¸€ä¸ªåˆ—è¡¨
        if isinstance(code_json_result, dict):
            code_json: List[Dict[str, Any]] = [code_json_result]
        elif isinstance(code_json_result, list):
            code_json = code_json_result
        else:
            code_json = []

        logger.info("ğŸ‘“ï¸ ç¼–ç ç»“æœï¼š" + str(code_json))
        event.data = f"[info] æå–ç¼–ç ç»“æœï¼š {code_json}"
        yield event.model_dump_json(ensure_ascii=False)

        # 9ï¸âƒ£ æå–æ–‡æ¡£ç±»å‹
        type_list = [
            {
                "type_code": getattr(i, "type_code"),
                "type_name": getattr(i, "type_name"),
                "description": getattr(i, "description"),
            }
            for i in doc_types
        ]

        type_prompt = TYPE_CLASSIFICATION_PROMPT.replace(
            "{{type_code}}", json.dumps(type_list, ensure_ascii=False)
        ).replace("{{doc}}", doc)
        type_json = await llm_client.extract_json_response(type_prompt, db=db)
        logger.info("ğŸ©± æ–‡æ¡£ç±»å‹ï¼š" + str(type_json))
        event.data = f"[info] æ–‡æ¡£ç±»å‹ï¼š {type_json}"
        yield event.model_dump_json(ensure_ascii=False)

        # 10ï¸âƒ£ åˆå¹¶ç¼–ç å’Œåˆ†ç±»ç»“æœ
        type_value = (
            type_json.get("type_code", "UNKNOWN")
            if isinstance(type_json, dict)
            else "UNKNOWN"
        )
        type_json_into_code_json = {
            "code": "TYPE",
            "value": type_value,
            "level": type_level,
        }

        code_json.append(type_json_into_code_json)
        # ç¡®ä¿åˆ—è¡¨ä¸­çš„å…ƒç´ æ˜¯å­—å…¸ç±»å‹
        dict_items = [item for item in code_json if isinstance(item, dict)]
        sorted_code_json = sorted(
            dict_items, key=lambda x: x.get("level", 0) if isinstance(x, dict) else 0
        )

        logger.info(
            "âœ… åˆå¹¶ç¼–ç å’Œåˆ†ç±»ç»“æœï¼š "
            + json.dumps(sorted_code_json, ensure_ascii=False)
        )

        # 11ï¸âƒ£ è·å–å¯¹åº” DocumentType
        type_code = (
            type_json.get("type_code", "UNKNOWN")
            if isinstance(type_json, dict)
            else "UNKNOWN"
        )
        doc_type_result = await db.execute(
            select(DocumentType).where(
                DocumentType.type_code == type_code,
                DocumentType.template_id == template_id,
            )
        )
        doc_type = doc_type_result.scalar_one_or_none()

        # 12ï¸âƒ£ æ„é€ æ–‡ä»¶ç¼–ç  TODO æœ‰æ—¶å€™Sectoræ— æ³•æ­£ç¡®è¯†åˆ«ï¼Œéœ€è¦å¤„ç†
        file_code_id_prefix = "-".join(
            (
                str(i.get("value"))
                if isinstance(i, dict) and i.get("value") is not None
                else "UNKNOWN"
            )
            for i in sorted_code_json
        )
        logger.info("âœ… ç¼–ç å‰ç¼€ï¼š" + file_code_id_prefix)
        event.data = f"[info] ç¼–ç å‰ç¼€ï¼š {file_code_id_prefix}"
        yield event.model_dump_json(ensure_ascii=False)

        # ç”Ÿæˆæ•°å­—åºå·ï¼šæŸ¥è¯¢è¯¥å‰ç¼€ä¸‹çš„æœ€å¤§åºå·
        event.data = "[info] ç”Ÿæˆæ–‡æ¡£åºå·..."
        yield event.model_dump_json(ensure_ascii=False)

        # æŸ¥è¯¢è¯¥æ¨¡æ¿ä¸‹æ‰€æœ‰ä»¥è¯¥å‰ç¼€å¼€å¤´çš„ç¼–ç 
        result = await db.execute(
            select(TemplateDocumentMapping.class_code).where(
                TemplateDocumentMapping.template_id == document_data.template_id,
                TemplateDocumentMapping.class_code.like(f"{file_code_id_prefix}-%"),
            )
        )
        existing_codes = result.scalars().all()

        # æå–æ‰€æœ‰æ•°å­—åºå·ï¼ˆå…¼å®¹UUIDæ ¼å¼ï¼‰
        max_seq = 0
        for code in existing_codes:
            if code:
                # æå–æœ€åä¸€æ®µï¼ˆåºå·éƒ¨åˆ†ï¼‰
                parts = code.split("-")
                if parts:
                    last_part = parts[-1]
                    # å°è¯•è§£æä¸ºæ•°å­—ï¼Œå¦‚æœæ˜¯UUIDåˆ™è·³è¿‡
                    try:
                        seq = int(last_part)
                        max_seq = max(max_seq, seq)
                    except ValueError:
                        # UUIDæ ¼å¼ï¼Œå¿½ç•¥
                        pass

        # æ–°åºå· = æœ€å¤§åºå· + 1ï¼ˆä¸é™é•¿åº¦ï¼Œè‡ªåŠ¨æ‰©å±•ï¼‰
        next_seq = max_seq + 1
        final_code_id = f"{file_code_id_prefix}-{next_seq}"

        logger.info(f"âœ… æœ€ç»ˆç¼–ç ï¼š{final_code_id} (åºå·: {next_seq})")
        event.data = f"[info] æœ€ç»ˆç¼–ç ï¼š {final_code_id}"
        yield event.model_dump_json(ensure_ascii=False)

        # 13ï¸âƒ£ æŸ¥è¯¢ç±»å‹å­—æ®µå®šä¹‰
        doc_type_fields_result = await db.execute(
            select(DocumentTypeField).where(
                DocumentTypeField.doc_type_id == (doc_type.id if doc_type else None)
            )
        )
        doc_type_fields = doc_type_fields_result.scalars().all()

        _extracted_data = {}

        if not doc_type_fields:
            event.data = "[info] æ–‡æ¡£ç±»å‹å­—æ®µä¸å­˜åœ¨,ä¸æå–å†…å®¹"
            yield event.model_dump_json(ensure_ascii=False)
        else:
            event.data = "[info] æ–‡æ¡£ç±»å‹å­—æ®µå­˜åœ¨ï¼Œå¼€å§‹æå–å†…å®¹"
            yield event.model_dump_json(ensure_ascii=False)

            _fields = [i.to_dict() for i in doc_type_fields]
            field_definitions = "\n".join(
                f"{i+1}. {f['field_name']}ï¼ˆ{f['field_type']}ï¼‰ï¼š{f['description']}"
                for i, f in enumerate(_fields)
            )
            prompt = EXTRACT_FIELES_PROMPT.replace(
                "{{field_definitions}}", field_definitions
            ).replace("{{document_content}}", doc)
            _extracted_data = await llm_client.extract_json_response(prompt, db=db)

        # 14ï¸âƒ£ ä¿å­˜æ–‡æ¡£ä¿¡æ¯
        document = Document(
            title=document_data.title,
            original_filename=filename,
            file_path=file_path,
            file_type=file_extension.lstrip("."),
            file_size=len(file_bytes),
            template_id=document_data.template_id,
            doc_metadata=document_data.metadata or {},
            uploader_id=user_id,
            content_text=doc,
            doc_type_id=doc_type.id if doc_type else 0,
        )

        db.add(document)
        await db.flush()  # è·å–æ–‡æ¡£ID

        # åˆ›å»ºæ¨¡æ¿å’Œæ–‡æ¡£çš„æ˜ å°„è®°å½•
        mapping = TemplateDocumentMapping(
            template_id=document_data.template_id,
            document_id=document.id,
            class_code=final_code_id,
            status="completed",
            processed_time=int(time.time()),
            extracted_data=(
                json.dumps(_extracted_data, ensure_ascii=False)
                if _extracted_data
                else None
            ),
        )
        db.add(mapping)

        await db.commit()

        # å°†æ–‡æ¡£ç´¢å¼•åˆ°Elasticsearch
        try:
            from utils.search_engine import get_search_client

            search_client = get_search_client()

            # è·å–upload_timeçš„å€¼
            upload_time = getattr(document, "upload_time", None)

            document_data_for_es = {
                "document_id": document.id,
                "title": document.title,
                "content": doc,
                "summary": doc[:500] if len(doc) > 500 else doc,
                "template_id": document.template_id,
                "file_type": document.file_type,
                "upload_time": (
                    datetime.fromtimestamp(upload_time).isoformat()
                    if upload_time
                    else None
                ),
                "metadata": _extracted_data,  # å°†extracted_dataå­˜å‚¨åœ¨metadataå­—æ®µä¸­
            }
            await search_client.index_document(document_data_for_es)
            logger.info(f"æ–‡æ¡£ {document.id} å·²æˆåŠŸç´¢å¼•åˆ°Elasticsearch")
        except Exception as e:
            logger.error(f"æ–‡æ¡£ {document.id} ç´¢å¼•åˆ°Elasticsearchå¤±è´¥: {e}")

        event.data = "[info] æ–‡æ¡£åˆ›å»ºæˆåŠŸ"
        event.done = True
        yield event.model_dump_json(ensure_ascii=False)

    @deprecated("ä½¿ç”¨upload_file_streamä»£æ›¿")
    @staticmethod
    async def upload_document(
        db: AsyncSession,
        storage_client: StorageClient,
        file_data: BinaryIO,
        filename: str,
        document_data: DocumentCreate,
        user_id: int,
    ) -> Document:
        """
        ä¸Šä¼ å¹¶è§£ææ–‡æ¡£

        Args:
            db: æ•°æ®åº“ä¼šè¯
            storage_client: å­˜å‚¨å®¢æˆ·ç«¯
            file_data: æ–‡ä»¶æ•°æ®æµ
            filename: åŸå§‹æ–‡ä»¶å
            document_data: æ–‡æ¡£åˆ›å»ºæ•°æ®
            user_id: ä¸Šä¼ ç”¨æˆ·ID

        Returns:
            åˆ›å»ºçš„æ–‡æ¡£è®°å½•
        """
        # è·å–æ–‡ä»¶æ‰©å±•å
        file_extension = Path(filename).suffix

        # ç”Ÿæˆå”¯ä¸€å¯¹è±¡å
        import datetime

        object_name = f"{datetime.datetime.utcnow().strftime('%Y/%m/%d')}/{uuid.uuid4()}{file_extension}"

        # è¯»å–æ–‡ä»¶æ•°æ®
        file_bytes = file_data.read()
        file_data.seek(0)

        # ä¸Šä¼ åˆ°å¯¹è±¡å­˜å‚¨
        file_path = await storage_client.upload_file(
            file_data,
            object_name,
            content_type=DocumentService._get_content_type(file_extension),
        )

        # åˆ›å»ºæ–‡æ¡£è®°å½•
        document = Document(
            title=document_data.title,
            original_filename=filename,
            file_path=file_path,
            file_type=file_extension.lstrip("."),
            file_size=len(file_bytes),
            template_id=document_data.template_id,
            doc_metadata=document_data.metadata or {},
            uploader_id=user_id,
        )

        db.add(document)
        await db.commit()
        await db.refresh(document)

        # å¼‚æ­¥è§£ææ–‡æ¡£ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨ Celery ä»»åŠ¡é˜Ÿåˆ—ï¼‰
        # REPLACE: æµå¼æ¥å£æ›´å¥½
        try:
            await DocumentService.parse_document(
                db, int(getattr(document, "id")), file_bytes, file_extension
            )
        except Exception as e:
            # æ›´æ–°æ˜ å°„è¡¨ä¸­çš„é”™è¯¯ä¿¡æ¯
            result = await db.execute(
                select(TemplateDocumentMapping).where(
                    TemplateDocumentMapping.document_id == getattr(document, "id")
                )
            )
            mapping = result.scalar_one_or_none()
            if mapping:
                setattr(mapping, "status", "failed")
                setattr(mapping, "error_message", str(e))
                await db.commit()
            else:
                # å¦‚æœæ˜ å°„è¡¨è®°å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
                mapping = TemplateDocumentMapping(
                    template_id=document_data.template_id,
                    document_id=getattr(document, "id"),
                    status="failed",
                    error_message=str(e),
                )
                db.add(mapping)
                await db.commit()

        return document

    @deprecated("å·²å¼ƒç”¨")
    @staticmethod
    async def parse_document(
        db: AsyncSession,
        document_id: int,
        file_data: bytes,
        file_extension: str,
    ):
        """è§£ææ–‡æ¡£å†…å®¹"""
        document = await DocumentService.get_document(db, document_id)
        if not document:
            return

        # æ›´æ–°æ˜ å°„è¡¨çŠ¶æ€ä¸ºå¤„ç†ä¸­
        result = await db.execute(
            select(TemplateDocumentMapping).where(
                TemplateDocumentMapping.document_id == document_id
            )
        )
        mapping = result.scalar_one_or_none()
        if mapping:
            setattr(mapping, "status", "processing")
            await db.commit()

        try:
            # è§£ææ–‡æœ¬å†…å®¹
            content_text = await DocumentParser.parse_file(file_data, file_extension)

            # æå–å…ƒä¿¡æ¯
            metadata = DocumentParser.extract_metadata(file_data, file_extension)

            # ç”Ÿæˆæ‘˜è¦ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è°ƒç”¨ LLMï¼‰
            summary = content_text[:500] if len(content_text) > 500 else content_text

            # æ›´æ–°æ–‡æ¡£
            setattr(document, "content_text", content_text)
            setattr(document, "summary", summary)
            # åˆå¹¶ doc_metadata
            current_metadata = getattr(document, "doc_metadata") or {}
            current_metadata.update(metadata)
            setattr(document, "doc_metadata", current_metadata)

            await db.commit()

            # æ›´æ–°æ˜ å°„è¡¨çŠ¶æ€ä¸ºå®Œæˆ
            if mapping:
                setattr(mapping, "status", "completed")
                setattr(mapping, "processed_time", int(time.time()))
                await db.commit()

        except Exception as e:
            # æ›´æ–°æ˜ å°„è¡¨ä¸­çš„é”™è¯¯ä¿¡æ¯
            if mapping:
                setattr(mapping, "status", "failed")
                setattr(mapping, "error_message", str(e))
                await db.commit()
            raise

    @staticmethod
    async def get_document(db: AsyncSession, document_id: int) -> Optional[Document]:
        """è·å–æ–‡æ¡£"""
        result = await db.execute(select(Document).where(Document.id == document_id))
        return result.scalar_one_or_none()

    @staticmethod
    async def list_documents(
        db: AsyncSession,
        skip: int = 0,
        limit: int = 20,
        template_id: Optional[int] = None,
        status: Optional[str] = None,
        user_id: Optional[int] = None,
    ) -> tuple[list[Document], int]:
        """è·å–æ–‡æ¡£åˆ—è¡¨"""
        query = select(Document)
        count_query = select(Document)

        if template_id:
            query = query.where(Document.template_id == template_id)
            count_query = count_query.where(Document.template_id == template_id)

        if status:
            query = query.where(Document.status == status)
            count_query = count_query.where(Document.status == status)

        if user_id:
            query = query.where(Document.uploader_id == user_id)
            count_query = count_query.where(Document.uploader_id == user_id)

        query = query.order_by(Document.upload_time.desc()).offset(skip).limit(limit)

        result = await db.execute(query)
        documents = result.scalars().all()

        count_result = await db.execute(count_query)
        total = len(count_result.scalars().all())

        return list(documents), total

    @staticmethod
    async def update_document(
        db: AsyncSession,
        document_id: int,
        document_data: DocumentUpdate,
    ) -> Optional[Document]:
        """æ›´æ–°æ–‡æ¡£"""
        document = await DocumentService.get_document(db, document_id)
        if not document:
            return None

        update_data = document_data.model_dump(exclude_unset=True)

        for field, value in update_data.items():
            setattr(document, field, value)

        await db.commit()
        await db.refresh(document)
        return document

    @staticmethod
    async def delete_document(
        db: AsyncSession, storage_client: StorageClient, document_id: int
    ) -> bool:
        """åˆ é™¤æ–‡æ¡£"""
        document = await DocumentService.get_document(db, document_id)
        if not document:
            return False

        # ä»å¯¹è±¡å­˜å‚¨åˆ é™¤æ–‡ä»¶
        file_path = getattr(document, "file_path")
        object_name = file_path.split("/", 1)[1] if "/" in file_path else file_path
        await storage_client.delete_file(object_name)

        # ä»æ•°æ®åº“åˆ é™¤
        await db.delete(document)
        await db.commit()
        return True

    @staticmethod
    async def get_download_url(
        db: AsyncSession, storage_client: StorageClient, document_id: int
    ) -> Optional[str]:
        """è·å–æ–‡æ¡£ä¸‹è½½é“¾æ¥"""
        document = await DocumentService.get_document(db, document_id)
        if not document:
            return None

        # æå–å¯¹è±¡å
        file_path = getattr(document, "file_path")
        object_name = file_path.split("/", 1)[1] if "/" in file_path else file_path

        return storage_client.get_presigned_url(object_name)

    @staticmethod
    async def create_document_manually(
        db: AsyncSession,
        llm_client: LLMClient,
        file_data: BinaryIO,
        filename: str,
        title: Optional[str],
        template_id: int,
        doc_type_id: int,
        class_code: str,
        user_id: int,
    ) -> AsyncGenerator[str, Any]:
        """
        æ‰‹åŠ¨åˆ›å»ºæ–‡æ¡£ï¼ˆæµå¼å¤„ç†ï¼Œç”¨æˆ·æŒ‡å®šåˆ†ç±»ä¿¡æ¯ï¼‰

        Args:
            db: æ•°æ®åº“ä¼šè¯
            llm_client: LLMå®¢æˆ·ç«¯
            file_data: æ–‡ä»¶æ•°æ®æµ
            filename: åŸå§‹æ–‡ä»¶å
            title: æ–‡æ¡£æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œä¸ºNoneåˆ™ä»æ–‡æ¡£å†…å®¹ä¸­æå–ï¼‰
            template_id: æ¨¡æ¿ID
            doc_type_id: æ–‡æ¡£ç±»å‹ID
            class_code: åˆ†ç±»ç¼–ç ï¼ˆç”¨æˆ·æ‰‹åŠ¨æŒ‡å®šï¼‰
            user_id: ä¸Šä¼ ç”¨æˆ·ID

        Yields:
            SSEäº‹ä»¶æµ
        """
        _id = str(uuid.uuid4())

        event = SSEEvent(
            event="create document manually", data=None, id=_id, done=False
        )

        file_extension = Path(filename).suffix
        object_name = f"{uuid.uuid4()}{file_extension}"

        # 1ï¸âƒ£ è¯»å–æ–‡ä»¶å†…å®¹
        file_bytes = file_data.read()
        if hasattr(file_data, "seek"):
            file_data.seek(0)

        # 2ï¸âƒ£ æ¨¡æ‹Ÿä¸Šä¼ ï¼ˆæš‚æ—¶æ²¡æœ‰å®ç°ä¸Šä¼ åˆ°s3çš„é€»è¾‘ï¼‰
        file_path = f"{object_name}"
        event.data = "[info] ä¸Šä¼ æ–‡ä»¶æˆåŠŸ"
        yield event.model_dump_json(ensure_ascii=False)

        # 3ï¸âƒ£ è§£ææ–‡æœ¬å†…å®¹
        event.data = "[info] è§£ææ–‡æ¡£å†…å®¹ä¸­..."
        yield event.model_dump_json(ensure_ascii=False)
        doc = await DocumentParser.parse_file(file_bytes, file_extension)

        # 4ï¸âƒ£ å¦‚æœæ²¡æœ‰æä¾›æ ‡é¢˜ï¼Œä½¿ç”¨æ–‡ä»¶å
        if not title:
            # å»æ‰æ–‡ä»¶æ‰©å±•åä½œä¸ºæ ‡é¢˜
            title = filename.rsplit(".", 1)[0] if "." in filename else filename
            event.data = f"[info] ä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ ‡é¢˜: {title}"
            yield event.model_dump_json(ensure_ascii=False)

        # 5ï¸âƒ£ ä¸ºåˆ†ç±»ç¼–ç è¡¥å……æ•°å­—åºå·
        event.data = "[info] ç”Ÿæˆæ–‡æ¡£åºå·..."
        yield event.model_dump_json(ensure_ascii=False)

        # æŸ¥è¯¢è¯¥æ¨¡æ¿ä¸‹æ‰€æœ‰ä»¥è¯¥å‰ç¼€å¼€å¤´çš„ç¼–ç 
        result = await db.execute(
            select(TemplateDocumentMapping.class_code).where(
                TemplateDocumentMapping.template_id == template_id,
                TemplateDocumentMapping.class_code.like(f"{class_code}-%"),
            )
        )
        existing_codes = result.scalars().all()

        # æå–æ‰€æœ‰æ•°å­—åºå·ï¼ˆå…¼å®¹UUIDæ ¼å¼ï¼‰
        max_seq = 0
        for code in existing_codes:
            if code:
                # æå–æœ€åä¸€æ®µï¼ˆåºå·éƒ¨åˆ†ï¼‰
                parts = code.split("-")
                if parts:
                    last_part = parts[-1]
                    # å°è¯•è§£æä¸ºæ•°å­—ï¼Œå¦‚æœæ˜¯UUIDåˆ™è·³è¿‡
                    try:
                        seq = int(last_part)
                        max_seq = max(max_seq, seq)
                    except ValueError:
                        # UUIDæ ¼å¼ï¼Œå¿½ç•¥
                        pass

        # æ–°åºå· = æœ€å¤§åºå· + 1ï¼ˆä¸é™é•¿åº¦ï¼Œè‡ªåŠ¨æ‰©å±•ï¼‰
        next_seq = max_seq + 1
        final_class_code = f"{class_code}-{next_seq}"

        logger.info(f"âœ… æœ€ç»ˆç¼–ç ï¼š{final_class_code} (åºå·: {next_seq})")
        event.data = f"[info] æœ€ç»ˆç¼–ç ï¼š {final_class_code}"
        yield event.model_dump_json(ensure_ascii=False)

        # 6ï¸âƒ£ æŸ¥è¯¢æ–‡æ¡£ç±»å‹å­—æ®µå®šä¹‰
        event.data = "[info] è·å–æ–‡æ¡£ç±»å‹å­—æ®µé…ç½®..."
        yield event.model_dump_json(ensure_ascii=False)

        doc_type_fields_result = await db.execute(
            select(DocumentTypeField).where(
                DocumentTypeField.doc_type_id == doc_type_id
            )
        )
        doc_type_fields = doc_type_fields_result.scalars().all()

        _extracted_data = {}

        if not doc_type_fields:
            event.data = "[info] æ–‡æ¡£ç±»å‹å­—æ®µä¸å­˜åœ¨,ä¸æå–å†…å®¹"
            yield event.model_dump_json(ensure_ascii=False)
        else:
            event.data = "[info] å¼€å§‹ä½¿ç”¨AIæå–å­—æ®µä¿¡æ¯..."
            yield event.model_dump_json(ensure_ascii=False)

            _fields = [i.to_dict() for i in doc_type_fields]
            field_definitions = "\n".join(
                f"{i+1}. {f['field_name']}ï¼ˆ{f['field_type']}ï¼‰ï¼š{f['description']}"
                for i, f in enumerate(_fields)
            )
            prompt = EXTRACT_FIELES_PROMPT.replace(
                "{{field_definitions}}", field_definitions
            ).replace("{{document_content}}", doc)

            _extracted_data = await llm_client.extract_json_response(prompt, db=db)
            event.data = f"[info] å­—æ®µæå–å®Œæˆ: {json.dumps(_extracted_data, ensure_ascii=False)}"
            yield event.model_dump_json(ensure_ascii=False)

        # 6ï¸âƒ£ ä¿å­˜æ–‡æ¡£ä¿¡æ¯
        event.data = "[info] ä¿å­˜æ–‡æ¡£ä¿¡æ¯..."
        yield event.model_dump_json(ensure_ascii=False)

        document = Document(
            title=title,
            original_filename=filename,
            file_path=file_path,
            file_type=file_extension.lstrip("."),
            file_size=len(file_bytes),
            template_id=template_id,
            doc_metadata={},
            uploader_id=user_id,
            content_text=doc,
            doc_type_id=doc_type_id,
        )

        db.add(document)
        await db.flush()  # è·å–æ–‡æ¡£ID

        # 7ï¸âƒ£ åˆ›å»ºæ¨¡æ¿å’Œæ–‡æ¡£çš„æ˜ å°„è®°å½•
        mapping = TemplateDocumentMapping(
            template_id=template_id,
            document_id=document.id,
            class_code=final_class_code,  # ä½¿ç”¨å¸¦åºå·çš„æœ€ç»ˆç¼–ç 
            status="completed",
            processed_time=int(time.time()),
            extracted_data=(
                json.dumps(_extracted_data, ensure_ascii=False)
                if _extracted_data
                else None
            ),
        )
        db.add(mapping)

        await db.commit()

        # 8ï¸âƒ£ å°†æ–‡æ¡£ç´¢å¼•åˆ°Elasticsearch
        event.data = "[info] ç´¢å¼•æ–‡æ¡£åˆ°æœç´¢å¼•æ“..."
        yield event.model_dump_json(ensure_ascii=False)

        try:
            from utils.search_engine import get_search_client

            search_client = get_search_client()

            # è·å–upload_timeçš„å€¼
            upload_time = getattr(document, "upload_time", None)

            document_data_for_es = {
                "document_id": document.id,
                "title": document.title,
                "content": doc,
                "summary": doc[:500] if len(doc) > 500 else doc,
                "template_id": document.template_id,
                "file_type": document.file_type,
                "upload_time": (
                    datetime.fromtimestamp(upload_time).isoformat()
                    if upload_time
                    else None
                ),
                "metadata": _extracted_data,  # å°†extracted_dataå­˜å‚¨åœ¨metadataå­—æ®µä¸­
            }
            await search_client.index_document(document_data_for_es)
            logger.info(f"æ–‡æ¡£ {document.id} å·²æˆåŠŸç´¢å¼•åˆ°Elasticsearch")
            event.data = "[info] æ–‡æ¡£ç´¢å¼•æˆåŠŸ"
            yield event.model_dump_json(ensure_ascii=False)
        except Exception as e:
            logger.error(f"æ–‡æ¡£ {document.id} ç´¢å¼•åˆ°Elasticsearchå¤±è´¥: {e}")
            event.data = f"[warning] æ–‡æ¡£ç´¢å¼•å¤±è´¥: {str(e)}"
            yield event.model_dump_json(ensure_ascii=False)

        event.data = "[info] æ–‡æ¡£åˆ›å»ºæˆåŠŸ"
        event.done = True
        yield event.model_dump_json(ensure_ascii=False)

    @staticmethod
    async def get_available_class_codes(
        db: AsyncSession,
        template_id: int,
    ) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šæ¨¡æ¿ä¸‹æ‰€æœ‰å·²å­˜åœ¨çš„åˆ†ç±»ç¼–ç ï¼ˆå¸¦è¯¦ç»†ä¿¡æ¯ï¼‰

        Args:
            db: æ•°æ®åº“ä¼šè¯
            template_id: æ¨¡æ¿ID

        Returns:
            åˆ†ç±»ç¼–ç åˆ—è¡¨ï¼ŒåŒ…å«ç¼–ç ã€æ–‡æ¡£IDã€æ–‡æ¡£æ ‡é¢˜ç­‰ä¿¡æ¯
        """
        # æŸ¥è¯¢è¯¥æ¨¡æ¿ä¸‹æ‰€æœ‰æ–‡æ¡£æ˜ å°„å…³ç³»
        result = await db.execute(
            select(
                TemplateDocumentMapping.class_code,
                TemplateDocumentMapping.document_id,
                Document.title,
                Document.original_filename,
                TemplateDocumentMapping.created_at,
                Document.file_size,
                Document.file_type,
            )
            .join(Document, TemplateDocumentMapping.document_id == Document.id)
            .where(
                TemplateDocumentMapping.template_id == template_id,
                TemplateDocumentMapping.class_code.isnot(None),
            )
            .order_by(TemplateDocumentMapping.class_code.desc())
        )

        mappings = result.all()

        return [
            {
                "class_code": mapping.class_code,
                "document_id": mapping.document_id,
                "title": mapping.title,
                "filename": mapping.original_filename,
                "created_at": mapping.created_at,
                "file_size": mapping.file_size,
                "file_type": mapping.file_type,
            }
            for mapping in mappings
            if mapping.class_code
        ]

    @staticmethod
    async def update_class_code(
        db: AsyncSession,
        document_id: int,
        new_class_code_prefix: str,
    ) -> bool:
        """
        æ›´æ–°æ–‡æ¡£çš„åˆ†ç±»ç¼–ç ï¼ˆåªæ›´æ–°å‰ç¼€éƒ¨åˆ†ï¼Œä¿ç•™åŸæœ‰åºå·ï¼‰

        Args:
            db: æ•°æ®åº“ä¼šè¯
            document_id: æ–‡æ¡£ID
            new_class_code_prefix: æ–°çš„åˆ†ç±»ç¼–ç å‰ç¼€ï¼ˆä¸åŒ…å«æœ€åçš„åºå·ï¼‰

        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        # æŸ¥è¯¢æ–‡æ¡£æ˜ å°„å…³ç³»
        result = await db.execute(
            select(TemplateDocumentMapping).where(
                TemplateDocumentMapping.document_id == document_id
            )
        )
        mapping = result.scalar_one_or_none()

        if not mapping:
            return False

        # è·å–åŸæœ‰ç¼–ç 
        original_code = mapping.class_code or ""
        if not original_code:
            return False

        # åˆ†å‰²åŸç¼–ç ï¼Œæå–åºå·éƒ¨åˆ†
        code_parts = original_code.split("-")
        if len(code_parts) < 2:
            # ç¼–ç æ ¼å¼ä¸æ­£ç¡®
            return False

        # æå–åŸç¼–ç çš„å‰ç¼€å’Œåºå·
        original_prefix = "-".join(code_parts[:-1])
        original_suffix = code_parts[-1]

        # æ ¡éªŒï¼šå¦‚æœå‰ç¼€æ²¡æœ‰å˜åŒ–ï¼Œä¸éœ€è¦æ›´æ–°
        if original_prefix == new_class_code_prefix:
            logger.info(f"æ–‡æ¡£ {document_id} çš„åˆ†ç±»ç¼–ç å‰ç¼€æœªå˜åŒ–ï¼Œæ— éœ€æ›´æ–°")
            return True  # è¿”å›æˆåŠŸï¼Œä½†ä¸åšä¿®æ”¹

        # æ‹¼æ¥æ–°ç¼–ç ï¼šæ–°å‰ç¼€ + åŸåºå·
        final_code = f"{new_class_code_prefix}-{original_suffix}"

        # æ›´æ–°ç¼–ç 
        mapping.class_code = final_code
        await db.commit()

        logger.info(
            f"æ–‡æ¡£ {document_id} çš„åˆ†ç±»ç¼–ç å·²æ›´æ–°: {original_code} -> {final_code}"
        )
        return True

    @staticmethod
    async def get_template_levels(
        db: AsyncSession,
        template_id: int,
    ) -> Dict[str, Any]:
        """
        è·å–æ¨¡æ¿çš„å±‚çº§ç»“æ„å®šä¹‰å’Œå€¼åŸŸé€‰é¡¹ï¼ˆåŒ…å«æ–‡æ¡£ç±»å‹å±‚ï¼‰

        Args:
            db: æ•°æ®åº“ä¼šè¯
            template_id: æ¨¡æ¿ID

        Returns:
            åŒ…å« levels å’Œ level_options çš„å­—å…¸
        """
        # è·å–æ¨¡æ¿
        result = await db.execute(
            select(ClassTemplate).where(ClassTemplate.id == template_id)
        )
        template = result.scalar_one_or_none()

        if not template:
            return {"levels": [], "level_options": {}}

        # è·å–æ¨¡æ¿çš„å±‚çº§å®šä¹‰ï¼ˆåŒ…æ‹¬æ–‡æ¡£ç±»å‹å±‚ï¼‰
        template_json_list: List[Dict[str, Any]] = getattr(template, "levels") or []

        # æ„å»ºæ‰€æœ‰å±‚çº§åˆ—è¡¨ï¼ŒæŒ‰ level æ’åº
        level_list = []
        for level_def in sorted(template_json_list, key=lambda x: x.get("level", 0)):
            level_list.append(
                {
                    "level": level_def.get("level"),
                    "name": level_def.get("name"),
                    "code": level_def.get("code"),
                    "description": level_def.get("description"),
                    "extraction_prompt": level_def.get("extraction_prompt"),
                    "placeholder_example": level_def.get("placeholder_example"),
                    # æ ‡è®°æ˜¯å¦ä¸ºæ–‡æ¡£ç±»å‹å±‚
                    "is_doc_type": level_def.get("is_doc_type", False),
                }
            )

        # è·å–é¢„å¤„ç†çš„å€¼åŸŸé€‰é¡¹
        level_options = getattr(template, "level_options") or {}

        # å¦‚æœæœ‰æ–‡æ¡£ç±»å‹å±‚ï¼Œéœ€è¦ä» DocumentType è¡¨è·å–å®é™…çš„æ–‡æ¡£ç±»å‹é€‰é¡¹
        for level_def in level_list:
            if level_def.get("is_doc_type"):
                # æŸ¥è¯¢è¯¥æ¨¡æ¿ä¸‹çš„æ‰€æœ‰æ–‡æ¡£ç±»å‹
                doc_types_result = await db.execute(
                    select(DocumentType).where(
                        DocumentType.template_id == template_id,
                        DocumentType.is_active == True,
                    )
                )
                doc_types = doc_types_result.scalars().all()

                # æ„å»ºæ–‡æ¡£ç±»å‹é€‰é¡¹ï¼ˆä½¿ç”¨ä¸å…¶ä»–å±‚çº§ç›¸åŒçš„æ ¼å¼ï¼‰
                level_code = level_def.get("code")
                if level_code:
                    level_options[level_code] = [
                        {
                            "name": doc_type.type_code,
                            "description": doc_type.type_name,
                            "doc_type_id": doc_type.id,  # é¢å¤–è¿”å› doc_type_id ä¾›åç»­ä½¿ç”¨
                        }
                        for doc_type in doc_types
                    ]

        return {
            "levels": level_list,
            "level_options": level_options,
        }

    @staticmethod
    def _get_content_type(file_extension: str) -> str:
        """è·å–æ–‡ä»¶ MIME ç±»å‹"""
        content_types = {
            ".pdf": "application/pdf",
            ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            ".doc": "application/msword",
            ".txt": "text/plain",
            ".md": "text/markdown",
            ".png": "image/png",
            ".jpg": "image/jpeg",
            ".jpeg": "image/jpeg",
        }
        return content_types.get(file_extension.lower(), "application/octet-stream")
