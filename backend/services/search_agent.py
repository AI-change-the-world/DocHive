import json
import asyncio
from typing import Any, Dict, List, Optional, TypedDict, Set
from sqlalchemy import or_, select
from sqlalchemy.ext.asyncio import AsyncSession
from elasticsearch import AsyncElasticsearch
from langgraph.graph import END, StateGraph

from config import get_settings
from models.database_models import (
    Document,
    DocumentType,
    DocumentTypeField,
    TemplateDocumentMapping,
)
from services.template_service import TemplateService
from utils.llm_client import llm_client
from loguru import logger

# å…¨å±€å˜é‡å­˜å‚¨graphçŠ¶æ€ï¼Œç”¨äºæ”¯æŒä¸­æ–­å’Œæ¢å¤
# æ³¨æ„: ç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redis ç­‰åˆ†å¸ƒå¼ç¼“å­˜æ›¿ä»£å†…å­˜å­˜å‚¨
graph_state_storage: Dict[str, Dict[str, Any]] = {}


class RetrievalState(TypedDict):
    """
    ä¼˜åŒ–åçš„ RAG æ™ºèƒ½ä½“çŠ¶æ€æœº
    
    å·¥ä½œæµç¨‹:
    1. ESå…¨æ–‡æ£€ç´¢ -> åŸºäºå…³é”®è¯å¿«é€Ÿå¬å›å€™é€‰æ–‡æ¡£
    2. SQLç»“æ„åŒ–æ£€ç´¢ -> åŸºäºæ¨¡æ¿å±‚çº§æå–ç»“æ„åŒ–æ¡ä»¶
    3. ç»“æœèåˆ -> åˆå¹¶ä¸¤è·¯æ£€ç´¢ç»“æœ
    4. ç²¾ç»†åŒ–ç­›é€‰ -> åŸºäºæ–‡æ¡£ç±»å‹ç‰¹å®šå­—æ®µè¿›ä¸€æ­¥ç­›é€‰
    5. ç”Ÿæˆç­”æ¡ˆ -> RAGç”Ÿæˆæœ€ç»ˆå›ç­”
    """

    # === å¿…éœ€è¾“å…¥ ===
    query: str  # ç”¨æˆ·æŸ¥è¯¢
    template_id: int  # æ¨¡æ¿ID
    db: AsyncSession  # æ•°æ®åº“ä¼šè¯ (æ³¨æ„: ä¸åº”åºåˆ—åŒ–åˆ°å­˜å‚¨)
    es_client: AsyncElasticsearch  # ES å®¢æˆ·ç«¯ (æ³¨æ„: ä¸åº”åºåˆ—åŒ–åˆ°å­˜å‚¨)
    session_id: str  # ä¼šè¯ID

    # === èŠ‚ç‚¹ 1 (ESå…¨æ–‡æ£€ç´¢) äº§å‡º ===
    es_fulltext_results: List[Dict[str, Any]]  # ESå…¨æ–‡æ£€ç´¢çš„åˆæ­¥ç»“æœ
    es_document_ids: Set[int]  # ESå¬å›çš„æ–‡æ¡£IDé›†åˆ

    # === èŠ‚ç‚¹ 2 (SQLç»“æ„åŒ–æ£€ç´¢) äº§å‡º ===
    class_template_levels: Optional[List[Dict[str, Any]]]  # æ¨¡æ¿å±‚çº§å®šä¹‰
    category: str  # è¯†åˆ«å‡ºçš„æ–‡æ¡£ç±»åˆ«
    category_field_code: Optional[str]  # ç±»åˆ«å­—æ®µç¼–ç 
    sql_extracted_conditions: List[Dict[str, Any]]  # LLMæå–çš„ç»“æ„åŒ–æ¡ä»¶
    sql_document_ids: Set[int]  # SQLå¬å›çš„æ–‡æ¡£IDé›†åˆ

    # === èŠ‚ç‚¹ 3 (ç»“æœèåˆ) äº§å‡º ===
    merged_document_ids: List[int]  # èåˆåçš„æ–‡æ¡£IDåˆ—è¡¨(æŒ‰ç›¸å…³æ€§æ’åº)
    merged_documents: List[Document]  # èåˆåçš„æ–‡æ¡£å¯¹è±¡åˆ—è¡¨
    fusion_strategy: str  # èåˆç­–ç•¥: 'intersection'(äº¤é›†), 'union'(å¹¶é›†), 'es_primary'(ESä¸ºä¸»)

    # === èŠ‚ç‚¹ 4 (ç²¾ç»†åŒ–ç­›é€‰) äº§å‡º ===
    document_type_fields: List[DocumentTypeField]  # æ–‡æ¡£ç±»å‹ç‰¹å®šå­—æ®µ
    refined_conditions: Dict[str, Any]  # ç²¾ç»†åŒ–æŸ¥è¯¢æ¡ä»¶
    final_es_query: Optional[Dict[str, Any]]  # æœ€ç»ˆESæŸ¥è¯¢
    final_results: List[Dict[str, Any]]  # ç²¾ç»†åŒ–ç­›é€‰åçš„æœ€ç»ˆç»“æœ

    # === èŠ‚ç‚¹ 5 (æ­§ä¹‰å¤„ç†) äº§å‡º ===
    ambiguity_message: Optional[str]  # æ­§ä¹‰æç¤ºæ¶ˆæ¯

    # === èŠ‚ç‚¹ 6 (ç”Ÿæˆç­”æ¡ˆ) äº§å‡º ===
    answer: Optional[str]  # æœ€ç»ˆRAGç­”æ¡ˆ


# ==================== èŠ‚ç‚¹ 1: ES å…¨æ–‡æ£€ç´¢ ====================
async def es_fulltext_retrieval(state: RetrievalState) -> RetrievalState:
    """
    èŠ‚ç‚¹ 1: ES å…¨æ–‡æ£€ç´¢
    
    åŸºäºç”¨æˆ·æŸ¥è¯¢åœ¨ Elasticsearch ä¸­è¿›è¡Œå…¨æ–‡æ£€ç´¢,å¿«é€Ÿå¬å›å€™é€‰æ–‡æ¡£ã€‚
    è¿™æ˜¯ç¬¬ä¸€é˜¶æ®µçš„ç²—å¬å›,åˆ©ç”¨ ES çš„å…¨æ–‡æœç´¢èƒ½åŠ›ã€‚
    
    è¾“å‡º:
    - es_fulltext_results: ES æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
    - es_document_ids: æ–‡æ¡£ ID é›†åˆ
    """
    logger.info("========== èŠ‚ç‚¹ 1: ES å…¨æ–‡æ£€ç´¢ ==========")
    
    settings = get_settings()
    query = state["query"]
    template_id = state["template_id"]
    
    # æ„é€  ES å…¨æ–‡æ£€ç´¢æŸ¥è¯¢
    es_query = {
        "query": {
            "bool": {
                "must": {
                    "multi_match": {
                        "query": query,
                        "fields": ["title^3", "content"],  # title æƒé‡æ›´é«˜
                        "type": "best_fields",
                        "fuzziness": "AUTO",  # æ”¯æŒæ¨¡ç³ŠåŒ¹é…
                    }
                },
                "filter": [
                    {"term": {"template_id": template_id}}  # é™å®šæ¨¡æ¿èŒƒå›´
                ],
            }
        },
        "size": 20,  # å¬å› Top 20
        "_source": ["document_id", "title", "content", "metadata"],
    }
    
    try:
        response = await state["es_client"].search(
            index=settings.ELASTICSEARCH_INDEX,
            body=es_query
        )
        
        hits = response.get("hits", {}).get("hits", [])
        state["es_fulltext_results"] = [hit["_source"] for hit in hits]
        state["es_document_ids"] = set(
            hit["_source"]["document_id"] for hit in hits
        )
        
        logger.info(f"âœ… ES å…¨æ–‡æ£€ç´¢å¬å› {len(hits)} ç¯‡æ–‡æ¡£")
        logger.info(f"   æ–‡æ¡£ ID: {list(state['es_document_ids'])}")
        
    except Exception as e:
        logger.error(f"âŒ ES å…¨æ–‡æ£€ç´¢å¤±è´¥: {e}")
        state["es_fulltext_results"] = []
        state["es_document_ids"] = set()
    
    return state


# ==================== èŠ‚ç‚¹ 2: SQL ç»“æ„åŒ–æ£€ç´¢ ====================
async def sql_structured_retrieval(state: RetrievalState) -> RetrievalState:
    """
    èŠ‚ç‚¹ 2: SQL ç»“æ„åŒ–æ£€ç´¢
    
    åŸºäºæ¨¡æ¿å±‚çº§å®šä¹‰,ä½¿ç”¨ LLM æå–ç»“æ„åŒ–æŸ¥è¯¢æ¡ä»¶,
    åœ¨æ•°æ®åº“ä¸­è¿›è¡Œç²¾ç¡®çš„ç»“æ„åŒ–æ£€ç´¢ã€‚
    
    è¾“å‡º:
    - class_template_levels: æ¨¡æ¿å±‚çº§å®šä¹‰
    - category: æ–‡æ¡£ç±»åˆ«
    - sql_extracted_conditions: æå–çš„ç»“æ„åŒ–æ¡ä»¶
    - sql_document_ids: SQL å¬å›çš„æ–‡æ¡£ ID é›†åˆ
    """
    logger.info("========== èŠ‚ç‚¹ 2: SQL ç»“æ„åŒ–æ£€ç´¢ ==========")
    
    # 1. è·å–æ¨¡æ¿å±‚çº§å®šä¹‰
    cls_template = await TemplateService.get_template(
        state["db"], state["template_id"]
    )
    
    if not cls_template:
        logger.warning("âš ï¸ æœªæ‰¾åˆ°æ¨¡æ¿,è·³è¿‡ SQL ç»“æ„åŒ–æ£€ç´¢")
        state["class_template_levels"] = []
        state["category"] = "*"
        state["sql_extracted_conditions"] = []
        state["sql_document_ids"] = set()
        return state
    
    # ä½¿ç”¨ property è·å–å±‚çº§å®šä¹‰ (è‡ªåŠ¨å¤„ç† JSON è½¬æ¢)
    cls_template_levels = cls_template.levels
    if not isinstance(cls_template_levels, list):
        logger.error("âŒ æ¨¡æ¿å±‚çº§å®šä¹‰æ ¼å¼é”™è¯¯")
        state["class_template_levels"] = []
        state["category"] = "*"
        state["sql_extracted_conditions"] = []
        state["sql_document_ids"] = set()
        return state
    
    state["class_template_levels"] = cls_template_levels
    
    # 2. æå–ç±»åˆ«å­—æ®µ
    type_code = ""
    for field in cls_template_levels:
        if field.get("is_doc_type", False):
            type_code = field.get("code", "")
            state["category_field_code"] = type_code
            break
    
    # 3. ä½¿ç”¨ LLM æå–ç»“æ„åŒ–æ¡ä»¶
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç»“æ„åŒ–æŸ¥è¯¢åŠ©æ‰‹ã€‚
ç”¨æˆ·ä¼šç»™å‡ºä¸€ä¸ªè‡ªç„¶è¯­è¨€æ£€ç´¢è¯·æ±‚,è¯·ä½ æ ¹æ®ä»¥ä¸‹å­—æ®µå®šä¹‰,æå–å‡ºç»“æ„åŒ–çš„æ£€ç´¢æ¡ä»¶ã€‚

å­—æ®µå®šä¹‰:
{json.dumps(cls_template_levels, ensure_ascii=False, indent=2)}

è¦æ±‚:
1. è¾“å‡º JSON å¯¹è±¡,æ ¼å¼: {{"conditions": [{{"code": "å­—æ®µç¼–ç ", "value": "æå–å€¼", "level": å±‚çº§}}], "category": "æ–‡æ¡£ç±»åˆ«"}}
2. å¦‚æœæ— æ³•ä»æŸ¥è¯¢ä¸­æ¨ç†å‡ºæŸä¸ªå­—æ®µ,value è®¾ä¸º "UNKNOWN"
3. category å­—æ®µåº”è¯¥æ˜¯ is_doc_type=true çš„å­—æ®µçš„å€¼
4. åªæå–ç”¨æˆ·æ˜ç¡®æåˆ°çš„ä¿¡æ¯,ä¸è¦çŒœæµ‹

ç”¨æˆ·æŸ¥è¯¢:
{state['query']}

è¯·ç›´æ¥è¾“å‡º JSON,ä¸è¦è§£é‡Šã€‚
    """
    
    try:
        llm_response = await llm_client.extract_json_response(prompt, db=state["db"])
        logger.info(f"ğŸ¤– LLM æå–çš„ç»“æ„åŒ–æ¡ä»¶: {llm_response}")
        
        conditions = llm_response.get("conditions", [])
        state["category"] = llm_response.get("category", "*")
        state["sql_extracted_conditions"] = conditions
        
    except Exception as e:
        logger.error(f"âŒ LLM æå–ç»“æ„åŒ–æ¡ä»¶å¤±è´¥: {e}")
        state["category"] = "*"
        state["sql_extracted_conditions"] = []
    
    # 4. æ„é€  SQL æŸ¥è¯¢æ¡ä»¶
    conditions_clauses = []
    for cond in state["sql_extracted_conditions"]:
        value = cond.get("value")
        if value and value != "UNKNOWN":
            if isinstance(value, list):
                for v in value:
                    conditions_clauses.append(
                        TemplateDocumentMapping.class_code.like(f"%{v}%")
                    )
            else:
                conditions_clauses.append(
                    TemplateDocumentMapping.class_code.like(f"%{value}%")
                )
    
    # 5. æ‰§è¡Œ SQL æŸ¥è¯¢
    stmt = select(TemplateDocumentMapping.document_id).where(
        TemplateDocumentMapping.template_id == state["template_id"]
    )
    if conditions_clauses:
        stmt = stmt.where(or_(*conditions_clauses))
    
    try:
        result = await state["db"].execute(stmt)
        document_ids = [row[0] for row in result.all()]
        state["sql_document_ids"] = set(document_ids)
        
        logger.info(f"âœ… SQL ç»“æ„åŒ–æ£€ç´¢å¬å› {len(document_ids)} ç¯‡æ–‡æ¡£")
        logger.info(f"   æ–‡æ¡£ ID: {list(state['sql_document_ids'])}")
        
    except Exception as e:
        logger.error(f"âŒ SQL æŸ¥è¯¢å¤±è´¥: {e}")
        state["sql_document_ids"] = set()
    
    return state


# ==================== èŠ‚ç‚¹ 3: ç»“æœèåˆ ====================
async def merge_retrieval_results(state: RetrievalState) -> RetrievalState:
    """
    èŠ‚ç‚¹ 3: ç»“æœèåˆ
    
    å°† ES å…¨æ–‡æ£€ç´¢å’Œ SQL ç»“æ„åŒ–æ£€ç´¢çš„ç»“æœè¿›è¡Œèåˆ,
    é‡‡ç”¨æ™ºèƒ½ç­–ç•¥å†³å®šå¦‚ä½•åˆå¹¶ä¸¤è·¯å¬å›ç»“æœã€‚
    
    èåˆç­–ç•¥:
    1. 'intersection': å–äº¤é›† (ç²¾ç¡®åŒ¹é…)
    2. 'union': å–å¹¶é›† (å¹¿æ³›å¬å›)
    3. 'es_primary': ESä¸ºä¸»,SQLä¸ºè¾…åŠ©ç­›é€‰
    
    è¾“å‡º:
    - merged_document_ids: èåˆåçš„æ–‡æ¡£ ID åˆ—è¡¨
    - merged_documents: èåˆåçš„æ–‡æ¡£å¯¹è±¡
    - fusion_strategy: ä½¿ç”¨çš„èåˆç­–ç•¥
    """
    logger.info("========== èŠ‚ç‚¹ 3: ç»“æœèåˆ ==========")
    
    es_ids = state.get("es_document_ids", set())
    sql_ids = state.get("sql_document_ids", set())
    
    logger.info(f"ğŸ“Š ES å¬å›: {len(es_ids)} ç¯‡, SQL å¬å›: {len(sql_ids)} ç¯‡")
    
    # å†³å®šèåˆç­–ç•¥
    if not es_ids and not sql_ids:
        # ä¸¤è·¯éƒ½æ²¡å¬å›
        logger.warning("âš ï¸ ES å’Œ SQL éƒ½æœªå¬å›ä»»ä½•æ–‡æ¡£")
        state["fusion_strategy"] = "none"
        state["merged_document_ids"] = []
        state["merged_documents"] = []
        return state
    
    elif not sql_ids:
        # åªæœ‰ ES å¬å›äº†
        logger.info("ğŸ“Œ ç­–ç•¥: ESä¸ºä¸» (SQLæœªå¬å›)")
        state["fusion_strategy"] = "es_only"
        merged_ids = list(es_ids)
    
    elif not es_ids:
        # åªæœ‰ SQL å¬å›äº†
        logger.info("ğŸ“Œ ç­–ç•¥: SQLä¸ºä¸» (ESæœªå¬å›)")
        state["fusion_strategy"] = "sql_only"
        merged_ids = list(sql_ids)
    
    else:
        # ä¸¤è·¯éƒ½å¬å›äº†,ä½¿ç”¨æ™ºèƒ½èåˆç­–ç•¥
        intersection = es_ids & sql_ids
        union = es_ids | sql_ids
        
        if len(intersection) >= 3:
            # äº¤é›†è¶³å¤Ÿå¤š,ä½¿ç”¨äº¤é›† (é«˜ç²¾åº¦)
            logger.info(f"ğŸ“Œ ç­–ç•¥: äº¤é›† (å…± {len(intersection)} ç¯‡æ–‡æ¡£)")
            state["fusion_strategy"] = "intersection"
            merged_ids = list(intersection)
        
        elif len(intersection) > 0:
            # äº¤é›†è¾ƒå°‘,ESä¸ºä¸»,SQLä¸ºè¾…
            logger.info(f"ğŸ“Œ ç­–ç•¥: ESä¸ºä¸»,SQLè¾…åŠ© (äº¤é›† {len(intersection)} ç¯‡)")
            state["fusion_strategy"] = "es_primary"
            # ES ç»“æœåœ¨å‰,äº¤é›†ä¼˜å…ˆ,ç„¶åæ˜¯ ES ç‹¬æœ‰
            merged_ids = list(intersection) + [id for id in es_ids if id not in intersection]
        
        else:
            # æ²¡æœ‰äº¤é›†,å–å¹¶é›†
            logger.info(f"ğŸ“Œ ç­–ç•¥: å¹¶é›† (ES {len(es_ids)} + SQL {len(sql_ids)})")
            state["fusion_strategy"] = "union"
            merged_ids = list(es_ids) + [id for id in sql_ids if id not in es_ids]
    
    # é™åˆ¶ç»“æœæ•°é‡ (Top 10)
    merged_ids = merged_ids[:10]
    state["merged_document_ids"] = merged_ids
    
    # ä»æ•°æ®åº“åŠ è½½æ–‡æ¡£å¯¹è±¡
    if merged_ids:
        try:
            docs_result = await state["db"].execute(
                select(Document).where(Document.id.in_(merged_ids))
            )
            docs = list(docs_result.scalars().all())
            
            # æŒ‰ merged_ids çš„é¡ºåºæ’åº
            docs_dict = {int(doc.id): doc for doc in docs}  # type: ignore
            state["merged_documents"] = [
                docs_dict[doc_id] for doc_id in merged_ids if doc_id in docs_dict
            ]
            
            logger.info(f"âœ… èåˆå®Œæˆ,æœ€ç»ˆä¿ç•™ {len(state['merged_documents'])} ç¯‡æ–‡æ¡£")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ–‡æ¡£å¯¹è±¡å¤±è´¥: {e}")
            state["merged_documents"] = []
    else:
        state["merged_documents"] = []
    
    return state


# ==================== èŠ‚ç‚¹ 4: ç²¾ç»†åŒ–ç­›é€‰ ====================
async def refined_filtering(state: RetrievalState) -> RetrievalState:
    """
    èŠ‚ç‚¹ 4: ç²¾ç»†åŒ–ç­›é€‰
    
    åŸºäºæ–‡æ¡£ç±»å‹çš„ç‰¹å®šå­—æ®µ (DocumentTypeField),
    ä½¿ç”¨ LLM æå–æ›´ç²¾ç»†çš„æŸ¥è¯¢æ¡ä»¶,åœ¨ ES ä¸­è¿›è¡ŒäºŒæ¬¡ç­›é€‰ã€‚
    
    è¾“å‡º:
    - document_type_fields: æ–‡æ¡£ç±»å‹ç‰¹å®šå­—æ®µ
    - refined_conditions: ç²¾ç»†åŒ–æ¡ä»¶
    - final_es_query: æœ€ç»ˆ ES æŸ¥è¯¢
    - final_results: æœ€ç»ˆæ£€ç´¢ç»“æœ
    """
    logger.info("========== èŠ‚ç‚¹ 4: ç²¾ç»†åŒ–ç­›é€‰ ==========")
    
    # å¦‚æœæ²¡æœ‰èåˆç»“æœ,ç›´æ¥è·³è¿‡
    if not state.get("merged_documents"):
        logger.warning("âš ï¸ æ— èåˆç»“æœ,è·³è¿‡ç²¾ç»†åŒ–ç­›é€‰")
        state["document_type_fields"] = []
        state["refined_conditions"] = {}
        state["final_es_query"] = None
        state["final_results"] = []
        return state
    
    # å¦‚æœç±»åˆ«ä¸ºé€šé…ç¬¦,è·³è¿‡ç²¾ç»†åŒ–ç­›é€‰
    category = state.get("category", "*")
    if category == "*":
        logger.info("ğŸ“Œ ç±»åˆ«ä¸ºé€šé…ç¬¦,è·³è¿‡ç²¾ç»†åŒ–ç­›é€‰,ç›´æ¥ä½¿ç”¨èåˆç»“æœ")
        state["document_type_fields"] = []
        state["refined_conditions"] = {}
        state["final_es_query"] = None
        state["final_results"] = _convert_docs_to_results(state["merged_documents"])
        return state
    
    # 1. è·å– DocumentType å’Œ DocumentTypeField
    try:
        doc_types_result = await state["db"].execute(
            select(DocumentType).where(
                DocumentType.template_id == state["template_id"],
                DocumentType.type_code == category,
            )
        )
        doc_types = doc_types_result.scalars().all()
        
        if not doc_types:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç±»åˆ« '{category}' çš„ DocumentType,è·³è¿‡ç²¾ç»†åŒ–ç­›é€‰")
            state["document_type_fields"] = []
            state["refined_conditions"] = {}
            state["final_results"] = _convert_docs_to_results(state["merged_documents"])
            return state
        
        document_type_fields_result = await state["db"].execute(
            select(DocumentTypeField).where(
                DocumentTypeField.doc_type_id.in_([dt.id for dt in doc_types])
            )
        )
        document_type_fields = list(document_type_fields_result.scalars().all())
        state["document_type_fields"] = document_type_fields
        
        if not document_type_fields:
            logger.info("ğŸ“Œ è¯¥ç±»åˆ«æ— ç‰¹å®šå­—æ®µ,è·³è¿‡ç²¾ç»†åŒ–ç­›é€‰")
            state["refined_conditions"] = {}
            state["final_results"] = _convert_docs_to_results(state["merged_documents"])
            return state
        
    except Exception as e:
        logger.error(f"âŒ è·å–æ–‡æ¡£ç±»å‹å­—æ®µå¤±è´¥: {e}")
        state["document_type_fields"] = []
        state["refined_conditions"] = {}
        state["final_results"] = _convert_docs_to_results(state["merged_documents"])
        return state
    
    # 2. ä½¿ç”¨ LLM æå–ç²¾ç»†åŒ–æ¡ä»¶
    field_definitions = ""
    field_map = {}  # å­—æ®µå -> å­—æ®µç±»å‹
    
    for f in document_type_fields:
        field_definitions += f"- {f.field_name}: {f.description} (ç±»å‹: {f.field_type})\n"
        field_map[f.field_name] = f.field_type
    
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç²¾ç»†åŒ–æŸ¥è¯¢åŠ©æ‰‹ã€‚
ç”¨æˆ·æ­£åœ¨æŸ¥è¯¢ç±»åˆ«ä¸º '{category}' çš„æ–‡æ¡£,è¯¥ç±»åˆ«æœ‰ä»¥ä¸‹ç‰¹å®šå­—æ®µ:

{field_definitions}

è¯·æ ¹æ®ç”¨æˆ·æŸ¥è¯¢æå–è¿™äº›å­—æ®µçš„å…·ä½“å€¼:

è¦æ±‚:
1. è¾“å‡º JSON å¯¹è±¡: {{"conditions": {{"å­—æ®µå": "å€¼"}}, "missing_fields": ["ç¼ºå¤±å­—æ®µ"]}}
2. åªæå–ç”¨æˆ·æ˜ç¡®æåˆ°çš„å­—æ®µå€¼
3. missing_fields åˆ—å‡ºå¯¹ç²¾ç¡®æ£€ç´¢æœ‰å¸®åŠ©ä½†ç”¨æˆ·æœªæä¾›çš„å­—æ®µ

ç”¨æˆ·æŸ¥è¯¢:
{state['query']}

è¯·ç›´æ¥è¾“å‡º JSON,ä¸è¦è§£é‡Šã€‚
    """
    
    try:
        llm_response = await llm_client.extract_json_response(prompt, db=state["db"])
        logger.info(f"ğŸ¤– LLM æå–çš„ç²¾ç»†åŒ–æ¡ä»¶: {llm_response}")
        
        conditions = llm_response.get("conditions", {})
        missing_fields = llm_response.get("missing_fields", [])
        
        state["refined_conditions"] = conditions
        
        # 3. æ£€æŸ¥æ­§ä¹‰
        if not conditions and missing_fields:
            missing_str = "ã€".join(missing_fields)
            state["ambiguity_message"] = (
                f"æ‚¨çš„é—®é¢˜ä¼¼ä¹æœ‰äº›å®½æ³›ã€‚ä¸ºäº†æ›´ç²¾ç¡®åœ°æŸ¥æ‰¾,èƒ½å¦æä¾›: {missing_str}?"
            )
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ­§ä¹‰,å»ºè®®è¡¥å……: {missing_str}")
            state["final_results"] = _convert_docs_to_results(state["merged_documents"])
            return state
        
    except Exception as e:
        logger.error(f"âŒ LLM æå–ç²¾ç»†åŒ–æ¡ä»¶å¤±è´¥: {e}")
        state["refined_conditions"] = {}
        state["final_results"] = _convert_docs_to_results(state["merged_documents"])
        return state
    
    # 4. æ„é€ ç²¾ç»†åŒ– ES æŸ¥è¯¢
    if not state["refined_conditions"]:
        logger.info("ğŸ“Œ æ— ç²¾ç»†åŒ–æ¡ä»¶,ç›´æ¥ä½¿ç”¨èåˆç»“æœ")
        state["final_es_query"] = None
        state["final_results"] = _convert_docs_to_results(state["merged_documents"])
        return state
    
    # åªåœ¨èåˆåçš„æ–‡æ¡£ä¸­ç­›é€‰
    merged_doc_ids = state["merged_document_ids"]
    
    must_clauses = []
    for field_name, value in state["refined_conditions"].items():
        if not value or value == "UNKNOWN":
            continue
        
        field_type = field_map.get(field_name, "text")
        
        # æ ¹æ®å­—æ®µç±»å‹æ„é€ æŸ¥è¯¢
        if field_type in ["text", "textarea"]:
            must_clauses.append({"match": {f"metadata.{field_name}": value}})
        elif field_type == "number":
            must_clauses.append({"term": {f"metadata.{field_name}": value}})
        elif field_type == "date":
            must_clauses.append({"range": {f"metadata.{field_name}": {"gte": value}}})
        else:
            must_clauses.append({"term": {f"metadata.{field_name}": value}})
    
    final_es_query = {
        "query": {
            "bool": {
                "must": must_clauses,
                "filter": [
                    {"terms": {"document_id": merged_doc_ids}}
                ]
            }
        },
        "size": 5,
        "_source": ["document_id", "title", "content", "metadata"],
    }
    
    state["final_es_query"] = final_es_query
    
    # 5. æ‰§è¡Œç²¾ç»†åŒ– ES æŸ¥è¯¢
    try:
        settings = get_settings()
        response = await state["es_client"].search(
            index=settings.ELASTICSEARCH_INDEX,
            body=final_es_query
        )
        
        hits = response.get("hits", {}).get("hits", [])
        state["final_results"] = [hit["_source"] for hit in hits]
        
        logger.info(f"âœ… ç²¾ç»†åŒ–ç­›é€‰å®Œæˆ,ä¿ç•™ {len(hits)} ç¯‡æ–‡æ¡£")
        
    except Exception as e:
        logger.error(f"âŒ ç²¾ç»†åŒ– ES æŸ¥è¯¢å¤±è´¥: {e}")
        # é™çº§: ä½¿ç”¨èåˆç»“æœ
        state["final_results"] = _convert_docs_to_results(state["merged_documents"])
    
    return state


def _convert_docs_to_results(documents: List[Document]) -> List[Dict[str, Any]]:
    """
    è¾…åŠ©å‡½æ•°: å°† Document å¯¹è±¡åˆ—è¡¨è½¬æ¢ä¸ºç»“æœå­—å…¸åˆ—è¡¨
    """
    results = []
    for doc in documents:
        results.append({
            "document_id": doc.id,
            "title": doc.title,
            "content": doc.content_text or "",
            "metadata": doc.doc_metadata or {},
        })
    return results


# ==================== èŠ‚ç‚¹ 5: æ­§ä¹‰å¤„ç† ====================
async def handle_ambiguity(state: RetrievalState) -> RetrievalState:
    """
    èŠ‚ç‚¹ 5: æ­§ä¹‰å¤„ç†
    
    å¦‚æœæŸ¥è¯¢æœ‰æ­§ä¹‰,æš‚åœæµç¨‹å¹¶å‘ç”¨æˆ·æé—®ã€‚
    è¿™æ˜¯ä¸€ä¸ªç»ˆç«¯èŠ‚ç‚¹ã€‚
    """
    logger.info("========== èŠ‚ç‚¹ 5: æ­§ä¹‰å¤„ç† ==========")
    logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ­§ä¹‰: {state.get('ambiguity_message')}")
    
    # çŠ¶æ€å·²åŒ…å« ambiguity_message,ç›´æ¥è¿”å›
    # å‰ç«¯ä¼šå±•ç¤ºè¿™ä¸ªæ¶ˆæ¯å¹¶ç­‰å¾…ç”¨æˆ·è¾“å…¥
    return state


# ==================== èŠ‚ç‚¹ 6: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ ====================
async def generate_answer(state: RetrievalState) -> RetrievalState:
    """
    èŠ‚ç‚¹ 6: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    
    åŸºäºæœ€ç»ˆæ£€ç´¢ç»“æœ,ä½¿ç”¨ RAG ç”Ÿæˆç”¨æˆ·é—®é¢˜çš„ç­”æ¡ˆã€‚
    
    è¾“å‡º:
    - answer: æœ€ç»ˆç­”æ¡ˆ
    """
    logger.info("========== èŠ‚ç‚¹ 6: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ ==========")
    
    query = state["query"]
    results = state.get("final_results", [])
    
    if not results:
        logger.warning("âš ï¸ æ— æœ€ç»ˆæ£€ç´¢ç»“æœ,æ— æ³•ç”Ÿæˆç­”æ¡ˆ")
        state["answer"] = "æŠ±æ­‰,æˆ‘æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„æ–‡æ¡£ã€‚å»ºè®®æ‚¨:\n1. å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯\n2. ç®€åŒ–æˆ–æ˜ç¡®æ‚¨çš„é—®é¢˜\n3. æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²ä¸Šä¼ åˆ°ç³»ç»Ÿä¸­"
        return state
    
    # æ„é€  RAG ä¸Šä¸‹æ–‡
    context_parts = []
    for i, doc in enumerate(results[:5], 1):  # æœ€å¤šä½¿ç”¨ 5 ç¯‡æ–‡æ¡£
        doc_context = f"ã€æ–‡æ¡£ {i}ã€‘\n"
        doc_context += f"æ ‡é¢˜: {doc.get('title', 'æœªçŸ¥æ ‡é¢˜')}\n"
        
        # æ™ºèƒ½æˆªå–å†…å®¹ç‰‡æ®µ (ä¼˜å…ˆåŒ…å«æŸ¥è¯¢å…³é”®è¯é™„è¿‘çš„å†…å®¹)
        content = doc.get('content', '')
        if len(content) > 800:
            # ç®€å•æˆªå–ç­–ç•¥,å®é™…å¯ä»¥ç”¨æ›´æ™ºèƒ½çš„æ–¹æ³•
            content = content[:800] + "..."
        doc_context += f"å†…å®¹: {content}\n"
        
        # æ·»åŠ å…ƒæ•°æ®
        metadata = doc.get('metadata', {})
        if metadata:
            doc_context += f"å…ƒæ•°æ®: {json.dumps(metadata, ensure_ascii=False)}\n"
        
        context_parts.append(doc_context)
    
    context_str = "\n".join(context_parts)
    
    # æ„é€  RAG prompt
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€æ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‘
{context_str}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
1. åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹è¿›è¡Œå›ç­”,å¦‚æœæ–‡æ¡£ä¸­æœ‰æ˜ç¡®ç­”æ¡ˆè¯·ç›´æ¥å¼•ç”¨
2. å¦‚æœéœ€è¦å¼•ç”¨æ–‡æ¡£,è¯·ä½¿ç”¨ "æ ¹æ®æ–‡æ¡£X" çš„æ ¼å¼
3. å¦‚æœæ–‡æ¡£ä¿¡æ¯ä¸è¶³ä»¥å®Œæ•´å›ç­”é—®é¢˜,è¯·æ˜ç¡®è¯´æ˜å“ªäº›éƒ¨åˆ†æ— æ³•ç¡®å®š
4. å›ç­”è¦ç®€æ´ã€å‡†ç¡®ã€ä¸“ä¸š
5. å¦‚æœæ–‡æ¡£å†…å®¹ä¸é—®é¢˜æ— å…³,è¯·å¦‚å®è¯´æ˜

è¯·å¼€å§‹å›ç­”:
    """
    
    try:
        answer = await llm_client.chat_completion(prompt, db=state["db"])
        state["answer"] = answer
        logger.info(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(answer)} å­—ç¬¦)")
        
    except Exception as e:
        logger.error(f"âŒ LLM ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
        state["answer"] = "æŠ±æ­‰,æˆ‘åœ¨ç”Ÿæˆç­”æ¡ˆæ—¶é‡åˆ°äº†æŠ€æœ¯é—®é¢˜,è¯·ç¨åé‡è¯•ã€‚"
    
    return state


# ==================== å†³ç­–å‡½æ•° ====================
def should_ask_user(state: RetrievalState) -> str:
    """
    å†³ç­–å‡½æ•°: åˆ¤æ–­æ˜¯å¦éœ€è¦å‘ç”¨æˆ·æé—®æ¾„æ¸…
    
    Returns:
        'ask_user': æœ‰æ­§ä¹‰,éœ€è¦ç”¨æˆ·æ¾„æ¸…
        'generate_answer': æ— æ­§ä¹‰,ç›´æ¥ç”Ÿæˆç­”æ¡ˆ
    """
    if state.get("ambiguity_message"):
        logger.info("ğŸ”€ å†³ç­–: æœ‰æ­§ä¹‰ -> ask_user")
        return "ask_user"
    else:
        logger.info("ğŸ”€ å†³ç­–: æ— æ­§ä¹‰ -> generate_answer")
        return "generate_answer"


# ==================== æ„å»º LangGraph å·¥ä½œæµ ====================
"""
ä¼˜åŒ–åçš„å·¥ä½œæµç¨‹:

1. ESå…¨æ–‡æ£€ç´¢ (es_fulltext_retrieval)
   â†“
2. SQLç»“æ„åŒ–æ£€ç´¢ (sql_structured_retrieval) 
   â†“
3. ç»“æœèåˆ (merge_retrieval_results)
   â†“
4. ç²¾ç»†åŒ–ç­›é€‰ (refined_filtering)
   â†“
5. [å†³ç­–] æ˜¯å¦æœ‰æ­§ä¹‰? (should_ask_user)
   â”œâ”€ ask_user: æ­§ä¹‰å¤„ç† (handle_ambiguity) â†’ END
   â””â”€ generate_answer: ç”Ÿæˆç­”æ¡ˆ (generate_answer) â†’ END
"""

# 1. åˆå§‹åŒ– StateGraph
workflow = StateGraph(RetrievalState)

# 2. æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
workflow.add_node("es_fulltext", es_fulltext_retrieval)        # èŠ‚ç‚¹1: ESå…¨æ–‡æ£€ç´¢
workflow.add_node("sql_structured", sql_structured_retrieval)  # èŠ‚ç‚¹2: SQLç»“æ„åŒ–æ£€ç´¢
workflow.add_node("merge_results", merge_retrieval_results)    # èŠ‚ç‚¹3: ç»“æœèåˆ
workflow.add_node("refined_filter", refined_filtering)          # èŠ‚ç‚¹4: ç²¾ç»†åŒ–ç­›é€‰
workflow.add_node("ask_user", handle_ambiguity)                 # èŠ‚ç‚¹5a: æ­§ä¹‰å¤„ç†
workflow.add_node("generate_answer", generate_answer)           # èŠ‚ç‚¹5b: ç”Ÿæˆç­”æ¡ˆ

# 3. è®¾ç½®å›¾çš„å…¥å£ç‚¹
workflow.set_entry_point("es_fulltext")

# 4. æ·»åŠ çº¿æ€§è¾¹ (Sequential Edges)
workflow.add_edge("es_fulltext", "sql_structured")   # ESå…¨æ–‡ â†’ SQLç»“æ„åŒ–
workflow.add_edge("sql_structured", "merge_results")  # SQLç»“æ„åŒ– â†’ ç»“æœèåˆ
workflow.add_edge("merge_results", "refined_filter")  # ç»“æœèåˆ â†’ ç²¾ç»†åŒ–ç­›é€‰

# 5. æ·»åŠ æ¡ä»¶è¾¹ (Conditional Edge)
#    åœ¨ç²¾ç»†åŒ–ç­›é€‰å,åˆ¤æ–­æ˜¯å¦æœ‰æ­§ä¹‰
workflow.add_conditional_edges(
    "refined_filter",   # æºèŠ‚ç‚¹
    should_ask_user,    # å†³ç­–å‡½æ•°
    {
        "ask_user": "ask_user",                 # æœ‰æ­§ä¹‰ â†’ å‘ç”¨æˆ·æé—®
        "generate_answer": "generate_answer",   # æ— æ­§ä¹‰ â†’ ç”Ÿæˆç­”æ¡ˆ
    },
)

# 6. è®¾ç½®å›¾çš„ç»ˆç‚¹
workflow.add_edge("ask_user", END)        # æ­§ä¹‰å¤„ç†åç»“æŸ
workflow.add_edge("generate_answer", END)  # ç”Ÿæˆç­”æ¡ˆåç»“æŸ

# 7. ç¼–è¯‘å›¾
app = workflow.compile()

logger.info("âœ… LangGraph æ™ºèƒ½ä½“å·¥ä½œæµç¼–è¯‘å®Œæˆ")
logger.info("ğŸ“Š å·¥ä½œæµç¨‹: ESå…¨æ–‡æ£€ç´¢ â†’ SQLç»“æ„åŒ–æ£€ç´¢ â†’ ç»“æœèåˆ â†’ ç²¾ç»†åŒ–ç­›é€‰ â†’ ç”Ÿæˆç­”æ¡ˆ/æ­§ä¹‰å¤„ç†")
