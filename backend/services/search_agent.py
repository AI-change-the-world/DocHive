import asyncio
import hashlib
import json
import re
from difflib import SequenceMatcher
from typing import Any, Dict, List, Optional, Set, TypedDict

from elasticsearch import AsyncElasticsearch
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, StateGraph
from langgraph.graph.state import CompiledStateGraph
from loguru import logger
from sqlalchemy import or_, select
from sqlalchemy.ext.asyncio import AsyncSession

from models.database_models import (
    Document,
    DocumentType,
    DocumentTypeField,
    TemplateDocumentMapping,
)
from services.intent_router import format_tool_result_as_answer, function_calling_router
from services.template_service import TemplateService
from utils.llm_client import get_llm_client

# å…¨å±€å˜é‡å­˜å‚¨graphçŠ¶æ€ï¼Œç”¨äºæ”¯æŒä¸­æ–­å’Œæ¢å¤
# æ³¨æ„: ç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redis ç­‰åˆ†å¸ƒå¼ç¼“å­˜æ›¿ä»£å†…å­˜å­˜å‚¨
graph_state_storage: Dict[str, Dict[str, Any]] = {}


# ==================== æ–‡æ¡£å»é‡å·¥å…·å‡½æ•° ====================


def normalize_text(text: str) -> str:
    """
    æ–‡æœ¬æ ‡å‡†åŒ–ï¼šå»é™¤HTML/Markdownæ ‡ç­¾ã€æ ‡ç‚¹ã€å¤šä½™ç©ºæ ¼ç­‰

    ç”¨äºåç»­çš„å“ˆå¸Œè®¡ç®—å’Œç›¸ä¼¼åº¦æ¯”å¯¹
    """
    if not text:
        return ""

    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r"<[^>]+>", "", text)
    # ç§»é™¤Markdownæ ‡é¢˜æ ‡è®°
    text = re.sub(r"^#+\s+", "", text, flags=re.MULTILINE)
    # ç§»é™¤Markdowné“¾æ¥
    text = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", text)
    # è½¬å°å†™
    text = text.lower()
    # æŠ˜å å¤šä½™ç©ºç™½ç¬¦
    text = re.sub(r"\s+", " ", text)
    # åªä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—
    text = re.sub(r"[^\w\u4e00-\u9fa5]+", "", text)

    return text.strip()


def compute_strong_hash(text: str) -> str:
    """
    è®¡ç®—æ–‡æœ¬çš„å¼ºå“ˆå¸Œå€¼ï¼ˆSHA256ï¼‰

    ç”¨äºæ£€æµ‹å®Œå…¨ç›¸åŒçš„æ–‡æ¡£
    """
    return hashlib.sha256(text.encode("utf-8")).hexdigest()


def compute_simhash(text: str, hashbits: int = 64) -> int:
    """
    è®¡ç®—SimHashï¼ˆå±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼‰

    ç”¨äºæ£€æµ‹é«˜åº¦ç›¸ä¼¼çš„æ–‡æ¡£
    ç®—æ³•ï¼šå¯¹æ–‡æœ¬åˆ†è¯åï¼Œä½¿ç”¨æ¯ä¸ªè¯çš„hashè¿›è¡ŒåŠ æƒæ±‚å’Œ
    """
    if not text:
        return 0

    # ç®€å•åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼ï¼‰
    tokens = text.split()
    if not tokens:
        return 0

    # åˆå§‹åŒ–ç‰¹å¾å‘é‡
    v = [0] * hashbits

    for token in tokens:
        # è®¡ç®—tokençš„hash
        h = int(hashlib.md5(token.encode("utf-8")).hexdigest(), 16)

        # å¯¹æ¯ä¸€ä½è¿›è¡ŒåŠ æƒ
        for i in range(hashbits):
            if h & (1 << i):
                v[i] += 1
            else:
                v[i] -= 1

    # ç”ŸæˆSimHashæŒ‡çº¹
    fingerprint = 0
    for i in range(hashbits):
        if v[i] > 0:
            fingerprint |= 1 << i

    return fingerprint


def hamming_distance(hash1: int, hash2: int) -> int:
    """
    è®¡ç®—ä¸¤ä¸ªSimHashçš„æ±‰æ˜è·ç¦»
    """
    x = hash1 ^ hash2
    distance = 0
    while x:
        distance += 1
        x &= x - 1  # æ¸…é™¤æœ€ä½ä½çš„1
    return distance


def compute_shingles(text: str, k: int = 5) -> Set[str]:
    """
    ç”Ÿæˆk-shinglesï¼ˆæ»‘åŠ¨çª—å£å­—ç¬¦ä¸²é›†åˆï¼‰

    ç”¨äºJaccardç›¸ä¼¼åº¦è®¡ç®—
    """
    if len(text) < k:
        return {text}

    shingles = set()
    for i in range(len(text) - k + 1):
        shingles.add(text[i: i + k])

    return shingles


def jaccard_similarity(set1: Set[str], set2: Set[str]) -> float:
    """
    è®¡ç®—Jaccardç›¸ä¼¼åº¦
    """
    if not set1 or not set2:
        return 0.0

    intersection = len(set1 & set2)
    union = len(set1 | set2)

    return intersection / union if union > 0 else 0.0


def should_remove_duplicate(
    doc_a: Dict[str, Any], doc_b: Dict[str, Any]
) -> Optional[int]:
    """
    åˆ¤æ–­ä¸¤ä¸ªæ–‡æ¡£æ˜¯å¦é‡å¤ï¼Œè¿”å›åº”è¯¥ç§»é™¤çš„æ–‡æ¡£ID

    è¿”å›å€¼ï¼š
    - None: ä¸é‡å¤
    - document_id: åº”è¯¥ç§»é™¤çš„æ–‡æ¡£IDï¼ˆä¿ç•™å†…å®¹æ›´é•¿ã€æ—¶é—´æ›´æ–°çš„ï¼‰

    Args:
        doc_a: æ–‡æ¡£Açš„dictï¼ŒåŒ…å« normalized, strong_hash, simhash, shingles, document_id, content
        doc_b: æ–‡æ¡£Bçš„dict
    """
    # é˜¶æ®µ1: å¼ºå“ˆå¸Œå®Œå…¨ç›¸åŒ
    if doc_a["strong_hash"] == doc_b["strong_hash"]:
        logger.debug(
            f"æ–‡æ¡£ {doc_a['document_id']} å’Œ {doc_b['document_id']} å¼ºå“ˆå¸Œç›¸åŒï¼ˆå®Œå…¨é‡å¤ï¼‰"
        )
        # ä¿ç•™å†…å®¹æ›´é•¿çš„
        if len(doc_a["content"]) < len(doc_b["content"]):
            return doc_a["document_id"]
        else:
            return doc_b["document_id"]

    # é˜¶æ®µ2: SimHashæ±‰æ˜è·ç¦»å¾ˆå°ï¼ˆé«˜åº¦ç›¸ä¼¼ï¼‰
    hamming_dist = hamming_distance(doc_a["simhash"], doc_b["simhash"])
    if hamming_dist <= 3:  # é˜ˆå€¼å¯è°ƒ
        logger.debug(
            f"æ–‡æ¡£ {doc_a['document_id']} å’Œ {doc_b['document_id']} SimHashè·ç¦»={hamming_dist}ï¼ˆé«˜åº¦ç›¸ä¼¼ï¼‰"
        )
        if len(doc_a["content"]) < len(doc_b["content"]):
            return doc_a["document_id"]
        else:
            return doc_b["document_id"]

    # é˜¶æ®µ3: Jaccardç›¸ä¼¼åº¦å¾ˆé«˜
    jac_sim = jaccard_similarity(doc_a["shingles"], doc_b["shingles"])
    if jac_sim > 0.75:  # é˜ˆå€¼å¯è°ƒ
        logger.debug(
            f"æ–‡æ¡£ {doc_a['document_id']} å’Œ {doc_b['document_id']} Jaccard={jac_sim:.3f}ï¼ˆå†…å®¹é‡å é«˜ï¼‰"
        )
        if len(doc_a["content"]) < len(doc_b["content"]):
            return doc_a["document_id"]
        else:
            return doc_b["document_id"]

    # é˜¶æ®µ4: åªå¯¹Jaccardåœ¨0.5-0.75ä¹‹é—´çš„åšç²¾ç»†difflibæ¯”å¯¹ï¼ˆé¿å…O(nÂ²)å¼€é”€ï¼‰
    if 0.5 < jac_sim <= 0.75:
        # difflibæ¯”å¯¹ï¼ˆè¾ƒæ…¢ï¼Œåªå¯¹å€™é€‰æ‰§è¡Œï¼‰
        ratio = SequenceMatcher(
            None, doc_a["normalized"], doc_b["normalized"]).ratio()
        if ratio > 0.80:  # é˜ˆå€¼å¯è°ƒ
            logger.debug(
                f"æ–‡æ¡£ {doc_a['document_id']} å’Œ {doc_b['document_id']} difflib={ratio:.3f}ï¼ˆç²¾ç»†æ¯”å¯¹é‡å¤ï¼‰"
            )
            if len(doc_a["content"]) < len(doc_b["content"]):
                return doc_a["document_id"]
            else:
                return doc_b["document_id"]

    return None


class RetrievalState(TypedDict):
    """
    ä¼˜åŒ–åçš„ RAG æ™ºèƒ½ä½“çŠ¶æ€æœº

    å·¥ä½œæµç¨‹:
    0. æ„å›¾è·¯ç”± -> LLM è‡ªä¸»è§„åˆ’æ•´ä¸ªæ‰§è¡Œæµç¨‹
    0.5. æ£€ç´¢å¢å¼º -> LLM è§£æç»“æ„åŒ–å­—æ®µå¹¶é‡å†™æŸ¥è¯¢ï¼ˆä»…æ–‡æ¡£æ£€ç´¢æ—¶ï¼‰
    1. ESå…¨æ–‡æ£€ç´¢ -> åŸºäºå…³é”®è¯å¿«é€Ÿå¬å›å€™é€‰æ–‡æ¡£
    2. SQLç»“æ„åŒ–æ£€ç´¢ -> åŸºäºæ¨¡æ¿å±‚çº§æå–ç»“æ„åŒ–æ¡ä»¶
    3. ç»“æœèåˆ -> åˆå¹¶ä¸¤è·¯æ£€ç´¢ç»“æœ
    4. ç²¾ç»†åŒ–ç­›é€‰ -> åŸºäºæ–‡æ¡£ç±»å‹ç‰¹å®šå­—æ®µè¿›ä¸€æ­¥ç­›é€‰
    5. ç”Ÿæˆç­”æ¡ˆ -> RAGç”Ÿæˆæœ€ç»ˆå›ç­”

    æ³¨æ„: db å’Œ es_client ä¸åœ¨ state ä¸­ï¼Œé€šè¿‡ config æ³¨å…¥
    """

    # === å¿…éœ€è¾“å…¥ ===
    query: str  # ç”¨æˆ·æŸ¥è¯¢
    template_id: int  # æ¨¡æ¿ID
    session_id: str  # ä¼šè¯ID

    # === èŠ‚ç‚¹ 0 (æ„å›¾è·¯ç”±) äº§å‡º ===
    execution_plan: List[Dict[str, Any]]  # LLM è§„åˆ’çš„æ‰§è¡Œè®¡åˆ’
    reasoning: str  # LLM çš„æ¨ç†è¿‡ç¨‹
    tool_results: List[Dict[str, Any]]  # å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨
    need_retrieval: bool  # æ˜¯å¦éœ€è¦æ–‡æ¡£æ£€ç´¢

    # === èŠ‚ç‚¹ 0.5 (æ£€ç´¢å¢å¼º) äº§å‡º ===
    parsed_fields: Dict[str, Any]  # LLM è§£æçš„ç»“æ„åŒ–å­—æ®µ
    rewritten_query: str  # LLM é‡å†™åçš„æŸ¥è¯¢

    # === èŠ‚ç‚¹ 1 (ESå…¨æ–‡æ£€ç´¢) äº§å‡º ===
    es_fulltext_results: List[Dict[str, Any]]  # ESå…¨æ–‡æ£€ç´¢çš„åˆæ­¥ç»“æœ
    es_document_ids: Set[int]  # ESå¬å›çš„æ–‡æ¡£IDé›†åˆ

    # === èŠ‚ç‚¹ 2 (SQLç»“æ„åŒ–æ£€ç´¢) äº§å‡º ===
    class_template_levels: Optional[List[Dict[str, Any]]]  # æ¨¡æ¿å±‚çº§å®šä¹‰
    category: str  # è¯†åˆ«å‡ºçš„æ–‡æ¡£ç±»åˆ«
    category_field_code: Optional[str]  # ç±»åˆ«å­—æ®µç¼–ç 
    sql_extracted_conditions: List[Dict[str, Any]]  # LLMæå–çš„ç»“æ„åŒ–æ¡ä»¶
    sql_document_ids: Set[int]  # SQLå¬å›çš„æ–‡æ¡£IDé›†åˆ

    # === èŠ‚ç‚¹ 3 (ç»“æœèåˆ) äº§å‡º ===
    merged_document_ids: List[int]  # èåˆåçš„æ–‡æ¡£IDåˆ—è¡¨(æŒ‰ç›¸å…³æ€§æ’åº)
    merged_documents: List[Document]  # èåˆåçš„æ–‡æ¡£å¯¹è±¡åˆ—è¡¨
    fusion_strategy: (
        str  # èåˆç­–ç•¥: 'intersection'(äº¤é›†), 'union'(å¹¶é›†), 'es_primary'(ESä¸ºä¸»)
    )

    # === èŠ‚ç‚¹ 4 (ç²¾ç»†åŒ–ç­›é€‰) äº§å‡º ===
    document_type_fields: List[DocumentTypeField]  # æ–‡æ¡£ç±»å‹ç‰¹å®šå­—æ®µ
    refined_conditions: Dict[str, Any]  # ç²¾ç»†åŒ–æŸ¥è¯¢æ¡ä»¶
    final_es_query: Optional[Dict[str, Any]]  # æœ€ç»ˆESæŸ¥è¯¢
    final_results: List[Dict[str, Any]]  # ç²¾ç»†åŒ–ç­›é€‰åçš„æœ€ç»ˆç»“æœ

    # === èŠ‚ç‚¹ 4.6 (æ‘˜è¦ç­›é€‰) äº§å‡º ===
    filtered_document_ids: List[int]  # LLMæ ¹æ®æ‘˜è¦ç­›é€‰åçš„æ–‡æ¡£IDåˆ—è¡¨
    filtered_results: List[Dict[str, Any]]  # ç­›é€‰åçš„æ–‡æ¡£ç»“æœ

    # === èŠ‚ç‚¹ 5 (æ­§ä¹‰å¤„ç†) äº§å‡º ===
    ambiguity_message: Optional[str]  # æ­§ä¹‰æç¤ºæ¶ˆæ¯

    # === èŠ‚ç‚¹ 6 (ç”Ÿæˆç­”æ¡ˆ) äº§å‡º ===
    answer: Optional[str]  # æœ€ç»ˆRAGç­”æ¡ˆ


# ==================== èŠ‚ç‚¹ 0: ä»»åŠ¡è§„åˆ’è·¯ç”± ====================
async def intent_routing(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 0: ä»»åŠ¡è§„åˆ’è·¯ç”±

    è®© LLM çœ‹åˆ°æ‰€æœ‰å·¥å…·ï¼Œè‡ªä¸»è§„åˆ’æ•´ä¸ªä»»åŠ¡çš„æ‰§è¡Œæµç¨‹ã€‚

    è¾“å‡º:
    - execution_plan: LLM è§„åˆ’çš„æ‰§è¡Œè®¡åˆ’
    - reasoning: LLM çš„æ¨ç†è¿‡ç¨‹
    - tool_results: å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨
    - need_retrieval: æ˜¯å¦éœ€è¦æ–‡æ¡£æ£€ç´¢
    """
    logger.info("========== èŠ‚ç‚¹ 0: ä»»åŠ¡è§„åˆ’è·¯ç”± ==========")

    # ä» config è·å– db
    db: AsyncSession = config.get("configurable", {}).get("db")  # type: ignore

    query = state["query"]
    template_id = state["template_id"]

    try:
        # è°ƒç”¨ Function Calling è·¯ç”±å™¨ï¼ˆç°åœ¨è¿”å›æ‰§è¡Œè®¡åˆ’ï¼‰
        routing_result = await function_calling_router(query, template_id, db)

        execution_plan = routing_result.get("execution_plan", [])
        reasoning = routing_result.get("reasoning", "")
        tool_results = routing_result.get("tool_results", [])
        need_retrieval = routing_result.get("need_retrieval", False)

        logger.info(f"ğŸ§  LLM è§„åˆ’ç»“æœ:")
        logger.info(f"   æ‰§è¡Œæ­¥éª¤: {len(execution_plan)}")
        logger.info(f"   å·¥å…·è°ƒç”¨: {len(tool_results)}")
        logger.info(f"   éœ€è¦æ£€ç´¢: {need_retrieval}")
        logger.info(f"   æ¨ç†è¿‡ç¨‹: {reasoning}")

        # æ‰“å°æ‰§è¡Œè®¡åˆ’
        for step in execution_plan:
            logger.info(
                f"   æ­¥éª¤ {step.get('step')}: {step.get('action')} - {step.get('description')}"
            )

        # æ›´æ–°çŠ¶æ€
        state["execution_plan"] = execution_plan
        state["reasoning"] = reasoning
        state["tool_results"] = tool_results
        state["need_retrieval"] = need_retrieval

        logger.info("âœ… ä»»åŠ¡è§„åˆ’å®Œæˆ")

    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡è§„åˆ’å¤±è´¥: {e}")
        import traceback

        logger.error(traceback.format_exc())
        # é»˜è®¤èµ°æ–‡æ¡£æ£€ç´¢
        state["execution_plan"] = [
            {"step": 1, "action": "document_retrieval", "description": "æ–‡æ¡£æ£€ç´¢"}
        ]
        state["reasoning"] = f"è§„åˆ’å¤±è´¥ï¼Œé™çº§åˆ°æ–‡æ¡£æ£€ç´¢: {str(e)}"
        state["tool_results"] = []
        state["need_retrieval"] = True

    return state


# ==================== èŠ‚ç‚¹: å·¥å…·è°ƒç”¨ç­”æ¡ˆç”Ÿæˆ ====================
async def generate_tool_answer(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    å·¥å…·è°ƒç”¨ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹

    å°†å¤šæ­¥éª¤å·¥å…·è°ƒç”¨ç»“æœæ ¼å¼åŒ–ä¸ºè‡ªç„¶è¯­è¨€ç­”æ¡ˆã€‚

    è¾“å‡º:
    - answer: æ ¼å¼åŒ–åçš„ç­”æ¡ˆï¼ˆå•çº¯å·¥å…·æŸ¥è¯¢ï¼‰
    - tool_answer_partial: éƒ¨åˆ†ç­”æ¡ˆï¼ˆç»„åˆæŸ¥è¯¢ï¼‰
    """
    logger.info("========== å·¥å…·è°ƒç”¨ç­”æ¡ˆç”Ÿæˆ ==========")

    # ä» config è·å– db
    db: AsyncSession = config.get("configurable", {}).get("db")  # type: ignore

    tool_results = state.get("tool_results", [])
    query = state["query"]
    need_retrieval = state.get("need_retrieval", False)
    execution_plan = state.get("execution_plan", [])

    try:
        # æ„å»ºå·¥å…·ç»“æœæ•°æ®
        combined_results = {
            "query": query,
            "execution_plan": execution_plan,
            "tool_results": tool_results,
        }

        # ä½¿ç”¨ LLM å°†å·¥å…·ç»“æœè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€
        tool_answer = await format_tool_result_as_answer(combined_results, query, db)

        # å¦‚æœæ˜¯ç»„åˆæŸ¥è¯¢ï¼Œä¿å­˜å·¥å…·ç­”æ¡ˆï¼Œä¸ç›´æ¥è®¾ç½®ä¸ºæœ€ç»ˆç­”æ¡ˆ
        if need_retrieval:
            state["tool_answer_partial"] = tool_answer  # ä¿å­˜éƒ¨åˆ†ç­”æ¡ˆ
            logger.info(
                f"âœ… ç”Ÿæˆå·¥å…·è°ƒç”¨éƒ¨åˆ†ç­”æ¡ˆï¼Œç­‰å¾…ç»§ç»­æ£€ç´¢: {tool_answer[:100]}..."
            )
        else:
            state["answer"] = tool_answer  # ç›´æ¥è®¾ç½®ä¸ºæœ€ç»ˆç­”æ¡ˆ
            logger.info(f"âœ… ç”Ÿæˆå·¥å…·è°ƒç”¨æœ€ç»ˆç­”æ¡ˆ: {tool_answer[:100]}...")
    except Exception as e:
        logger.error(f"âŒ æ ¼å¼åŒ–å·¥å…·ç»“æœå¤±è´¥: {e}")
        import traceback

        logger.error(traceback.format_exc())
        # é™çº§å¤„ç†
        fallback_answer = (
            f"æŸ¥è¯¢ç»“æœï¼š\n{json.dumps(tool_results, ensure_ascii=False, indent=2)}"
        )
        if need_retrieval:
            state["tool_answer_partial"] = fallback_answer
        else:
            state["answer"] = fallback_answer

    return state


# ==================== èŠ‚ç‚¹ 0.5: æ£€ç´¢å¢å¼ºï¼ˆQuery Enhancementï¼‰====================
async def enhance_retrieval_query(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 0.5: æ£€ç´¢å¢å¼º

    ä½¿ç”¨ LLM è§£æç”¨æˆ·æŸ¥è¯¢ä¸­çš„ç»“æ„åŒ–å­—æ®µï¼Œå¹¶é‡å†™æŸ¥è¯¢ä»¥æå‡æ£€ç´¢æ•ˆæœã€‚
    è¿™ä¸ªèŠ‚ç‚¹åªåœ¨éœ€è¦æ–‡æ¡£æ£€ç´¢æ—¶æ‰§è¡Œã€‚

    è¾“å‡º:
    - parsed_fields: LLM è§£æå‡ºçš„ç»“æ„åŒ–å­—æ®µï¼ˆå­—æ®µå -> {value, hierarchyç­‰}ï¼‰
    - rewritten_query: LLM é‡å†™åçš„æŸ¥è¯¢ï¼ˆå»é™¤ç»“æ„åŒ–ä¿¡æ¯ï¼Œä¿ç•™è¯­ä¹‰å…³é”®è¯ï¼‰
    """
    logger.info("========== èŠ‚ç‚¹ 0.5: æ£€ç´¢å¢å¼º ==========")

    # ä» config è·å– db
    db: AsyncSession = config.get("configurable", {}).get("db")  # type: ignore

    query = state["query"]
    template_id = state["template_id"]

    # åˆå§‹åŒ–é»˜è®¤å€¼
    state["parsed_fields"] = {}
    state["rewritten_query"] = query  # é»˜è®¤ä½¿ç”¨åŸå§‹æŸ¥è¯¢

    try:
        # 1. è·å–æ¨¡æ¿å±‚çº§å®šä¹‰
        cls_template = await TemplateService.get_template(db, template_id)

        if not cls_template:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°æ¨¡æ¿ï¼Œè·³è¿‡æ£€ç´¢å¢å¼º")
            return state

        cls_template_levels = cls_template.levels
        if not isinstance(cls_template_levels, list) or not cls_template_levels:
            logger.warning("âš ï¸ æ¨¡æ¿å±‚çº§å®šä¹‰ä¸ºç©ºï¼Œè·³è¿‡æ£€ç´¢å¢å¼º")
            return state

        # 2. æ„é€  LLM Prompt
        # ç®€åŒ–æ¨¡æ¿å±‚çº§å®šä¹‰ï¼Œåªä¿ç•™å…³é”®ä¿¡æ¯
        simplified_levels = []
        for field in cls_template_levels:
            simplified_levels.append(
                {
                    "code": field.get("code"),
                    "name": field.get("name"),
                    "level": field.get("level"),
                    "is_doc_type": field.get("is_doc_type", False),
                }
            )

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ£€ç´¢å¢å¼ºåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ä»ç”¨æˆ·æŸ¥è¯¢ä¸­è¯†åˆ«å¹¶æå–ç»“æ„åŒ–å­—æ®µï¼ˆç”¨äºç²¾ç¡®ç­›é€‰ï¼‰
2. ç”Ÿæˆä¸€ä¸ªå¢å¼ºçš„æ£€ç´¢æŸ¥è¯¢ï¼Œ**ä¿ç•™æ‰€æœ‰è¯­ä¹‰ä¿¡æ¯å’Œç»†èŠ‚**ï¼ŒåŒæ—¶æ‰©å±•åŒä¹‰è¯å’Œç›¸å…³è¯æ±‡

ã€æ¨¡æ¿å­—æ®µå®šä¹‰ã€‘
{json.dumps(simplified_levels, ensure_ascii=False, indent=2)}

ã€ç”¨æˆ·æŸ¥è¯¢ã€‘
{query}

ã€è¾“å‡ºè¦æ±‚ã€‘
è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "fields": {{
        "å­—æ®µç¼–ç ": {{
            "value": "å­—æ®µå€¼æˆ–å€¼åˆ—è¡¨",
            "hierarchy": ["å±‚çº§1", "å±‚çº§2"]
        }}
    }},
    "query_rewrite": "å¢å¼ºåçš„æ£€ç´¢æŸ¥è¯¢"
}}

ã€ç¤ºä¾‹1ã€‘
ç”¨æˆ·æŸ¥è¯¢: "æ±Ÿè‹çœè‹å·å¸‚2023å¹´å°±ä¸šå¸®æ‰¶æ”¿ç­–"
è¾“å‡º:
{{
    "fields": {{
        "YEAR": {{"value": "2023"}},
        "REGION": {{"value": ["æ±Ÿè‹çœ", "è‹å·å¸‚"], "hierarchy": ["çœ", "å¸‚"]}}
    }},
    "query_rewrite": "æ±Ÿè‹çœ è‹å·å¸‚ 2023å¹´ å°±ä¸šå¸®æ‰¶æ”¿ç­– å°±ä¸šä¿ƒè¿› å°±ä¸šæ”¯æŒ å°±ä¸šæ´åŠ© å°±ä¸šæœåŠ¡ å°±ä¸šæ‰¶æŒ æƒ æ°‘æ”¿ç­–"
}}

ã€ç¤ºä¾‹2ã€‘
ç”¨æˆ·æŸ¥è¯¢: "ä¸­å›½å‘ç”Ÿè¿‡å“ªäº›å¤§å‹åœ°éœ‡"
è¾“å‡º:
{{
    "fields": {{}},
    "query_rewrite": "ä¸­å›½ å¤§å‹åœ°éœ‡ åœ°éœ‡çµå®³ åœ°éœ‡å†å² é‡å¤§åœ°éœ‡ ç ´åæ€§åœ°éœ‡ åœ°éœ‡äº‹ä»¶ éœ‡çº§ ä¼¤äº¡ ç¾æƒ…"
}}

ã€ç¤ºä¾‹3ã€‘
ç”¨æˆ·æŸ¥è¯¢: "åœ°éœ‡åº”æ€¥é¢„æ¡ˆçš„å…·ä½“æªæ–½"
è¾“å‡º:
{{
    "fields": {{}},
    "query_rewrite": "åœ°éœ‡åº”æ€¥é¢„æ¡ˆ å…·ä½“æªæ–½ åº”å¯¹æªæ–½ é˜²ç¾å‡ç¾ æ•‘æ´æªæ–½ åº”æ€¥å¤„ç½® é¢„é˜²æªæ–½ æ•‘ç¾æµç¨‹ åº”æ€¥å“åº” é˜²éœ‡å‡†å¤‡"
}}

ã€é‡è¦åŸåˆ™ã€‘
1. **ä¿ç•™åŸå§‹ä¿¡æ¯**ï¼šquery_rewrite å¿…é¡»åŒ…å«ç”¨æˆ·æŸ¥è¯¢ä¸­çš„**æ‰€æœ‰å…³é”®ä¿¡æ¯**ï¼Œä¸è¦åˆ é™¤ä»»ä½•è¯­ä¹‰ç»†èŠ‚
2. **æ‰©å±•åŒä¹‰è¯**ï¼šæ·»åŠ ç›¸å…³çš„åŒä¹‰è¯ã€è¿‘ä¹‰è¯ã€ä¸Šä¸‹ä½è¯ï¼Œå¢åŠ å¬å›ç‡
3. **ä¿ç•™ç»“æ„åŒ–ä¿¡æ¯**ï¼šåœ°åŒºã€æ—¶é—´ã€æœºæ„ç­‰ç»“æ„åŒ–ä¿¡æ¯è¦ä¿ç•™åœ¨query_rewriteä¸­
4. **fieldsæå–**ï¼šåªæå–èƒ½åŒ¹é…æ¨¡æ¿å­—æ®µå®šä¹‰çš„ç»“æ„åŒ–ä¿¡æ¯
5. **é¿å…ä¸¢å¤±ç»†èŠ‚**ï¼šä¸è¦ç®€åŒ–æˆ–æ¦‚æ‹¬ç”¨æˆ·çš„æŸ¥è¯¢æ„å›¾ï¼Œä¿æŒå…·ä½“æ€§
6. **å¢å¼ºæ£€ç´¢**ï¼šé€šè¿‡æ·»åŠ ç›¸å…³è¯æ±‡ï¼Œæé«˜æ£€ç´¢çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§

è¯·ç›´æ¥è¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""

        # 3. è°ƒç”¨ LLM
        logger.info("ğŸ¤– è°ƒç”¨ LLM è¿›è¡Œæ£€ç´¢å¢å¼º...")
        llm_client = get_llm_client()
        llm_response = await llm_client.extract_json_response(prompt, db=db)

        logger.info(
            f"ğŸ“Š LLM è§£æç»“æœ: {json.dumps(llm_response, ensure_ascii=False)}")

        # 4. æå–ç»“æœ
        parsed_fields = llm_response.get("fields", {})
        rewritten_query = llm_response.get("query_rewrite", query)

        # éªŒè¯é‡å†™æŸ¥è¯¢ä¸ä¸ºç©º
        if not rewritten_query or not rewritten_query.strip():
            logger.warning("âš ï¸ LLM é‡å†™æŸ¥è¯¢ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
            rewritten_query = query

        # 5. æ›´æ–°çŠ¶æ€
        state["parsed_fields"] = parsed_fields
        state["rewritten_query"] = rewritten_query

        logger.info(
            f"âœ… æ£€ç´¢å¢å¼ºå®Œæˆ: æå– {len(parsed_fields)} ä¸ªå­—æ®µï¼Œé‡å†™æŸ¥è¯¢: '{rewritten_query}'"
        )

    except Exception as e:
        logger.error(f"âŒ æ£€ç´¢å¢å¼ºå¤±è´¥: {e}")
        import traceback

        logger.error(traceback.format_exc())
        # å¤±è´¥æ—¶ä½¿ç”¨åŸå§‹æŸ¥è¯¢
        state["parsed_fields"] = {}
        state["rewritten_query"] = query

    return state


# ==================== èŠ‚ç‚¹ 1: ES å…¨æ–‡æ£€ç´¢ ====================
async def es_fulltext_retrieval(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 1: ES å…¨æ–‡æ£€ç´¢

    åŸºäºç”¨æˆ·æŸ¥è¯¢åœ¨ Elasticsearch ä¸­è¿›è¡Œå…¨æ–‡æ£€ç´¢,å¿«é€Ÿå¬å›å€™é€‰æ–‡æ¡£ã€‚
    è¿™æ˜¯ç¬¬ä¸€é˜¶æ®µçš„ç²—å¬å›,åˆ©ç”¨ ES çš„å…¨æ–‡æœç´¢èƒ½åŠ›ã€‚

    è¾“å‡º:
    - es_fulltext_results: ES æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
    - es_document_ids: æ–‡æ¡£ ID é›†åˆ
    """
    logger.info("========== èŠ‚ç‚¹ 1: ES å…¨æ–‡æ£€ç´¢ ==========")

    # ä» config è·å– es_client å’Œ es_index
    es_client: AsyncElasticsearch = config.get("configurable", {}).get(
        "es"
    )  # type: ignore
    es_index: str = config.get("configurable", {}).get(
        "es_index", "dochive_documents"
    )  # type: ignore

    # ä½¿ç”¨é‡å†™åçš„æŸ¥è¯¢ï¼ˆå¦‚æœæœ‰ï¼‰
    query = state.get("rewritten_query", state["query"])
    template_id = state["template_id"]

    logger.info(f"ğŸ” ä½¿ç”¨æŸ¥è¯¢: '{query}'")

    # æ„é€  ES å…¨æ–‡æ£€ç´¢æŸ¥è¯¢
    es_query = {
        "query": {
            "bool": {
                "must": {
                    "multi_match": {
                        "query": query,
                        "fields": ["title^3", "content"],  # title æƒé‡æ›´é«˜
                        "type": "best_fields",
                        "fuzziness": "AUTO",  # æ”¯æŒæ¨¡ç³ŠåŒ¹é…
                    }
                },
                "filter": [{"term": {"template_id": template_id}}],  # é™å®šæ¨¡æ¿èŒƒå›´
            }
        },
        "size": 20,  # å¬å› Top 20
        "_source": ["document_id", "title", "content", "metadata"],
    }

    try:
        response = await es_client.search(index=es_index, body=es_query)

        hits = response.get("hits", {}).get("hits", [])
        state["es_fulltext_results"] = [hit["_source"] for hit in hits]
        state["es_document_ids"] = set(
            hit["_source"]["document_id"] for hit in hits)

        logger.info(f"âœ… ES å…¨æ–‡æ£€ç´¢å¬å› {len(hits)} ç¯‡æ–‡æ¡£")
        logger.info(f"   æ–‡æ¡£ ID: {list(state['es_document_ids'])}")

    except Exception as e:
        logger.error(f"âŒ ES å…¨æ–‡æ£€ç´¢å¤±è´¥: {e}")
        state["es_fulltext_results"] = []
        state["es_document_ids"] = set()

    return state


# ==================== èŠ‚ç‚¹ 2: SQL ç»“æ„åŒ–æ£€ç´¢ ====================
async def sql_structured_retrieval(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 2: SQL ç»“æ„åŒ–æ£€ç´¢

    åŸºäºæ¨¡æ¿å±‚çº§å®šä¹‰,ä½¿ç”¨ LLM æå–ç»“æ„åŒ–æŸ¥è¯¢æ¡ä»¶,
    åœ¨æ•°æ®åº“ä¸­è¿›è¡Œç²¾ç¡®çš„ç»“æ„åŒ–æ£€ç´¢ã€‚

    è¾“å‡º:
    - class_template_levels: æ¨¡æ¿å±‚çº§å®šä¹‰
    - category: æ–‡æ¡£ç±»åˆ«
    - sql_extracted_conditions: æå–çš„ç»“æ„åŒ–æ¡ä»¶
    - sql_document_ids: SQL å¬å›çš„æ–‡æ¡£ ID é›†åˆ
    """
    logger.info("========== èŠ‚ç‚¹ 2: SQL ç»“æ„åŒ–æ£€ç´¢ ==========")

    # ä» config è·å– db
    db: AsyncSession = config.get("configurable", {}).get("db")  # type: ignore

    # 1. è·å–æ¨¡æ¿å±‚çº§å®šä¹‰
    cls_template = await TemplateService.get_template(db, state["template_id"])

    if not cls_template:
        logger.warning("âš ï¸ æœªæ‰¾åˆ°æ¨¡æ¿,è·³è¿‡ SQL ç»“æ„åŒ–æ£€ç´¢")
        state["class_template_levels"] = []
        state["category"] = "*"
        state["sql_extracted_conditions"] = []
        state["sql_document_ids"] = set()
        return state

    # ä½¿ç”¨ property è·å–å±‚çº§å®šä¹‰ (è‡ªåŠ¨å¤„ç† JSON è½¬æ¢)
    cls_template_levels = cls_template.levels
    if not isinstance(cls_template_levels, list):
        logger.error("âŒ æ¨¡æ¿å±‚çº§å®šä¹‰æ ¼å¼é”™è¯¯")
        state["class_template_levels"] = []
        state["category"] = "*"
        state["sql_extracted_conditions"] = []
        state["sql_document_ids"] = set()
        return state

    state["class_template_levels"] = cls_template_levels

    # 2. æå–ç±»åˆ«å­—æ®µ
    type_code = ""
    for field in cls_template_levels:
        if field.get("is_doc_type", False):
            type_code = field.get("code", "")
            state["category_field_code"] = type_code
            break

    # 3. ä½¿ç”¨ LLM æå–ç»“æ„åŒ–æ¡ä»¶
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç»“æ„åŒ–æŸ¥è¯¢åŠ©æ‰‹ã€‚
ç”¨æˆ·ä¼šç»™å‡ºä¸€ä¸ªè‡ªç„¶è¯­è¨€æ£€ç´¢è¯·æ±‚,è¯·ä½ æ ¹æ®ä»¥ä¸‹å­—æ®µå®šä¹‰,æå–å‡ºç»“æ„åŒ–çš„æ£€ç´¢æ¡ä»¶ã€‚

å­—æ®µå®šä¹‰:
{json.dumps(cls_template_levels, ensure_ascii=False, indent=2)}

è¦æ±‚:
1. è¾“å‡º JSON å¯¹è±¡,æ ¼å¼: {{"conditions": [{{"code": "å­—æ®µç¼–ç ", "value": "æå–å€¼", "level": å±‚çº§}}], "category": "æ–‡æ¡£ç±»åˆ«"}}
2. å¦‚æœæ— æ³•ä»æŸ¥è¯¢ä¸­æ¨ç†å‡ºæŸä¸ªå­—æ®µ,value è®¾ä¸º "UNKNOWN"
3. category å­—æ®µåº”è¯¥æ˜¯ is_doc_type=true çš„å­—æ®µçš„å€¼
4. åªæå–ç”¨æˆ·æ˜ç¡®æåˆ°çš„ä¿¡æ¯,ä¸è¦çŒœæµ‹

ç”¨æˆ·æŸ¥è¯¢:
{state['query']}

è¯·ç›´æ¥è¾“å‡º JSON,ä¸è¦è§£é‡Šã€‚
    """

    try:
        llm_client = get_llm_client()
        llm_response = await llm_client.extract_json_response(prompt, db=db)
        logger.info(f"ğŸ¤– LLM æå–çš„ç»“æ„åŒ–æ¡ä»¶: {llm_response}")

        conditions = llm_response.get("conditions", [])
        state["category"] = llm_response.get("category", "*")
        state["sql_extracted_conditions"] = conditions

    except Exception as e:
        logger.error(f"âŒ LLM æå–ç»“æ„åŒ–æ¡ä»¶å¤±è´¥: {e}")
        state["category"] = "*"
        state["sql_extracted_conditions"] = []

    # 4. æ„é€  SQL æŸ¥è¯¢æ¡ä»¶
    # ä¼˜å…ˆä½¿ç”¨æ£€ç´¢å¢å¼ºèŠ‚ç‚¹è§£æçš„å­—æ®µ
    parsed_fields = state.get("parsed_fields", {})
    conditions_clauses = []

    if parsed_fields:
        # ä½¿ç”¨æ£€ç´¢å¢å¼ºèŠ‚ç‚¹è§£æçš„ç»“æ„åŒ–å­—æ®µ
        logger.info(f"ğŸ“Š ä½¿ç”¨æ£€ç´¢å¢å¼ºå­—æ®µæ„é€ SQL: {parsed_fields}")
        for field_code, field_data in parsed_fields.items():
            value = field_data.get("value")
            if value:
                if isinstance(value, list):
                    for v in value:
                        conditions_clauses.append(
                            TemplateDocumentMapping.class_code.like(f"%{v}%")
                        )
                else:
                    conditions_clauses.append(
                        TemplateDocumentMapping.class_code.like(f"%{value}%")
                    )
    else:
        # é™çº§ï¼šä½¿ç”¨æ—§çš„ LLM æå–é€»è¾‘
        logger.info("ğŸ“Œ æœªä½¿ç”¨æ£€ç´¢å¢å¼ºï¼Œä½¿ç”¨ä¼ ç»Ÿç»“æ„åŒ–æ¡ä»¶æå–")
        for cond in state["sql_extracted_conditions"]:
            value = cond.get("value")
            if value and value != "UNKNOWN":
                if isinstance(value, list):
                    for v in value:
                        conditions_clauses.append(
                            TemplateDocumentMapping.class_code.like(f"%{v}%")
                        )
                else:
                    conditions_clauses.append(
                        TemplateDocumentMapping.class_code.like(f"%{value}%")
                    )

    # 5. æ‰§è¡Œ SQL æŸ¥è¯¢
    stmt = select(TemplateDocumentMapping.document_id).where(
        TemplateDocumentMapping.template_id == state["template_id"]
    )
    if conditions_clauses:
        stmt = stmt.where(or_(*conditions_clauses))

    try:
        result = await db.execute(stmt)
        document_ids = [row[0] for row in result.all()]
        state["sql_document_ids"] = set(document_ids)

        logger.info(f"âœ… SQL ç»“æ„åŒ–æ£€ç´¢å¬å› {len(document_ids)} ç¯‡æ–‡æ¡£")
        logger.info(f"   æ–‡æ¡£ ID: {list(state['sql_document_ids'])}")

    except Exception as e:
        logger.error(f"âŒ SQL æŸ¥è¯¢å¤±è´¥: {e}")
        state["sql_document_ids"] = set()

    return state


# ==================== èŠ‚ç‚¹ 3: ç»“æœèåˆ ====================
async def merge_retrieval_results(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 3: ç»“æœèåˆ

    å°† ES å…¨æ–‡æ£€ç´¢å’Œ SQL ç»“æ„åŒ–æ£€ç´¢çš„ç»“æœè¿›è¡Œèåˆ,
    é‡‡ç”¨æ™ºèƒ½ç­–ç•¥å†³å®šå¦‚ä½•åˆå¹¶ä¸¤è·¯å¬å›ç»“æœã€‚

    èåˆç­–ç•¥:
    1. 'intersection': å–äº¤é›† (ç²¾ç¡®åŒ¹é…)
    2. 'union': å–å¹¶é›† (å¹¿æ³›å¬å›)
    3. 'es_primary': ESä¸ºä¸»,SQLä¸ºè¾…åŠ©ç­›é€‰

    è¾“å‡º:
    - merged_document_ids: èåˆåçš„æ–‡æ¡£ ID åˆ—è¡¨
    - merged_documents: èåˆåçš„æ–‡æ¡£å¯¹è±¡
    - fusion_strategy: ä½¿ç”¨çš„èåˆç­–ç•¥
    """
    logger.info("========== èŠ‚ç‚¹ 3: ç»“æœèåˆ ==========")

    # ä» config è·å– db
    db: AsyncSession = config.get("configurable", {}).get("db")  # type: ignore

    es_ids = state.get("es_document_ids", set())
    sql_ids = state.get("sql_document_ids", set())

    logger.info(f"ğŸ“Š ES å¬å›: {len(es_ids)} ç¯‡, SQL å¬å›: {len(sql_ids)} ç¯‡")

    # å†³å®šèåˆç­–ç•¥
    if not es_ids and not sql_ids:
        # ä¸¤è·¯éƒ½æ²¡å¬å›
        logger.warning("âš ï¸ ES å’Œ SQL éƒ½æœªå¬å›ä»»ä½•æ–‡æ¡£")
        state["fusion_strategy"] = "none"
        state["merged_document_ids"] = []
        state["merged_documents"] = []
        return state

    elif not sql_ids:
        # åªæœ‰ ES å¬å›äº†
        logger.info("ğŸ“Œ ç­–ç•¥: ESä¸ºä¸» (SQLæœªå¬å›)")
        state["fusion_strategy"] = "es_only"
        merged_ids = list(es_ids)

    elif not es_ids:
        # åªæœ‰ SQL å¬å›äº†
        logger.info("ğŸ“Œ ç­–ç•¥: SQLä¸ºä¸» (ESæœªå¬å›)")
        state["fusion_strategy"] = "sql_only"
        merged_ids = list(sql_ids)

    else:
        # ä¸¤è·¯éƒ½å¬å›äº†,ä½¿ç”¨æ™ºèƒ½èåˆç­–ç•¥
        intersection = es_ids & sql_ids
        union = es_ids | sql_ids

        if len(intersection) >= 3:
            # äº¤é›†è¶³å¤Ÿå¤š,ä½¿ç”¨äº¤é›† (é«˜ç²¾åº¦)
            logger.info(f"ğŸ“Œ ç­–ç•¥: äº¤é›† (å…± {len(intersection)} ç¯‡æ–‡æ¡£)")
            state["fusion_strategy"] = "intersection"
            merged_ids = list(intersection)

        elif len(intersection) > 0:
            # äº¤é›†è¾ƒå°‘,ESä¸ºä¸»,SQLä¸ºè¾…
            logger.info(f"ğŸ“Œ ç­–ç•¥: ESä¸ºä¸»,SQLè¾…åŠ© (äº¤é›† {len(intersection)} ç¯‡)")
            state["fusion_strategy"] = "es_primary"
            # ES ç»“æœåœ¨å‰,äº¤é›†ä¼˜å…ˆ,ç„¶åæ˜¯ ES ç‹¬æœ‰
            merged_ids = list(intersection) + [
                id for id in es_ids if id not in intersection
            ]

        else:
            # æ²¡æœ‰äº¤é›†,å–å¹¶é›†
            logger.info(f"ğŸ“Œ ç­–ç•¥: å¹¶é›† (ES {len(es_ids)} + SQL {len(sql_ids)})")
            state["fusion_strategy"] = "union"
            merged_ids = list(es_ids) + \
                [id for id in sql_ids if id not in es_ids]

    # é™åˆ¶ç»“æœæ•°é‡ (Top 10)
    merged_ids = merged_ids[:10]
    state["merged_document_ids"] = merged_ids

    # ä»æ•°æ®åº“åŠ è½½æ–‡æ¡£å¯¹è±¡
    if merged_ids:
        try:
            docs_result = await db.execute(
                select(Document).where(Document.id.in_(merged_ids))
            )
            docs = list(docs_result.scalars().all())

            # æŒ‰ merged_ids çš„é¡ºåºæ’åº
            docs_dict = {int(doc.id): doc for doc in docs}  # type: ignore
            state["merged_documents"] = [
                docs_dict[doc_id] for doc_id in merged_ids if doc_id in docs_dict
            ]

            logger.info(f"âœ… èåˆå®Œæˆ,æœ€ç»ˆä¿ç•™ {len(state['merged_documents'])} ç¯‡æ–‡æ¡£")

        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ–‡æ¡£å¯¹è±¡å¤±è´¥: {e}")
            state["merged_documents"] = []
    else:
        state["merged_documents"] = []

    return state


# ==================== èŠ‚ç‚¹ 4: ç²¾ç»†åŒ–ç­›é€‰ ====================
async def refined_filtering(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 4: ç²¾ç»†åŒ–ç­›é€‰

    åŸºäºæ–‡æ¡£ç±»å‹çš„ç‰¹å®šå­—æ®µ (DocumentTypeField),
    ä½¿ç”¨ LLM æå–æ›´ç²¾ç»†çš„æŸ¥è¯¢æ¡ä»¶,åœ¨ ES ä¸­è¿›è¡ŒäºŒæ¬¡ç­›é€‰ã€‚

    è¾“å‡º:
    - document_type_fields: æ–‡æ¡£ç±»å‹ç‰¹å®šå­—æ®µ
    - refined_conditions: ç²¾ç»†åŒ–æ¡ä»¶
    - final_es_query: æœ€ç»ˆ ES æŸ¥è¯¢
    - final_results: æœ€ç»ˆæ£€ç´¢ç»“æœ
    """
    logger.info("========== èŠ‚ç‚¹ 4: ç²¾ç»†åŒ–ç­›é€‰ ==========")

    # ä» config è·å– db å’Œ es_client
    db: AsyncSession = config.get("configurable", {}).get("db")  # type: ignore
    es_client: AsyncElasticsearch = config.get("configurable", {}).get(
        "es"
    )  # type: ignore

    # å¦‚æœæ²¡æœ‰èåˆç»“æœ,ç›´æ¥è·³è¿‡
    if not state.get("merged_documents"):
        logger.warning("âš ï¸ æ— èåˆç»“æœ,è·³è¿‡ç²¾ç»†åŒ–ç­›é€‰")
        state["document_type_fields"] = []
        state["refined_conditions"] = {}
        state["final_es_query"] = None
        state["final_results"] = []
        return state

    # å¦‚æœç±»åˆ«ä¸ºé€šé…ç¬¦,è·³è¿‡ç²¾ç»†åŒ–ç­›é€‰
    category = state.get("category", "*")
    if category == "*":
        logger.info("ğŸ“Œ ç±»åˆ«ä¸ºé€šé…ç¬¦,è·³è¿‡ç²¾ç»†åŒ–ç­›é€‰,ç›´æ¥ä½¿ç”¨èåˆç»“æœ")
        state["document_type_fields"] = []
        state["refined_conditions"] = {}
        state["final_es_query"] = None
        state["final_results"] = _convert_docs_to_results(
            state["merged_documents"])
        return state

    # 1. è·å– DocumentType å’Œ DocumentTypeField
    try:
        doc_types_result = await db.execute(
            select(DocumentType).where(
                DocumentType.template_id == state["template_id"],
                DocumentType.type_code == category,
            )
        )
        doc_types = doc_types_result.scalars().all()

        if not doc_types:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç±»åˆ« '{category}' çš„ DocumentType,è·³è¿‡ç²¾ç»†åŒ–ç­›é€‰")
            state["document_type_fields"] = []
            state["refined_conditions"] = {}
            state["final_results"] = _convert_docs_to_results(
                state["merged_documents"])
            return state

        document_type_fields_result = await db.execute(
            select(DocumentTypeField).where(
                DocumentTypeField.doc_type_id.in_([dt.id for dt in doc_types])
            )
        )
        document_type_fields = list(
            document_type_fields_result.scalars().all())
        state["document_type_fields"] = document_type_fields

        if not document_type_fields:
            logger.info("ğŸ“Œ è¯¥ç±»åˆ«æ— ç‰¹å®šå­—æ®µ,è·³è¿‡ç²¾ç»†åŒ–ç­›é€‰")
            state["refined_conditions"] = {}
            state["final_results"] = _convert_docs_to_results(
                state["merged_documents"])
            return state

    except Exception as e:
        logger.error(f"âŒ è·å–æ–‡æ¡£ç±»å‹å­—æ®µå¤±è´¥: {e}")
        state["document_type_fields"] = []
        state["refined_conditions"] = {}
        state["final_results"] = _convert_docs_to_results(
            state["merged_documents"])
        return state

    # 2. ä½¿ç”¨ LLM æå–ç²¾ç»†åŒ–æ¡ä»¶
    field_definitions = ""
    field_map = {}  # å­—æ®µå -> å­—æ®µç±»å‹

    for f in document_type_fields:
        field_definitions += (
            f"- {f.field_name}: {f.description} (ç±»å‹: {f.field_type})\n"
        )
        field_map[f.field_name] = f.field_type

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç²¾ç»†åŒ–æŸ¥è¯¢åŠ©æ‰‹ã€‚
ç”¨æˆ·æ­£åœ¨æŸ¥è¯¢ç±»åˆ«ä¸º '{category}' çš„æ–‡æ¡£,è¯¥ç±»åˆ«æœ‰ä»¥ä¸‹ç‰¹å®šå­—æ®µ:

{field_definitions}

è¯·æ ¹æ®ç”¨æˆ·æŸ¥è¯¢æå–è¿™äº›å­—æ®µçš„å…·ä½“å€¼:

è¦æ±‚:
1. è¾“å‡º JSON å¯¹è±¡: {{"conditions": {{"å­—æ®µå": "å€¼"}}, "missing_fields": ["ç¼ºå¤±å­—æ®µ"]}}
2. åªæå–ç”¨æˆ·æ˜ç¡®æåˆ°çš„å­—æ®µå€¼
3. missing_fields åˆ—å‡ºå¯¹ç²¾ç¡®æ£€ç´¢æœ‰å¸®åŠ©ä½†ç”¨æˆ·æœªæä¾›çš„å­—æ®µ

ç”¨æˆ·æŸ¥è¯¢:
{state['query']}

è¯·ç›´æ¥è¾“å‡º JSON,ä¸è¦è§£é‡Šã€‚
    """
    llm_client = get_llm_client()

    try:
        llm_response = await llm_client.extract_json_response(prompt, db=db)
        logger.info(f"ğŸ¤– LLM æå–çš„ç²¾ç»†åŒ–æ¡ä»¶: {llm_response}")

        conditions = llm_response.get("conditions", {})
        missing_fields = llm_response.get("missing_fields", [])

        state["refined_conditions"] = conditions

        # 3. æ£€æŸ¥æ­§ä¹‰
        if not conditions and missing_fields:
            missing_str = "ã€".join(missing_fields)
            state["ambiguity_message"] = (
                f"æ‚¨çš„é—®é¢˜ä¼¼ä¹æœ‰äº›å®½æ³›ã€‚ä¸ºäº†æ›´ç²¾ç¡®åœ°æŸ¥æ‰¾,èƒ½å¦æä¾›: {missing_str}?"
            )
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ­§ä¹‰,å»ºè®®è¡¥å……: {missing_str}")
            state["final_results"] = _convert_docs_to_results(
                state["merged_documents"])
            return state

    except Exception as e:
        logger.error(f"âŒ LLM æå–ç²¾ç»†åŒ–æ¡ä»¶å¤±è´¥: {e}")
        state["refined_conditions"] = {}
        state["final_results"] = _convert_docs_to_results(
            state["merged_documents"])
        return state

    # 4. æ„é€ ç²¾ç»†åŒ– ES æŸ¥è¯¢
    if not state["refined_conditions"]:
        logger.info("ğŸ“Œ æ— ç²¾ç»†åŒ–æ¡ä»¶,ç›´æ¥ä½¿ç”¨èåˆç»“æœ")
        state["final_es_query"] = None
        state["final_results"] = _convert_docs_to_results(
            state["merged_documents"])
        return state

    # åªåœ¨èåˆåçš„æ–‡æ¡£ä¸­ç­›é€‰
    merged_doc_ids = state["merged_document_ids"]

    must_clauses = []
    for field_name, value in state["refined_conditions"].items():
        if not value or value == "UNKNOWN":
            continue

        field_type = field_map.get(field_name, "text")

        # æ ¹æ®å­—æ®µç±»å‹æ„é€ æŸ¥è¯¢
        if field_type in ["text", "textarea"]:
            must_clauses.append({"match": {f"metadata.{field_name}": value}})
        elif field_type == "number":
            must_clauses.append({"term": {f"metadata.{field_name}": value}})
        elif field_type == "date":
            must_clauses.append(
                {"range": {f"metadata.{field_name}": {"gte": value}}})
        else:
            must_clauses.append({"term": {f"metadata.{field_name}": value}})

    final_es_query = {
        "query": {
            "bool": {
                "must": must_clauses,
                "filter": [{"terms": {"document_id": merged_doc_ids}}],
            }
        },
        "size": 5,
        "_source": ["document_id", "title", "content", "metadata"],
    }

    state["final_es_query"] = final_es_query

    # 5. æ‰§è¡Œç²¾ç»†åŒ– ES æŸ¥è¯¢
    try:
        # ä» config è·å– es_index
        es_index: str = config.get("configurable", {}).get(
            "es_index", "dochive_documents"
        )  # type: ignore
        response = await es_client.search(index=es_index, body=final_es_query)

        hits = response.get("hits", {}).get("hits", [])
        state["final_results"] = [hit["_source"] for hit in hits]

        logger.info(f"âœ… ç²¾ç»†åŒ–ç­›é€‰å®Œæˆ,ä¿ç•™ {len(hits)} ç¯‡æ–‡æ¡£")

    except Exception as e:
        logger.error(f"âŒ ç²¾ç»†åŒ– ES æŸ¥è¯¢å¤±è´¥: {e}")
        # é™çº§: ä½¿ç”¨èåˆç»“æœ
        state["final_results"] = _convert_docs_to_results(
            state["merged_documents"])

    return state


def _convert_docs_to_results(documents: List[Document]) -> List[Dict[str, Any]]:
    """
    è¾…åŠ©å‡½æ•°: å°† Document å¯¹è±¡åˆ—è¡¨è½¬æ¢ä¸ºç»“æœå­—å…¸åˆ—è¡¨
    """
    results = []
    for doc in documents:
        results.append(
            {
                "document_id": doc.id,
                "title": doc.title,
                "content": doc.content_text or "",
                "metadata": doc.doc_metadata or {},
            }
        )
    return results


# ==================== èŠ‚ç‚¹ 4.5: æ–‡æ¡£å»é‡ ====================
async def deduplicate_documents(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 4.5: æ–‡æ¡£å»é‡

    åŸºäºä¸‰é˜¶æ®µå»é‡ç®—æ³•ï¼Œç§»é™¤é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„æ–‡æ¡£ã€‚

    ä¸‰é˜¶æ®µç­–ç•¥ï¼š
    1. å¼ºå“ˆå¸Œ (SHA256): æ£€æµ‹å®Œå…¨ç›¸åŒçš„æ–‡æ¡£
    2. SimHash + æ±‰æ˜è·ç¦»: æ£€æµ‹é«˜åº¦ç›¸ä¼¼çš„æ–‡æ¡£
    3. Jaccardç›¸ä¼¼åº¦ + difflib: æ£€æµ‹â€œç²˜è´´å¼é‡å¤â€ï¼ˆä¸€ä¸ªæ–‡æ¡£è¢«ç²˜è´´åˆ°å¦ä¸€ä¸ªæ–‡æ¡£ä¸­ï¼‰

    è¾“å‡º:
    - final_results: å»é‡åçš„æ–‡æ¡£åˆ—è¡¨
    """
    logger.info("========== èŠ‚ç‚¹ 4.5: æ–‡æ¡£å»é‡ ===========")

    results = state.get("final_results", [])

    if not results or len(results) <= 1:
        logger.info("æ–‡æ¡£æ•°é‡â‰¤ 1ï¼Œæ— éœ€å»é‡")
        return state

    logger.info(f"å¼€å§‹å»é‡ï¼ŒåŸå§‹æ–‡æ¡£æ•°: {len(results)}")

    # é˜¶æ®µ 0: é¢„å¤„ç† - ä¸ºæ¯ä¸ªæ–‡æ¡£è®¡ç®—ç‰¹å¾
    doc_features = []
    for doc in results:
        content = doc.get("content", "")
        if not content:
            continue

        # æ ‡å‡†åŒ–æ–‡æœ¬
        normalized = normalize_text(content)
        if not normalized:
            continue

        doc_features.append(
            {
                "document_id": doc.get("document_id"),
                "title": doc.get("title", ""),
                "content": content,
                "normalized": normalized,
                "strong_hash": compute_strong_hash(normalized),
                "simhash": compute_simhash(normalized),
                "shingles": compute_shingles(normalized, k=5),
                "original_index": results.index(doc),
            }
        )

    logger.info(f"é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ–‡æ¡£æ•°: {len(doc_features)}")

    if len(doc_features) <= 1:
        return state

    # é˜¶æ®µ 1-4: è¿›è¡Œå»é‡æ¯”å¯¹
    removed_ids = set()

    for i in range(len(doc_features)):
        if doc_features[i]["document_id"] in removed_ids:
            continue

        for j in range(i + 1, len(doc_features)):
            if doc_features[j]["document_id"] in removed_ids:
                continue

            # åˆ¤æ–­æ˜¯å¦é‡å¤
            remove_id = should_remove_duplicate(
                doc_features[i], doc_features[j])

            if remove_id is not None:
                removed_ids.add(remove_id)
                logger.info(f"âœ–ï¸  æ–‡æ¡£ {remove_id} è¢«æ ‡è®°ä¸ºé‡å¤ï¼Œå°†è¢«ç§»é™¤")

    # è¿‡æ»¤é‡å¤æ–‡æ¡£
    deduplicated_results = [
        doc for doc in results if doc.get("document_id") not in removed_ids
    ]

    logger.info(
        f"âœ… å»é‡å®Œæˆ: åŸå§‹ {len(results)} ç¯‡ â†’ å»é‡å {len(deduplicated_results)} ç¯‡ ï¼ˆç§»é™¤ {len(removed_ids)} ç¯‡ï¼‰"
    )

    # æ›´æ–°çŠ¶æ€
    state["final_results"] = deduplicated_results

    return state


# ==================== èŠ‚ç‚¹ 4.7: æ‘˜è¦ç­›é€‰ ====================
async def filter_documents_by_summary(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 4.7: æ‘˜è¦ç­›é€‰

    ä½¿ç”¨ LLM å¿«é€Ÿåˆ¤æ–­æ–‡æ¡£æ‘˜è¦ä¸ç”¨æˆ·æŸ¥è¯¢çš„ç›¸å…³æ€§ï¼Œç­›é€‰å‡ºæœ‰ç”¨çš„æ–‡æ¡£ã€‚

    è¾“å‡º:
    - filtered_document_ids: ç­›é€‰åçš„æ–‡æ¡£IDåˆ—è¡¨
    - filtered_results: ç­›é€‰åçš„æ–‡æ¡£ç»“æœ
    """
    logger.info("========== èŠ‚ç‚¹ 4.7: æ‘˜è¦ç­›é€‰ ===========")

    # ä» config è·å– db
    db: AsyncSession = config.get("configurable", {}).get("db")  # type: ignore

    query = state["query"]
    results = state.get("final_results", [])

    if not results or len(results) <= 1:
        logger.info("ğŸ“Œ æ–‡æ¡£æ•°é‡â‰¤1ï¼Œè·³è¿‡æ‘˜è¦ç­›é€‰")
        state["filtered_document_ids"] = [
            doc.get("document_id") for doc in results]
        state["filtered_results"] = results
        return state

    logger.info(f"ğŸ“‹ å¼€å§‹æ‘˜è¦ç­›é€‰ï¼ŒåŸå§‹æ–‡æ¡£æ•°: {len(results)}")

    try:
        # 1. ä»databaseè·å–æ–‡æ¡£æ‘˜è¦
        document_ids = [doc.get("document_id") for doc in results]
        from models.database_models import Document

        docs_result = await db.execute(
            select(Document.id, Document.title, Document.ai_summary).where(
                Document.id.in_(document_ids)
            )
        )
        docs_with_summary = docs_result.all()

        # 2. æ„é€ æ‘˜è¦åˆ—è¡¨
        summaries = []
        for doc in docs_with_summary:
            summary = doc.ai_summary or "æœªç”Ÿæˆæ‘˜è¦"
            summaries.append(
                {
                    "document_id": doc.id,
                    "title": doc.title,
                    "summary": summary,
                }
            )

        if not summaries:
            logger.warning("âš ï¸ æ— æ–‡æ¡£æ‘˜è¦ï¼Œè·³è¿‡ç­›é€‰")
            state["filtered_document_ids"] = document_ids
            state["filtered_results"] = results
            return state

        # 3. æ„é€  LLM Prompt
        summaries_text = "\n".join(
            f"{i+1}. [æ–‡æ¡£ID: {s['document_id']}] {s['title']}\n   æ‘˜è¦: {s['summary']}"
            for i, s in enumerate(summaries)
        )

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£ç›¸å…³æ€§åˆ¤æ–­åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œæ–‡æ¡£æ‘˜è¦ï¼Œå¿«é€Ÿåˆ¤æ–­å“ªäº›æ–‡æ¡£ç›´æ¥ç›¸å…³ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€æ–‡æ¡£æ‘˜è¦åˆ—è¡¨ã€‘
{summaries_text}

ã€åˆ¤æ–­æ ‡å‡†ã€‘
1. **ç›´æ¥ç›¸å…³**ï¼šæ–‡æ¡£å†…å®¹èƒ½å¤Ÿç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œæˆ–åŒ…å«ç”¨æˆ·é—®é¢˜æ‰€éœ€çš„å…³é”®ä¿¡æ¯
2. **ä¸ç›¸å…³**ï¼šæ–‡æ¡£å†…å®¹ä¸é—®é¢˜ä¸»é¢˜ä¸åŒï¼Œæˆ–è€…åªæ˜¯è¾¹ç¼˜ç›¸å…³ï¼Œæ— æ³•å›ç­”ç”¨æˆ·é—®é¢˜

ä¾‹å¦‚ï¼š
- é—®é¢˜ï¼šâ€œä¸­å›½å‘ç”Ÿè¿‡å“ªäº›å¤§å‹åœ°éœ‡â€
- ç›¸å…³æ–‡æ¡£ï¼šã€Šä¸­å›½åœ°éœ‡å²ã€‹ã€ã€Š5.12æ±¶å·åœ°éœ‡çºªå®ã€‹
- ä¸ç›¸å…³æ–‡æ¡£ï¼šã€Šåœ°éœ‡åº”æ€¥é¢„æ¡ˆã€‹ï¼ˆè®²è¿°åº”å¯¹æªæ–½ï¼Œä¸æ˜¯å†å²äº‹ä»¶ï¼‰

ã€è¾“å‡ºè¦æ±‚ã€‘
è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "relevant_document_ids": [ç›´æ¥ç›¸å…³çš„æ–‡æ¡£IDåˆ—è¡¨],
    "reasoning": "ç®€è¦è¯´æ˜ä¸ºä»€ä¹ˆè¿™äº›æ–‡æ¡£ç›¸å…³æˆ–ä¸ç›¸å…³"
}}

**æ³¨æ„**ï¼š
1. åªè¿”å›ç›´æ¥ç›¸å…³çš„æ–‡æ¡£ID
2. å¦‚æœæ‰€æœ‰æ–‡æ¡£éƒ½ä¸ç›¸å…³ï¼Œè¿”å›ç©ºåˆ—è¡¨ []
3. å¦‚æœæ— æ³•åˆ¤æ–­ï¼Œå®å¯ä¿ç•™ï¼ˆè¿”å›è¯¥æ–‡æ¡£IDï¼‰
è¯·ç›´æ¥è¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""

        llm_client = get_llm_client()
        llm_response = await llm_client.extract_json_response(prompt, db=db)

        logger.info(f"ğŸ¤– LLMç­›é€‰ç»“æœ: {llm_response}")

        relevant_ids = llm_response.get("relevant_document_ids", [])
        reasoning = llm_response.get("reasoning", "")

        # 4. ç­›é€‰ç»“æœ
        if not relevant_ids:
            # æ‰€æœ‰æ–‡æ¡£éƒ½ä¸ç›¸å…³
            logger.warning(f"âš ï¸ LLMåˆ¤æ–­æ‰€æœ‰æ–‡æ¡£éƒ½ä¸ç›¸å…³: {reasoning}")
            state["filtered_document_ids"] = []
            state["filtered_results"] = []
        else:
            # ç­›é€‰å‡ºç›¸å…³æ–‡æ¡£
            filtered = [
                doc for doc in results if doc.get("document_id") in relevant_ids
            ]
            state["filtered_document_ids"] = relevant_ids
            state["filtered_results"] = filtered

            logger.info(
                f"âœ… æ‘˜è¦ç­›é€‰å®Œæˆ: åŸå§‹ {len(results)} ç¯‡ â†’ ç­›é€‰å {len(filtered)} ç¯‡"
            )
            logger.info(f"ğŸ“ ç­›é€‰åŸå› : {reasoning}")

    except Exception as e:
        logger.error(f"âŒ æ‘˜è¦ç­›é€‰å¤±è´¥: {e}")
        import traceback

        logger.error(traceback.format_exc())
        # å¤±è´¥æ—¶ä¿ç•™æ‰€æœ‰æ–‡æ¡£
        state["filtered_document_ids"] = [
            doc.get("document_id") for doc in results]
        state["filtered_results"] = results

    return state


# ==================== èŠ‚ç‚¹ 5: æ­§ä¹‰å¤„ç† ====================
async def handle_ambiguity(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 5: æ­§ä¹‰å¤„ç†

    å¦‚æœæŸ¥è¯¢æœ‰æ­§ä¹‰,æš‚åœæµç¨‹å¹¶å‘ç”¨æˆ·æé—®ã€‚
    è¿™æ˜¯ä¸€ä¸ªç»ˆç«¯èŠ‚ç‚¹ã€‚
    """
    logger.info("========== èŠ‚ç‚¹ 5: æ­§ä¹‰å¤„ç† ==========")
    logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ­§ä¹‰: {state.get('ambiguity_message')}")

    # çŠ¶æ€å·²åŒ…å« ambiguity_message,ç›´æ¥è¿”å›
    # å‰ç«¯ä¼šå±•ç¤ºè¿™ä¸ªæ¶ˆæ¯å¹¶ç­‰å¾…ç”¨æˆ·è¾“å…¥
    return state


# ==================== èŠ‚ç‚¹ 6: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ ====================
async def generate_answer(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 6: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

    åŸºäºæœ€ç»ˆæ£€ç´¢ç»“æœ,ä½¿ç”¨ RAG ç”Ÿæˆç”¨æˆ·é—®é¢˜çš„ç­”æ¡ˆã€‚
    å¦‚æœæ£€ç´¢ä¸åˆ°ä»»ä½•æ–‡æ¡£ï¼Œåˆ™ä½¿ç”¨å¤§æ¨¡å‹ç›´æ¥å›ç­”ï¼ˆä¸åŸºäºæ–‡æ¡£ï¼‰ã€‚
    æ™ºèƒ½åˆ†ç»„é—®ç­”ï¼šæ ¹æ®ä¸Šä¸‹æ–‡é•¿åº¦å†³å®šæ˜¯åˆå¹¶è¿˜æ˜¯åˆ†åˆ«é—®ç­”ã€‚

    è¾“å‡º:
    - answer: æœ€ç»ˆç­”æ¡ˆ
    """
    logger.info("========== èŠ‚ç‚¹ 6: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ ==========")

    # ä» config è·å– db å’Œ RAG é…ç½®
    db: AsyncSession = config.get("configurable", {}).get("db")  # type: ignore
    max_context_length: int = config.get("configurable", {}).get(
        "rag_max_length", 10000
    )

    query = state["query"]
    # ä½¿ç”¨ç­›é€‰åçš„ç»“æœ
    results = state.get("filtered_results", state.get("final_results", []))

    if not results:
        logger.warning("âš ï¸ æ— æœ€ç»ˆæ£€ç´¢ç»“æœï¼Œä½¿ç”¨å¤§æ¨¡å‹ç›´æ¥å›ç­”ï¼ˆä¸åŸºäºæ–‡æ¡£ï¼‰")

        # ä½¿ç”¨å¤§æ¨¡å‹ç›´æ¥å›ç­”ï¼Œä¸ä¾èµ–æ–‡æ¡£
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚ç”¨æˆ·çš„é—®é¢˜åœ¨ç³»ç»Ÿä¸­æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†ç›´æ¥å›ç­”ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
1. è¯·åŸºäºä½ çš„çŸ¥è¯†å‚¨å¤‡ï¼Œå°½å¯èƒ½å‡†ç¡®ã€ä¸“ä¸šåœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜
2. å›ç­”è¦ç®€æ´æ˜äº†ï¼Œæ¡ç†æ¸…æ™°
3. åœ¨å›ç­”å¼€å¤´æ˜ç¡®è¯´æ˜ï¼šæ­¤å›ç­”æœªåŸºäºç³»ç»Ÿæ–‡æ¡£ï¼Œä»…ä¾›å‚è€ƒ
4. å¦‚æœé—®é¢˜è¿‡äºä¸“ä¸šæˆ–ä½ ä¸ç¡®å®šï¼Œè¯·å¦‚å®è¯´æ˜

è¯·å¼€å§‹å›ç­”ï¼š
        """

        llm_client = get_llm_client()

        try:
            answer = await llm_client.chat_completion(prompt, db=db)
            # åœ¨ç­”æ¡ˆæœ«å°¾æ·»åŠ æœªä½¿ç”¨å‚è€ƒæ–‡çŒ®çš„æ ‡æ³¨
            state["answer"] = (
                f"{answer}\n\n---\n**æ³¨ï¼šæœ¬å›ç­”æœªä½¿ç”¨ä»»ä½•å‚è€ƒæ–‡çŒ®ï¼Œä»…åŸºäºAIçŸ¥è¯†åº“ç”Ÿæˆï¼Œå»ºè®®è°¨æ…å‚è€ƒã€‚**"
            )
            logger.info(
                f"âœ… ä½¿ç”¨å¤§æ¨¡å‹ç›´æ¥ç”Ÿæˆç­”æ¡ˆï¼ˆæ— æ–‡æ¡£å‚è€ƒï¼‰ (é•¿åº¦: {len(answer)} å­—ç¬¦)"
            )
        except Exception as e:
            logger.error(f"âŒ LLM ç›´æ¥ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
            state["answer"] = (
                "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„æ–‡æ¡£ï¼Œä¸”åœ¨å°è¯•ç›´æ¥å›ç­”æ—¶é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ã€‚å»ºè®®æ‚¨:\n1. å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯\n2. ç®€åŒ–æˆ–æ˜ç¡®æ‚¨çš„é—®é¢˜\n3. æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²ä¸Šä¼ åˆ°ç³»ç»Ÿä¸­"
            )

        return state

    # è®¡ç®—æ€»ä¸Šä¸‹æ–‡é•¿åº¦
    total_length = sum(len(doc.get("content", "")) for doc in results)
    logger.info(f"ğŸ“Š æ€»ä¸Šä¸‹æ–‡é•¿åº¦: {total_length} å­—ç¬¦, é˜ˆå€¼: {max_context_length}")

    llm_client = get_llm_client()
    tool_answer_partial = state.get("tool_answer_partial")

    # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†ç»„é—®ç­”
    if total_length <= max_context_length:
        # ä¸Šä¸‹æ–‡é•¿åº¦åˆé€‚ï¼Œåˆå¹¶æ‰€æœ‰æ–‡æ¡£ä¸€æ¬¡é—®ç­”
        logger.info("ğŸ“¦ ä¸Šä¸‹æ–‡é•¿åº¦åˆé€‚ï¼Œåˆå¹¶æ‰€æœ‰æ–‡æ¡£ä¸€æ¬¡é—®ç­”")
        answer = await _generate_single_answer(
            query, results, tool_answer_partial, llm_client, db
        )
        state["answer"] = answer

    else:
        # ä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œå¯¹æ¯ä¸ªæ–‡æ¡£å•ç‹¬é—®ç­”ï¼Œå†ç»„åˆç»“æœ
        logger.info("ğŸ“¦ ä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œå¯¹æ¯ä¸ªæ–‡æ¡£å•ç‹¬é—®ç­”ï¼Œå†ç»„åˆç»“æœ")
        answer = await _generate_grouped_answer(
            query, results, tool_answer_partial, llm_client, db
        )
        state["answer"] = answer

    return state

    # æ„é€  RAG ä¸Šä¸‹æ–‡
    context_parts = []
    for i, doc in enumerate(results[:5], 1):  # æœ€å¤šä½¿ç”¨ 5 ç¯‡æ–‡æ¡£
        doc_context = f"ã€æ–‡æ¡£ {i}ã€‘\n"
        doc_context += f"æ ‡é¢˜: {doc.get('title', 'æœªçŸ¥æ ‡é¢˜')}\n"

        # æ™ºèƒ½æˆªå–å†…å®¹ç‰‡æ®µ (ä¼˜å…ˆåŒ…å«æŸ¥è¯¢å…³é”®è¯é™„è¿‘çš„å†…å®¹)
        content = doc.get("content", "")
        if len(content) > 800:
            # ç®€å•æˆªå–ç­–ç•¥,å®é™…å¯ä»¥ç”¨æ›´æ™ºèƒ½çš„æ–¹æ³•
            content = content[:800] + "..."
        doc_context += f"å†…å®¹: {content}\n"

        # æ·»åŠ å…ƒæ•°æ®
        metadata = doc.get("metadata", {})
        if metadata:
            doc_context += f"å…ƒæ•°æ®: {json.dumps(metadata, ensure_ascii=False)}\n"

        context_parts.append(doc_context)

    context_str = "\n".join(context_parts)

    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨çš„éƒ¨åˆ†ç­”æ¡ˆï¼ˆç»„åˆæŸ¥è¯¢ï¼‰
    tool_answer_partial = state.get("tool_answer_partial")

    # æ„é€  RAG prompt
    if tool_answer_partial:
        # ç»„åˆæŸ¥è¯¢ï¼šéœ€è¦åˆå¹¶å·¥å…·ç­”æ¡ˆå’Œæ–‡æ¡£ç­”æ¡ˆ
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚ç”¨æˆ·çš„é—®é¢˜åŒ…å«å¤šä¸ªå­ä»»åŠ¡ï¼Œä½ å·²ç»é€šè¿‡å·¥å…·è°ƒç”¨å›ç­”äº†éƒ¨åˆ†é—®é¢˜ï¼Œç°åœ¨éœ€è¦ç»“åˆæ–‡æ¡£å†…å®¹å›ç­”å‰©ä½™éƒ¨åˆ†ã€‚

ã€å·¥å…·è°ƒç”¨ç»“æœï¼ˆå·²å›ç­”çš„éƒ¨åˆ†ï¼‰ã€‘
{tool_answer_partial}

ã€æ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‘
{context_str}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
1. å…ˆç®€è¦åˆ—å‡ºå·¥å…·è°ƒç”¨å·²ç»å›ç­”çš„éƒ¨åˆ†
2. å†åŸºäºæ–‡æ¡£å†…å®¹å›ç­”å‰©ä½™é—®é¢˜
3. å¦‚æœéœ€è¦å¼•ç”¨æ–‡æ¡£ï¼Œè¯·ä½¿ç”¨ "æ ¹æ®æ–‡æ¡£X" çš„æ ¼å¼
4. å›ç­”è¦å…¨é¢ã€å‡†ç¡®ã€æ¸…æ™°
5. å¦‚æœæ–‡æ¡£å†…å®¹ä¸å‰©ä½™é—®é¢˜æ— å…³ï¼Œè¯·å¦‚å®è¯´æ˜

è¯·å¼€å§‹å›ç­”ï¼š
    """
    else:
        # å•çº¯æ–‡æ¡£æ£€ç´¢
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€æ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‘
{context_str}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
1. åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹è¿›è¡Œå›ç­”,å¦‚æœæ–‡æ¡£ä¸­æœ‰æ˜ç¡®ç­”æ¡ˆè¯·ç›´æ¥å¼•ç”¨
2. å¦‚æœéœ€è¦å¼•ç”¨æ–‡æ¡£,è¯·ä½¿ç”¨ "æ ¹æ®æ–‡æ¡£X" çš„æ ¼å¼
3. å¦‚æœæ–‡æ¡£ä¿¡æ¯ä¸è¶³ä»¥å®Œæ•´å›ç­”é—®é¢˜,è¯·æ˜ç¡®è¯´æ˜å“ªäº›éƒ¨åˆ†æ— æ³•ç¡®å®š
4. å›ç­”è¦ç®€æ´ã€å‡†ç¡®ã€ä¸“ä¸š
5. å¦‚æœæ–‡æ¡£å†…å®¹ä¸é—®é¢˜æ— å…³,è¯·å¦‚å®è¯´æ˜

è¯·å¼€å§‹å›ç­”:
    """
    llm_client = get_llm_client()

    try:
        answer = await llm_client.chat_completion(prompt, db=db)
        state["answer"] = answer
        logger.info(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(answer)} å­—ç¬¦)")

    except Exception as e:
        logger.error(f"âŒ LLM ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
        state["answer"] = "æŠ±æ­‰,æˆ‘åœ¨ç”Ÿæˆç­”æ¡ˆæ—¶é‡åˆ°äº†æŠ€æœ¯é—®é¢˜,è¯·ç¨åé‡è¯•ã€‚"

    return state


# ==================== è¾…åŠ©å‡½æ•°ï¼šå•æ¬¡é—®ç­” ====================
async def _generate_single_answer(
    query: str,
    results: List[Dict[str, Any]],
    tool_answer_partial: Optional[str],
    llm_client,
    db: AsyncSession,
) -> str:
    """
    å•æ¬¡é—®ç­”ï¼šåˆå¹¶æ‰€æœ‰æ–‡æ¡£å†…å®¹ï¼Œä¸€æ¬¡é—®ç­”
    """
    # æ„é€  RAG ä¸Šä¸‹æ–‡
    context_parts = []
    for i, doc in enumerate(results, 1):
        doc_context = f"ã€æ–‡æ¡£ {i}ã€‘\n"
        doc_context += f"æ ‡é¢˜: {doc.get('title', 'æœªçŸ¥æ ‡é¢˜')}\n"

        # æ™ºèƒ½æˆªå–å†…å®¹ç‰‡æ®µ
        content = doc.get("content", "")
        if len(content) > 1500:  # æ¯ä¸ªæ–‡æ¡£æœ€å¤š1500å­—
            content = content[:1500] + "..."
        doc_context += f"å†…å®¹: {content}\n"

        # æ·»åŠ å…ƒæ•°æ®
        metadata = doc.get("metadata", {})
        if metadata:
            doc_context += f"å…ƒæ•°æ®: {json.dumps(metadata, ensure_ascii=False)}\n"

        context_parts.append(doc_context)

    context_str = "\n".join(context_parts)

    # æ„é€  RAG prompt
    if tool_answer_partial:
        # ç»„åˆæŸ¥è¯¢ï¼šéœ€è¦åˆå¹¶å·¥å…·ç­”æ¡ˆå’Œæ–‡æ¡£ç­”æ¡ˆ
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚ç”¨æˆ·çš„é—®é¢˜åŒ…å«å¤šä¸ªå­ä»»åŠ¡ï¼Œä½ å·²ç»é€šè¿‡å·¥å…·è°ƒç”¨å›ç­”äº†éƒ¨åˆ†é—®é¢˜ï¼Œç°åœ¨éœ€è¦ç»“åˆæ–‡æ¡£å†…å®¹å›ç­”å‰©ä½™éƒ¨åˆ†ã€‚

ã€å·¥å…·è°ƒç”¨ç»“æœï¼ˆå·²å›ç­”çš„éƒ¨åˆ†ï¼‰ã€‘
{tool_answer_partial}

ã€æ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‘
{context_str}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
1. å…ˆç®€è¦åˆ—å‡ºå·¥å…·è°ƒç”¨å·²ç»å›ç­”çš„éƒ¨åˆ†
2. å†åŸºäºæ–‡æ¡£å†…å®¹å›ç­”å‰©ä½™é—®é¢˜
3. å¦‚æœéœ€è¦å¼•ç”¨æ–‡æ¡£ï¼Œè¯·ä½¿ç”¨ "æ ¹æ®æ–‡æ¡£X" çš„æ ¼å¼
4. å›ç­”è¦å…¨é¢ã€å‡†ç¡®ã€æ¸…æ™°
5. å¦‚æœæ–‡æ¡£å†…å®¹ä¸å‰©ä½™é—®é¢˜æ— å…³ï¼Œè¯·å¦‚å®è¯´æ˜

è¯·å¼€å§‹å›ç­”ï¼š
    """
    else:
        # å•çº¯æ–‡æ¡£æ£€ç´¢
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€æ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‘
{context_str}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
1. åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹è¿›è¡Œå›ç­”ï¼Œå¦‚æœæ–‡æ¡£ä¸­æœ‰æ˜ç¡®ç­”æ¡ˆè¯·ç›´æ¥å¼•ç”¨
2. å¦‚æœéœ€è¦å¼•ç”¨æ–‡æ¡£ï¼Œè¯·ä½¿ç”¨ "æ ¹æ®æ–‡æ¡£X" çš„æ ¼å¼
3. å¦‚æœæ–‡æ¡£ä¿¡æ¯ä¸è¶³ä»¥å®Œæ•´å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜å“ªäº›éƒ¨åˆ†æ— æ³•ç¡®å®š
4. å›ç­”è¦ç®€æ´ã€å‡†ç¡®ã€ä¸“ä¸š
5. å¦‚æœæ–‡æ¡£å†…å®¹ä¸é—®é¢˜æ— å…³ï¼Œè¯·å¦‚å®è¯´æ˜

è¯·å¼€å§‹å›ç­”ï¼š
    """

    try:
        answer = await llm_client.chat_completion(prompt, db=db)
        logger.info(f"âœ… å•æ¬¡é—®ç­”ç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(answer)} å­—ç¬¦)")
        return answer
    except Exception as e:
        logger.error(f"âŒ LLM ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
        return "æŠ±æ­‰ï¼Œæˆ‘åœ¨ç”Ÿæˆç­”æ¡ˆæ—¶é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"


# ==================== è¾…åŠ©å‡½æ•°ï¼šåˆ†ç»„é—®ç­” ====================
async def _generate_grouped_answer(
    query: str,
    results: List[Dict[str, Any]],
    tool_answer_partial: Optional[str],
    llm_client,
    db: AsyncSession,
) -> str:
    """
    åˆ†ç»„é—®ç­”ï¼šå¯¹æ¯ä¸ªæ–‡æ¡£å•ç‹¬é—®ç­”ï¼Œå†ç»„åˆç»“æœ
    """
    logger.info(f"ğŸ”€ å¼€å§‹åˆ†ç»„é—®ç­”ï¼Œæ€»å…± {len(results)} ä¸ªæ–‡æ¡£")

    # 1. å¯¹æ¯ä¸ªæ–‡æ¡£å•ç‹¬é—®ç­”
    doc_answers = []
    for i, doc in enumerate(results, 1):
        doc_context = f"""
ã€æ–‡æ¡£æ ‡é¢˜ã€‘{doc.get('title', 'æœªçŸ¥æ ‡é¢˜')}

ã€æ–‡æ¡£å†…å®¹ã€‘
{doc.get('content', '')}
"""
        if doc.get("metadata"):
            doc_context += (
                f"\nã€å…ƒæ•°æ®ã€‘{json.dumps(doc['metadata'], ensure_ascii=False)}"
            )

        single_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

{doc_context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
1. åªåŸºäºè¿™ä¸ªæ–‡æ¡£å›ç­”
2. å¦‚æœæ–‡æ¡£èƒ½å›ç­”é—®é¢˜ï¼Œè¯·ç®€æ´ã€å‡†ç¡®åœ°å›ç­”
3. å¦‚æœæ–‡æ¡£ä¸èƒ½å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ­¤æ–‡æ¡£æ— æ³•å›ç­”è¯¥é—®é¢˜"

è¯·å¼€å§‹å›ç­”ï¼š
"""

        try:
            doc_answer = await llm_client.chat_completion(single_prompt, db=db)
            doc_answers.append(
                {"doc_index": i, "title": doc.get(
                    "title"), "answer": doc_answer}
            )
            logger.info(f"  âœ… æ–‡æ¡£ {i} é—®ç­”å®Œæˆ")
        except Exception as e:
            logger.error(f"  âŒ æ–‡æ¡£ {i} é—®ç­”å¤±è´¥: {e}")
            doc_answers.append(
                {
                    "doc_index": i,
                    "title": doc.get("title"),
                    "answer": "æ— æ³•ç”Ÿæˆç­”æ¡ˆ",
                }
            )

    # 2. ç»„åˆæ‰€æœ‰æ–‡æ¡£çš„ç­”æ¡ˆ
    combined_doc_answers = "\n\n".join(
        f"ã€æ–‡æ¡£ {da['doc_index']}: {da['title']}ã€‘\n{da['answer']}"
        for da in doc_answers
    )

    # 3. æœ€ç»ˆç»„åˆ
    if tool_answer_partial:
        # ç»„åˆæŸ¥è¯¢ï¼šåˆå¹¶å·¥å…·ç­”æ¡ˆå’Œæ–‡æ¡£ç­”æ¡ˆ
        final_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ€»ç»“åŠ©æ‰‹ã€‚ç”¨æˆ·çš„é—®é¢˜åŒ…å«å¤šä¸ªå­ä»»åŠ¡ï¼Œç°åœ¨éœ€è¦åˆå¹¶å·¥å…·è°ƒç”¨ç»“æœå’Œå¤šä¸ªæ–‡æ¡£çš„ç­”æ¡ˆã€‚

ã€å·¥å…·è°ƒç”¨ç»“æœã€‘
{tool_answer_partial}

ã€å„æ–‡æ¡£çš„ç­”æ¡ˆã€‘
{combined_doc_answers}

ã€ç”¨æˆ·é—®é¢˜ã€
{query}

ã€æ€»ç»“è¦æ±‚ã€‘
1. å…ˆåˆ—å‡ºå·¥å…·è°ƒç”¨çš„ç»“æœ
2. å†æ€»ç»“å„æ–‡æ¡£çš„ç­”æ¡ˆï¼Œå»é™¤é‡å¤ä¿¡æ¯
3. åˆå¹¶ä¸ºä¸€ä¸ªå…¨é¢ã€æ¸…æ™°çš„ç­”æ¡ˆ
4. æ ‡æ˜ä¿¡æ¯æ¥æºï¼ˆå·¥å…·/å“ªä¸ªæ–‡æ¡£ï¼‰

è¯·å¼€å§‹æ€»ç»“ï¼š
"""
    else:
        # å•çº¯æ–‡æ¡£æ£€ç´¢ï¼šåªéœ€æ€»ç»“æ–‡æ¡£ç­”æ¡ˆ
        final_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ€»ç»“åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯å¤šä¸ªæ–‡æ¡£åˆ†åˆ«å¯¹ç”¨æˆ·é—®é¢˜çš„å›ç­”ï¼Œè¯·åˆå¹¶ä¸ºä¸€ä¸ªç®€æ´ã€å…¨é¢çš„ç­”æ¡ˆã€‚

ã€å„æ–‡æ¡£çš„ç­”æ¡ˆã€‘
{combined_doc_answers}

ã€ç”¨æˆ·é—®é¢˜ã€
{query}

ã€æ€»ç»“è¦æ±‚ã€‘
1. å»é™¤é‡å¤ä¿¡æ¯ï¼Œåˆå¹¶ç›¸ä¼¼å†…å®¹
2. ä¿ç•™æ‰€æœ‰å…³é”®ä¿¡æ¯
3. æŒ‰é€»è¾‘ç»„ç»‡æˆæ¸…æ™°çš„ç­”æ¡ˆ
4. å¦‚æœæœ‰æ–‡æ¡£æ— æ³•å›ç­”ï¼Œå¯ä»¥å¿½ç•¥
5. æ ‡æ˜ä¿¡æ¯æ¥æºï¼ˆæ–‡æ¡£æ ‡é¢˜ï¼‰

è¯·å¼€å§‹æ€»ç»“ï¼š
"""

    try:
        final_answer = await llm_client.chat_completion(final_prompt, db=db)
        logger.info(f"âœ… åˆ†ç»„é—®ç­”æ€»ç»“å®Œæˆ (é•¿åº¦: {len(final_answer)} å­—ç¬¦)")
        return final_answer
    except Exception as e:
        logger.error(f"âŒ æ€»ç»“ç­”æ¡ˆå¤±è´¥: {e}")
        return "æŠ±æ­‰ï¼Œæˆ‘åœ¨æ€»ç»“ç­”æ¡ˆæ—¶é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"


# ==================== å†³ç­–å‡½æ•° ====================
def should_use_tool(state: RetrievalState) -> str:
    """
    å†³ç­–å‡½æ•°: æ ¹æ®æ‰§è¡Œè®¡åˆ’åˆ¤æ–­è·¯ç”±

    Returns:
        'tool_answer': æ‰§è¡Œè®¡åˆ’åŒ…å«å·¥å…·è°ƒç”¨ï¼ˆä¹‹åå¯èƒ½è¿˜éœ€è¦æ£€ç´¢ï¼‰
        'retrieval': æ‰§è¡Œè®¡åˆ’åªæœ‰æ–‡æ¡£æ£€ç´¢
    """
    execution_plan = state.get("execution_plan", [])

    # æ£€æŸ¥æ‰§è¡Œè®¡åˆ’ä¸­æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
    has_tool_call = any(step.get("action") ==
                        "tool_call" for step in execution_plan)

    if has_tool_call:
        logger.info("ğŸ”§ å†³ç­–: æ‰§è¡Œè®¡åˆ’åŒ…å«å·¥å…·è°ƒç”¨ -> tool_answer")
        return "tool_answer"
    else:
        logger.info("ğŸ” å†³ç­–: ä»…æ–‡æ¡£æ£€ç´¢ -> retrieval")
        return "retrieval"


def should_ask_user(state: RetrievalState) -> str:
    """
    å†³ç­–å‡½æ•°: åˆ¤æ–­æ˜¯å¦éœ€è¦å‘ç”¨æˆ·æé—®æ¾„æ¸…

    Returns:
        'ask_user': æœ‰æ­§ä¹‰,éœ€è¦ç”¨æˆ·æ¾„æ¸…
        'generate_answer': æ— æ­§ä¹‰,ç›´æ¥ç”Ÿæˆç­”æ¡ˆ
    """
    if state.get("ambiguity_message"):
        logger.info("ğŸ”€ å†³ç­–: æœ‰æ­§ä¹‰ -> ask_user")
        return "ask_user"
    else:
        logger.info("ğŸ”€ å†³ç­–: æ— æ­§ä¹‰ -> generate_answer")
        return "generate_answer"


# ==================== æ„å»º LangGraph å·¥ä½œæµ ====================
# ==================== æ„å»º LangGraph å·¥ä½œæµ ====================
# ä¼˜åŒ–åçš„å·¥ä½œæµç¨‹ï¼š
# 0. ä»»åŠ¡è§„åˆ’ (intent_routing) - LLM è§„åˆ’æ‰§è¡Œæ­¥éª¤
# 1. [å†³ç­–] æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨? (should_use_tool)
#    - tool_answer: åŒ…å«å·¥å…·è°ƒç”¨ -> æ‰§è¡Œå·¥å…· -> [å†³ç­–] æ˜¯å¦éœ€è¦æ£€ç´¢?
#      - éœ€è¦ -> æ£€ç´¢å¢å¼º -> æ–‡æ¡£æ£€ç´¢æµç¨‹
#      - ä¸éœ€è¦ -> END
#    - retrieval: åªæœ‰æ–‡æ¡£æ£€ç´¢ -> æ£€ç´¢å¢å¼º -> ESå…¨æ–‡æ£€ç´¢...
# 2. æ£€ç´¢å¢å¼º (enhance_retrieval_query) - LLM è§£æå­—æ®µå¹¶é‡å†™æŸ¥è¯¢
# 3. ESå…¨æ–‡æ£€ç´¢ (es_fulltext_retrieval)
# 4. SQLç»“æ„åŒ–æ£€ç´¢ (sql_structured_retrieval)
# 5. ç»“æœèåˆ (merge_retrieval_results)
# 6. ç²¾ç»†åŒ–ç­›é€‰ (refined_filtering)
# 7. æ–‡æ¡£å»é‡ (deduplicate_documents)
# 8. æ‘˜è¦ç­›é€‰ (filter_documents_by_summary) - LLMåˆ¤æ–­æ–‡æ¡£ç›¸å…³æ€§
# 9. [å†³ç­–] æ˜¯å¦æœ‰æ­§ä¹‰? (should_ask_user)
#    - ask_user: æ­§ä¹‰å¤„ç† (handle_ambiguity) -> END
#    - generate_answer: ç”Ÿæˆç­”æ¡ˆ (generate_answer) -> END

# 1. åˆå§‹åŒ– StateGraph
workflow = StateGraph(RetrievalState)

# 2. æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
workflow.add_node("intent_routing", intent_routing)  # èŠ‚ç‚¹0: ä»»åŠ¡è§„åˆ’
workflow.add_node("tool_answer", generate_tool_answer)  # å·¥å…·è°ƒç”¨ç­”æ¡ˆç”Ÿæˆ
workflow.add_node("enhance_query", enhance_retrieval_query)  # èŠ‚ç‚¹0.5: æ£€ç´¢å¢å¼º
workflow.add_node("es_fulltext", es_fulltext_retrieval)  # èŠ‚ç‚¹1: ESå…¨æ–‡æ£€ç´¢
workflow.add_node("sql_structured", sql_structured_retrieval)  # èŠ‚ç‚¹2: SQLç»“æ„åŒ–æ£€ç´¢
workflow.add_node("merge_results", merge_retrieval_results)  # èŠ‚ç‚¹3: ç»“æœèåˆ
workflow.add_node("refined_filter", refined_filtering)  # èŠ‚ç‚¹4: ç²¾ç»†åŒ–ç­›é€‰
workflow.add_node("deduplicate", deduplicate_documents)  # èŠ‚ç‚¹4.5: æ–‡æ¡£å»é‡
workflow.add_node("filter_summary", filter_documents_by_summary)  # èŠ‚ç‚¹4.7: æ‘˜è¦ç­›é€‰
workflow.add_node("ask_user", handle_ambiguity)  # èŠ‚ç‚¹5a: æ­§ä¹‰å¤„ç†
workflow.add_node("generate_answer", generate_answer)  # èŠ‚ç‚¹5b: ç”Ÿæˆç­”æ¡ˆ

# 3. è®¾ç½®å›¾çš„å…¥å£ç‚¹ï¼ˆä»ä»»åŠ¡è§„åˆ’å¼€å§‹ï¼‰
workflow.set_entry_point("intent_routing")

# 4. æ·»åŠ æ¡ä»¶è¾¹ï¼šä»»åŠ¡è§„åˆ’åå†³å®šèµ°å‘
workflow.add_conditional_edges(
    "intent_routing",  # æºèŠ‚ç‚¹
    should_use_tool,  # å†³ç­–å‡½æ•°
    {
        "tool_answer": "tool_answer",  # åŒ…å«å·¥å…·è°ƒç”¨ â†’ å·¥å…·ç­”æ¡ˆç”Ÿæˆ â†’ [å†³ç­–]
        "retrieval": "enhance_query",  # ä»…æ–‡æ¡£æ£€ç´¢ â†’ æ£€ç´¢å¢å¼º â†’ ESå…¨æ–‡æ£€ç´¢
    },
)

# 4.5 å·¥å…·è°ƒç”¨åï¼Œæ ¹æ®æ‰§è¡Œè®¡åˆ’å†³å®šæ˜¯å¦ç»§ç»­æ£€ç´¢
workflow.add_conditional_edges(
    "tool_answer",  # æºèŠ‚ç‚¹
    lambda state: "continue_retrieval" if state.get(
        "need_retrieval", False) else "end",
    {
        "continue_retrieval": "enhance_query",  # ç»§ç»­æ£€ç´¢ â†’ å…ˆå¢å¼ºæŸ¥è¯¢
        "end": END,  # ç›´æ¥ç»“æŸ
    },
)

# 4.6 æ£€ç´¢å¢å¼ºåï¼Œè¿›å…¥ ES å…¨æ–‡æ£€ç´¢
workflow.add_edge("enhance_query", "es_fulltext")  # æ£€ç´¢å¢å¼º â†’ ESå…¨æ–‡æ£€ç´¢

# 5. æ·»åŠ æ–‡æ¡£æ£€ç´¢æµç¨‹çš„çº¿æ€§è¾¹
workflow.add_edge("es_fulltext", "sql_structured")  # ESå…¨æ–‡ â†’ SQLç»“æ„åŒ–
workflow.add_edge("sql_structured", "merge_results")  # SQLç»“æ„åŒ– â†’ ç»“æœèåˆ
workflow.add_edge("merge_results", "refined_filter")  # ç»“æœèåˆ â†’ ç²¾ç»†åŒ–ç­›é€‰
workflow.add_edge("refined_filter", "deduplicate")  # ç²¾ç»†åŒ–ç­›é€‰ â†’ æ–‡æ¡£å»é‡
workflow.add_edge("deduplicate", "filter_summary")  # æ–‡æ¡£å»é‡ â†’ æ‘˜è¦ç­›é€‰

# 6. æ·»åŠ æ¡ä»¶è¾¹ï¼šåœ¨æ‘˜è¦ç­›é€‰åï¼Œåˆ¤æ–­æ˜¯å¦æœ‰æ­§ä¹‰
workflow.add_conditional_edges(
    "filter_summary",  # æºèŠ‚ç‚¹
    should_ask_user,  # å†³ç­–å‡½æ•°
    {
        "ask_user": "ask_user",  # æœ‰æ­§ä¹‰ â†’ å‘ç”¨æˆ·æé—®
        "generate_answer": "generate_answer",  # æ— æ­§ä¹‰ â†’ ç”Ÿæˆç­”æ¡ˆ
    },
)

# 7. è®¾ç½®å›¾çš„ç»ˆç‚¹ï¼ˆæ³¨æ„ï¼štool_answer ç°åœ¨æœ‰æ¡ä»¶è¾¹ï¼Œä¸å†ç›´æ¥åˆ° ENDï¼‰
workflow.add_edge("ask_user", END)  # æ­§ä¹‰å¤„ç†åç»“æŸ
workflow.add_edge("generate_answer", END)  # ç”Ÿæˆç­”æ¡ˆåç»“æŸ

# 8. ç¼–è¯‘å›¾
app: CompiledStateGraph = workflow.compile()

logger.info("âœ… LangGraph æ™ºèƒ½ä½“å·¥ä½œæµç¼–è¯‘å®Œæˆ")
logger.info(
    "ğŸ“Š å·¥ä½œæµç¨‹: æ„å›¾è·¯ç”± â†’ [å·¥å…·è°ƒç”¨ | æ£€ç´¢å¢å¼º â†’ æ–‡æ¡£æ£€ç´¢æµç¨‹] â†’ ç”Ÿæˆç­”æ¡ˆ/æ­§ä¹‰å¤„ç†"
)
