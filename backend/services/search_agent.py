import asyncio
import hashlib
import json
import re
from difflib import SequenceMatcher
from typing import Any, Dict, List, Optional, Set, TypedDict

from elasticsearch import AsyncElasticsearch
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, StateGraph
from langgraph.graph.state import CompiledStateGraph
from loguru import logger
from sqlalchemy import or_, select
from sqlalchemy.ext.asyncio import AsyncSession

from models.database_models import (
    Document,
    DocumentType,
    DocumentTypeField,
    TemplateDocumentMapping,
)
from services.intent_router import format_tool_result_as_answer, function_calling_router
from services.template_service import TemplateService
from utils.llm_client import get_llm_client

# å…¨å±€å˜é‡å­˜å‚¨graphçŠ¶æ€ï¼Œç”¨äºæ”¯æŒä¸­æ–­å’Œæ¢å¤
# æ³¨æ„: ç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redis ç­‰åˆ†å¸ƒå¼ç¼“å­˜æ›¿ä»£å†…å­˜å­˜å‚¨
graph_state_storage: Dict[str, Dict[str, Any]] = {}


# ==================== æ–‡æ¡£å»é‡å·¥å…·å‡½æ•° ====================


def normalize_text(text: str) -> str:
    """
    æ–‡æœ¬æ ‡å‡†åŒ–ï¼šå»é™¤HTML/Markdownæ ‡ç­¾ã€æ ‡ç‚¹ã€å¤šä½™ç©ºæ ¼ç­‰

    ç”¨äºåç»­çš„å“ˆå¸Œè®¡ç®—å’Œç›¸ä¼¼åº¦æ¯”å¯¹
    """
    if not text:
        return ""

    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r"<[^>]+>", "", text)
    # ç§»é™¤Markdownæ ‡é¢˜æ ‡è®°
    text = re.sub(r"^#+\s+", "", text, flags=re.MULTILINE)
    # ç§»é™¤Markdowné“¾æ¥
    text = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", text)
    # è½¬å°å†™
    text = text.lower()
    # æŠ˜å å¤šä½™ç©ºç™½ç¬¦
    text = re.sub(r"\s+", " ", text)
    # åªä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—
    text = re.sub(r"[^\w\u4e00-\u9fa5]+", "", text)

    return text.strip()


def compute_strong_hash(text: str) -> str:
    """
    è®¡ç®—æ–‡æœ¬çš„å¼ºå“ˆå¸Œå€¼ï¼ˆSHA256ï¼‰

    ç”¨äºæ£€æµ‹å®Œå…¨ç›¸åŒçš„æ–‡æ¡£
    """
    return hashlib.sha256(text.encode("utf-8")).hexdigest()


def compute_simhash(text: str, hashbits: int = 64) -> int:
    """
    è®¡ç®—SimHashï¼ˆå±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼‰

    ç”¨äºæ£€æµ‹é«˜åº¦ç›¸ä¼¼çš„æ–‡æ¡£
    ç®—æ³•ï¼šå¯¹æ–‡æœ¬åˆ†è¯åï¼Œä½¿ç”¨æ¯ä¸ªè¯çš„hashè¿›è¡ŒåŠ æƒæ±‚å’Œ
    """
    if not text:
        return 0

    # ç®€å•åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼ï¼‰
    tokens = text.split()
    if not tokens:
        return 0

    # åˆå§‹åŒ–ç‰¹å¾å‘é‡
    v = [0] * hashbits

    for token in tokens:
        # è®¡ç®—tokençš„hash
        h = int(hashlib.md5(token.encode("utf-8")).hexdigest(), 16)

        # å¯¹æ¯ä¸€ä½è¿›è¡ŒåŠ æƒ
        for i in range(hashbits):
            if h & (1 << i):
                v[i] += 1
            else:
                v[i] -= 1

    # ç”ŸæˆSimHashæŒ‡çº¹
    fingerprint = 0
    for i in range(hashbits):
        if v[i] > 0:
            fingerprint |= 1 << i

    return fingerprint


def hamming_distance(hash1: int, hash2: int) -> int:
    """
    è®¡ç®—ä¸¤ä¸ªSimHashçš„æ±‰æ˜è·ç¦»
    """
    x = hash1 ^ hash2
    distance = 0
    while x:
        distance += 1
        x &= x - 1  # æ¸…é™¤æœ€ä½ä½çš„1
    return distance


def compute_shingles(text: str, k: int = 5) -> Set[str]:
    """
    ç”Ÿæˆk-shinglesï¼ˆæ»‘åŠ¨çª—å£å­—ç¬¦ä¸²é›†åˆï¼‰

    ç”¨äºJaccardç›¸ä¼¼åº¦è®¡ç®—
    """
    if len(text) < k:
        return {text}

    shingles = set()
    for i in range(len(text) - k + 1):
        shingles.add(text[i : i + k])

    return shingles


def jaccard_similarity(set1: Set[str], set2: Set[str]) -> float:
    """
    è®¡ç®—Jaccardç›¸ä¼¼åº¦
    """
    if not set1 or not set2:
        return 0.0

    intersection = len(set1 & set2)
    union = len(set1 | set2)

    return intersection / union if union > 0 else 0.0


def should_remove_duplicate(
    doc_a: Dict[str, Any], doc_b: Dict[str, Any]
) -> Optional[int]:
    """
    åˆ¤æ–­ä¸¤ä¸ªæ–‡æ¡£æ˜¯å¦é‡å¤ï¼Œè¿”å›åº”è¯¥ç§»é™¤çš„æ–‡æ¡£ID

    è¿”å›å€¼ï¼š
    - None: ä¸é‡å¤
    - document_id: åº”è¯¥ç§»é™¤çš„æ–‡æ¡£IDï¼ˆä¿ç•™å†…å®¹æ›´é•¿ã€æ—¶é—´æ›´æ–°çš„ï¼‰

    Args:
        doc_a: æ–‡æ¡£Açš„dictï¼ŒåŒ…å« normalized, strong_hash, simhash, shingles, document_id, content
        doc_b: æ–‡æ¡£Bçš„dict
    """
    # é˜¶æ®µ1: å¼ºå“ˆå¸Œå®Œå…¨ç›¸åŒ
    if doc_a["strong_hash"] == doc_b["strong_hash"]:
        logger.debug(
            f"æ–‡æ¡£ {doc_a['document_id']} å’Œ {doc_b['document_id']} å¼ºå“ˆå¸Œç›¸åŒï¼ˆå®Œå…¨é‡å¤ï¼‰"
        )
        # ä¿ç•™å†…å®¹æ›´é•¿çš„
        if len(doc_a["content"]) < len(doc_b["content"]):
            return doc_a["document_id"]
        else:
            return doc_b["document_id"]

    # é˜¶æ®µ2: SimHashæ±‰æ˜è·ç¦»å¾ˆå°ï¼ˆé«˜åº¦ç›¸ä¼¼ï¼‰
    hamming_dist = hamming_distance(doc_a["simhash"], doc_b["simhash"])
    if hamming_dist <= 3:  # é˜ˆå€¼å¯è°ƒ
        logger.debug(
            f"æ–‡æ¡£ {doc_a['document_id']} å’Œ {doc_b['document_id']} SimHashè·ç¦»={hamming_dist}ï¼ˆé«˜åº¦ç›¸ä¼¼ï¼‰"
        )
        if len(doc_a["content"]) < len(doc_b["content"]):
            return doc_a["document_id"]
        else:
            return doc_b["document_id"]

    # é˜¶æ®µ3: Jaccardç›¸ä¼¼åº¦å¾ˆé«˜
    jac_sim = jaccard_similarity(doc_a["shingles"], doc_b["shingles"])
    if jac_sim > 0.75:  # é˜ˆå€¼å¯è°ƒ
        logger.debug(
            f"æ–‡æ¡£ {doc_a['document_id']} å’Œ {doc_b['document_id']} Jaccard={jac_sim:.3f}ï¼ˆå†…å®¹é‡å é«˜ï¼‰"
        )
        if len(doc_a["content"]) < len(doc_b["content"]):
            return doc_a["document_id"]
        else:
            return doc_b["document_id"]

    # é˜¶æ®µ4: åªå¯¹Jaccardåœ¨0.5-0.75ä¹‹é—´çš„åšç²¾ç»†difflibæ¯”å¯¹ï¼ˆé¿å…O(nÂ²)å¼€é”€ï¼‰
    if 0.5 < jac_sim <= 0.75:
        # difflibæ¯”å¯¹ï¼ˆè¾ƒæ…¢ï¼Œåªå¯¹å€™é€‰æ‰§è¡Œï¼‰
        ratio = SequenceMatcher(None, doc_a["normalized"], doc_b["normalized"]).ratio()
        if ratio > 0.80:  # é˜ˆå€¼å¯è°ƒ
            logger.debug(
                f"æ–‡æ¡£ {doc_a['document_id']} å’Œ {doc_b['document_id']} difflib={ratio:.3f}ï¼ˆç²¾ç»†æ¯”å¯¹é‡å¤ï¼‰"
            )
            if len(doc_a["content"]) < len(doc_b["content"]):
                return doc_a["document_id"]
            else:
                return doc_b["document_id"]

    return None


class RetrievalState(TypedDict):
    """
    ä¼˜åŒ–åçš„ RAG æ™ºèƒ½ä½“çŠ¶æ€æœº

    å·¥ä½œæµç¨‹:
    0. æ„å›¾è·¯ç”± -> LLM è‡ªä¸»è§„åˆ’æ•´ä¸ªæ‰§è¡Œæµç¨‹
    1. ESå…¨æ–‡æ£€ç´¢ -> åŸºäºå…³é”®è¯å¿«é€Ÿå¬å›å€™é€‰æ–‡æ¡£
    2. SQLç»“æ„åŒ–æ£€ç´¢ -> åŸºäºæ¨¡æ¿å±‚çº§æå–ç»“æ„åŒ–æ¡ä»¶
    3. ç»“æœèåˆ -> åˆå¹¶ä¸¤è·¯æ£€ç´¢ç»“æœ
    4. ç²¾ç»†åŒ–ç­›é€‰ -> åŸºäºæ–‡æ¡£ç±»å‹ç‰¹å®šå­—æ®µè¿›ä¸€æ­¥ç­›é€‰
    5. ç”Ÿæˆç­”æ¡ˆ -> RAGç”Ÿæˆæœ€ç»ˆå›ç­”

    æ³¨æ„: db å’Œ es_client ä¸åœ¨ state ä¸­ï¼Œé€šè¿‡ config æ³¨å…¥
    """

    # === å¿…éœ€è¾“å…¥ ===
    query: str  # ç”¨æˆ·æŸ¥è¯¢
    template_id: int  # æ¨¡æ¿ID
    session_id: str  # ä¼šè¯ID

    # === èŠ‚ç‚¹ 0 (æ„å›¾è·¯ç”±) äº§å‡º ===
    execution_plan: List[Dict[str, Any]]  # LLM è§„åˆ’çš„æ‰§è¡Œè®¡åˆ’
    reasoning: str  # LLM çš„æ¨ç†è¿‡ç¨‹
    tool_results: List[Dict[str, Any]]  # å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨
    need_retrieval: bool  # æ˜¯å¦éœ€è¦æ–‡æ¡£æ£€ç´¢

    # === èŠ‚ç‚¹ 1 (ESå…¨æ–‡æ£€ç´¢) äº§å‡º ===
    es_fulltext_results: List[Dict[str, Any]]  # ESå…¨æ–‡æ£€ç´¢çš„åˆæ­¥ç»“æœ
    es_document_ids: Set[int]  # ESå¬å›çš„æ–‡æ¡£IDé›†åˆ

    # === èŠ‚ç‚¹ 2 (SQLç»“æ„åŒ–æ£€ç´¢) äº§å‡º ===
    class_template_levels: Optional[List[Dict[str, Any]]]  # æ¨¡æ¿å±‚çº§å®šä¹‰
    category: str  # è¯†åˆ«å‡ºçš„æ–‡æ¡£ç±»åˆ«
    category_field_code: Optional[str]  # ç±»åˆ«å­—æ®µç¼–ç 
    sql_extracted_conditions: List[Dict[str, Any]]  # LLMæå–çš„ç»“æ„åŒ–æ¡ä»¶
    sql_document_ids: Set[int]  # SQLå¬å›çš„æ–‡æ¡£IDé›†åˆ

    # === èŠ‚ç‚¹ 3 (ç»“æœèåˆ) äº§å‡º ===
    merged_document_ids: List[int]  # èåˆåçš„æ–‡æ¡£IDåˆ—è¡¨(æŒ‰ç›¸å…³æ€§æ’åº)
    merged_documents: List[Document]  # èåˆåçš„æ–‡æ¡£å¯¹è±¡åˆ—è¡¨
    fusion_strategy: (
        str  # èåˆç­–ç•¥: 'intersection'(äº¤é›†), 'union'(å¹¶é›†), 'es_primary'(ESä¸ºä¸»)
    )

    # === èŠ‚ç‚¹ 4 (ç²¾ç»†åŒ–ç­›é€‰) äº§å‡º ===
    document_type_fields: List[DocumentTypeField]  # æ–‡æ¡£ç±»å‹ç‰¹å®šå­—æ®µ
    refined_conditions: Dict[str, Any]  # ç²¾ç»†åŒ–æŸ¥è¯¢æ¡ä»¶
    final_es_query: Optional[Dict[str, Any]]  # æœ€ç»ˆESæŸ¥è¯¢
    final_results: List[Dict[str, Any]]  # ç²¾ç»†åŒ–ç­›é€‰åçš„æœ€ç»ˆç»“æœ

    # === èŠ‚ç‚¹ 5 (æ­§ä¹‰å¤„ç†) äº§å‡º ===
    ambiguity_message: Optional[str]  # æ­§ä¹‰æç¤ºæ¶ˆæ¯

    # === èŠ‚ç‚¹ 6 (ç”Ÿæˆç­”æ¡ˆ) äº§å‡º ===
    answer: Optional[str]  # æœ€ç»ˆRAGç­”æ¡ˆ


# ==================== èŠ‚ç‚¹ 0: ä»»åŠ¡è§„åˆ’è·¯ç”± ====================
async def intent_routing(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 0: ä»»åŠ¡è§„åˆ’è·¯ç”±

    è®© LLM çœ‹åˆ°æ‰€æœ‰å·¥å…·ï¼Œè‡ªä¸»è§„åˆ’æ•´ä¸ªä»»åŠ¡çš„æ‰§è¡Œæµç¨‹ã€‚

    è¾“å‡º:
    - execution_plan: LLM è§„åˆ’çš„æ‰§è¡Œè®¡åˆ’
    - reasoning: LLM çš„æ¨ç†è¿‡ç¨‹
    - tool_results: å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨
    - need_retrieval: æ˜¯å¦éœ€è¦æ–‡æ¡£æ£€ç´¢
    """
    logger.info("========== èŠ‚ç‚¹ 0: ä»»åŠ¡è§„åˆ’è·¯ç”± ==========")

    # ä» config è·å– db
    db: AsyncSession = config.get("configurable", {}).get("db")  # type: ignore

    query = state["query"]
    template_id = state["template_id"]

    try:
        # è°ƒç”¨ Function Calling è·¯ç”±å™¨ï¼ˆç°åœ¨è¿”å›æ‰§è¡Œè®¡åˆ’ï¼‰
        routing_result = await function_calling_router(query, template_id, db)

        execution_plan = routing_result.get("execution_plan", [])
        reasoning = routing_result.get("reasoning", "")
        tool_results = routing_result.get("tool_results", [])
        need_retrieval = routing_result.get("need_retrieval", False)

        logger.info(f"ğŸ§  LLM è§„åˆ’ç»“æœ:")
        logger.info(f"   æ‰§è¡Œæ­¥éª¤: {len(execution_plan)}")
        logger.info(f"   å·¥å…·è°ƒç”¨: {len(tool_results)}")
        logger.info(f"   éœ€è¦æ£€ç´¢: {need_retrieval}")
        logger.info(f"   æ¨ç†è¿‡ç¨‹: {reasoning}")

        # æ‰“å°æ‰§è¡Œè®¡åˆ’
        for step in execution_plan:
            logger.info(
                f"   æ­¥éª¤ {step.get('step')}: {step.get('action')} - {step.get('description')}"
            )

        # æ›´æ–°çŠ¶æ€
        state["execution_plan"] = execution_plan
        state["reasoning"] = reasoning
        state["tool_results"] = tool_results
        state["need_retrieval"] = need_retrieval

        logger.info("âœ… ä»»åŠ¡è§„åˆ’å®Œæˆ")

    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡è§„åˆ’å¤±è´¥: {e}")
        import traceback

        logger.error(traceback.format_exc())
        # é»˜è®¤èµ°æ–‡æ¡£æ£€ç´¢
        state["execution_plan"] = [
            {"step": 1, "action": "document_retrieval", "description": "æ–‡æ¡£æ£€ç´¢"}
        ]
        state["reasoning"] = f"è§„åˆ’å¤±è´¥ï¼Œé™çº§åˆ°æ–‡æ¡£æ£€ç´¢: {str(e)}"
        state["tool_results"] = []
        state["need_retrieval"] = True

    return state


# ==================== èŠ‚ç‚¹: å·¥å…·è°ƒç”¨ç­”æ¡ˆç”Ÿæˆ ====================
async def generate_tool_answer(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    å·¥å…·è°ƒç”¨ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹

    å°†å¤šæ­¥éª¤å·¥å…·è°ƒç”¨ç»“æœæ ¼å¼åŒ–ä¸ºè‡ªç„¶è¯­è¨€ç­”æ¡ˆã€‚

    è¾“å‡º:
    - answer: æ ¼å¼åŒ–åçš„ç­”æ¡ˆï¼ˆå•çº¯å·¥å…·æŸ¥è¯¢ï¼‰
    - tool_answer_partial: éƒ¨åˆ†ç­”æ¡ˆï¼ˆç»„åˆæŸ¥è¯¢ï¼‰
    """
    logger.info("========== å·¥å…·è°ƒç”¨ç­”æ¡ˆç”Ÿæˆ ==========")

    # ä» config è·å– db
    db: AsyncSession = config.get("configurable", {}).get("db")  # type: ignore

    tool_results = state.get("tool_results", [])
    query = state["query"]
    need_retrieval = state.get("need_retrieval", False)
    execution_plan = state.get("execution_plan", [])

    try:
        # æ„å»ºå·¥å…·ç»“æœæ•°æ®
        combined_results = {
            "query": query,
            "execution_plan": execution_plan,
            "tool_results": tool_results,
        }

        # ä½¿ç”¨ LLM å°†å·¥å…·ç»“æœè½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€
        tool_answer = await format_tool_result_as_answer(combined_results, query, db)

        # å¦‚æœæ˜¯ç»„åˆæŸ¥è¯¢ï¼Œä¿å­˜å·¥å…·ç­”æ¡ˆï¼Œä¸ç›´æ¥è®¾ç½®ä¸ºæœ€ç»ˆç­”æ¡ˆ
        if need_retrieval:
            state["tool_answer_partial"] = tool_answer  # ä¿å­˜éƒ¨åˆ†ç­”æ¡ˆ
            logger.info(
                f"âœ… ç”Ÿæˆå·¥å…·è°ƒç”¨éƒ¨åˆ†ç­”æ¡ˆï¼Œç­‰å¾…ç»§ç»­æ£€ç´¢: {tool_answer[:100]}..."
            )
        else:
            state["answer"] = tool_answer  # ç›´æ¥è®¾ç½®ä¸ºæœ€ç»ˆç­”æ¡ˆ
            logger.info(f"âœ… ç”Ÿæˆå·¥å…·è°ƒç”¨æœ€ç»ˆç­”æ¡ˆ: {tool_answer[:100]}...")
    except Exception as e:
        logger.error(f"âŒ æ ¼å¼åŒ–å·¥å…·ç»“æœå¤±è´¥: {e}")
        import traceback

        logger.error(traceback.format_exc())
        # é™çº§å¤„ç†
        fallback_answer = (
            f"æŸ¥è¯¢ç»“æœï¼š\n{json.dumps(tool_results, ensure_ascii=False, indent=2)}"
        )
        if need_retrieval:
            state["tool_answer_partial"] = fallback_answer
        else:
            state["answer"] = fallback_answer

    return state


# ==================== èŠ‚ç‚¹ 1: ES å…¨æ–‡æ£€ç´¢ ====================
async def es_fulltext_retrieval(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 1: ES å…¨æ–‡æ£€ç´¢

    åŸºäºç”¨æˆ·æŸ¥è¯¢åœ¨ Elasticsearch ä¸­è¿›è¡Œå…¨æ–‡æ£€ç´¢,å¿«é€Ÿå¬å›å€™é€‰æ–‡æ¡£ã€‚
    è¿™æ˜¯ç¬¬ä¸€é˜¶æ®µçš„ç²—å¬å›,åˆ©ç”¨ ES çš„å…¨æ–‡æœç´¢èƒ½åŠ›ã€‚

    è¾“å‡º:
    - es_fulltext_results: ES æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
    - es_document_ids: æ–‡æ¡£ ID é›†åˆ
    """
    logger.info("========== èŠ‚ç‚¹ 1: ES å…¨æ–‡æ£€ç´¢ ==========")

    # ä» config è·å– es_client å’Œ es_index
    es_client: AsyncElasticsearch = config.get("configurable", {}).get(
        "es"
    )  # type: ignore
    es_index: str = config.get("configurable", {}).get(
        "es_index", "dochive_documents"
    )  # type: ignore

    query = state["query"]
    template_id = state["template_id"]

    # æ„é€  ES å…¨æ–‡æ£€ç´¢æŸ¥è¯¢
    es_query = {
        "query": {
            "bool": {
                "must": {
                    "multi_match": {
                        "query": query,
                        "fields": ["title^3", "content"],  # title æƒé‡æ›´é«˜
                        "type": "best_fields",
                        "fuzziness": "AUTO",  # æ”¯æŒæ¨¡ç³ŠåŒ¹é…
                    }
                },
                "filter": [{"term": {"template_id": template_id}}],  # é™å®šæ¨¡æ¿èŒƒå›´
            }
        },
        "size": 20,  # å¬å› Top 20
        "_source": ["document_id", "title", "content", "metadata"],
    }

    try:
        response = await es_client.search(index=es_index, body=es_query)

        hits = response.get("hits", {}).get("hits", [])
        state["es_fulltext_results"] = [hit["_source"] for hit in hits]
        state["es_document_ids"] = set(hit["_source"]["document_id"] for hit in hits)

        logger.info(f"âœ… ES å…¨æ–‡æ£€ç´¢å¬å› {len(hits)} ç¯‡æ–‡æ¡£")
        logger.info(f"   æ–‡æ¡£ ID: {list(state['es_document_ids'])}")

    except Exception as e:
        logger.error(f"âŒ ES å…¨æ–‡æ£€ç´¢å¤±è´¥: {e}")
        state["es_fulltext_results"] = []
        state["es_document_ids"] = set()

    return state


# ==================== èŠ‚ç‚¹ 2: SQL ç»“æ„åŒ–æ£€ç´¢ ====================
async def sql_structured_retrieval(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 2: SQL ç»“æ„åŒ–æ£€ç´¢

    åŸºäºæ¨¡æ¿å±‚çº§å®šä¹‰,ä½¿ç”¨ LLM æå–ç»“æ„åŒ–æŸ¥è¯¢æ¡ä»¶,
    åœ¨æ•°æ®åº“ä¸­è¿›è¡Œç²¾ç¡®çš„ç»“æ„åŒ–æ£€ç´¢ã€‚

    è¾“å‡º:
    - class_template_levels: æ¨¡æ¿å±‚çº§å®šä¹‰
    - category: æ–‡æ¡£ç±»åˆ«
    - sql_extracted_conditions: æå–çš„ç»“æ„åŒ–æ¡ä»¶
    - sql_document_ids: SQL å¬å›çš„æ–‡æ¡£ ID é›†åˆ
    """
    logger.info("========== èŠ‚ç‚¹ 2: SQL ç»“æ„åŒ–æ£€ç´¢ ==========")

    # ä» config è·å– db
    db: AsyncSession = config.get("configurable", {}).get("db")  # type: ignore

    # 1. è·å–æ¨¡æ¿å±‚çº§å®šä¹‰
    cls_template = await TemplateService.get_template(db, state["template_id"])

    if not cls_template:
        logger.warning("âš ï¸ æœªæ‰¾åˆ°æ¨¡æ¿,è·³è¿‡ SQL ç»“æ„åŒ–æ£€ç´¢")
        state["class_template_levels"] = []
        state["category"] = "*"
        state["sql_extracted_conditions"] = []
        state["sql_document_ids"] = set()
        return state

    # ä½¿ç”¨ property è·å–å±‚çº§å®šä¹‰ (è‡ªåŠ¨å¤„ç† JSON è½¬æ¢)
    cls_template_levels = cls_template.levels
    if not isinstance(cls_template_levels, list):
        logger.error("âŒ æ¨¡æ¿å±‚çº§å®šä¹‰æ ¼å¼é”™è¯¯")
        state["class_template_levels"] = []
        state["category"] = "*"
        state["sql_extracted_conditions"] = []
        state["sql_document_ids"] = set()
        return state

    state["class_template_levels"] = cls_template_levels

    # 2. æå–ç±»åˆ«å­—æ®µ
    type_code = ""
    for field in cls_template_levels:
        if field.get("is_doc_type", False):
            type_code = field.get("code", "")
            state["category_field_code"] = type_code
            break

    # 3. ä½¿ç”¨ LLM æå–ç»“æ„åŒ–æ¡ä»¶
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç»“æ„åŒ–æŸ¥è¯¢åŠ©æ‰‹ã€‚
ç”¨æˆ·ä¼šç»™å‡ºä¸€ä¸ªè‡ªç„¶è¯­è¨€æ£€ç´¢è¯·æ±‚,è¯·ä½ æ ¹æ®ä»¥ä¸‹å­—æ®µå®šä¹‰,æå–å‡ºç»“æ„åŒ–çš„æ£€ç´¢æ¡ä»¶ã€‚

å­—æ®µå®šä¹‰:
{json.dumps(cls_template_levels, ensure_ascii=False, indent=2)}

è¦æ±‚:
1. è¾“å‡º JSON å¯¹è±¡,æ ¼å¼: {{"conditions": [{{"code": "å­—æ®µç¼–ç ", "value": "æå–å€¼", "level": å±‚çº§}}], "category": "æ–‡æ¡£ç±»åˆ«"}}
2. å¦‚æœæ— æ³•ä»æŸ¥è¯¢ä¸­æ¨ç†å‡ºæŸä¸ªå­—æ®µ,value è®¾ä¸º "UNKNOWN"
3. category å­—æ®µåº”è¯¥æ˜¯ is_doc_type=true çš„å­—æ®µçš„å€¼
4. åªæå–ç”¨æˆ·æ˜ç¡®æåˆ°çš„ä¿¡æ¯,ä¸è¦çŒœæµ‹

ç”¨æˆ·æŸ¥è¯¢:
{state['query']}

è¯·ç›´æ¥è¾“å‡º JSON,ä¸è¦è§£é‡Šã€‚
    """

    try:
        llm_client = get_llm_client()
        llm_response = await llm_client.extract_json_response(prompt, db=db)
        logger.info(f"ğŸ¤– LLM æå–çš„ç»“æ„åŒ–æ¡ä»¶: {llm_response}")

        conditions = llm_response.get("conditions", [])
        state["category"] = llm_response.get("category", "*")
        state["sql_extracted_conditions"] = conditions

    except Exception as e:
        logger.error(f"âŒ LLM æå–ç»“æ„åŒ–æ¡ä»¶å¤±è´¥: {e}")
        state["category"] = "*"
        state["sql_extracted_conditions"] = []

    # 4. æ„é€  SQL æŸ¥è¯¢æ¡ä»¶
    conditions_clauses = []
    for cond in state["sql_extracted_conditions"]:
        value = cond.get("value")
        if value and value != "UNKNOWN":
            if isinstance(value, list):
                for v in value:
                    conditions_clauses.append(
                        TemplateDocumentMapping.class_code.like(f"%{v}%")
                    )
            else:
                conditions_clauses.append(
                    TemplateDocumentMapping.class_code.like(f"%{value}%")
                )

    # 5. æ‰§è¡Œ SQL æŸ¥è¯¢
    stmt = select(TemplateDocumentMapping.document_id).where(
        TemplateDocumentMapping.template_id == state["template_id"]
    )
    if conditions_clauses:
        stmt = stmt.where(or_(*conditions_clauses))

    try:
        result = await db.execute(stmt)
        document_ids = [row[0] for row in result.all()]
        state["sql_document_ids"] = set(document_ids)

        logger.info(f"âœ… SQL ç»“æ„åŒ–æ£€ç´¢å¬å› {len(document_ids)} ç¯‡æ–‡æ¡£")
        logger.info(f"   æ–‡æ¡£ ID: {list(state['sql_document_ids'])}")

    except Exception as e:
        logger.error(f"âŒ SQL æŸ¥è¯¢å¤±è´¥: {e}")
        state["sql_document_ids"] = set()

    return state


# ==================== èŠ‚ç‚¹ 3: ç»“æœèåˆ ====================
async def merge_retrieval_results(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 3: ç»“æœèåˆ

    å°† ES å…¨æ–‡æ£€ç´¢å’Œ SQL ç»“æ„åŒ–æ£€ç´¢çš„ç»“æœè¿›è¡Œèåˆ,
    é‡‡ç”¨æ™ºèƒ½ç­–ç•¥å†³å®šå¦‚ä½•åˆå¹¶ä¸¤è·¯å¬å›ç»“æœã€‚

    èåˆç­–ç•¥:
    1. 'intersection': å–äº¤é›† (ç²¾ç¡®åŒ¹é…)
    2. 'union': å–å¹¶é›† (å¹¿æ³›å¬å›)
    3. 'es_primary': ESä¸ºä¸»,SQLä¸ºè¾…åŠ©ç­›é€‰

    è¾“å‡º:
    - merged_document_ids: èåˆåçš„æ–‡æ¡£ ID åˆ—è¡¨
    - merged_documents: èåˆåçš„æ–‡æ¡£å¯¹è±¡
    - fusion_strategy: ä½¿ç”¨çš„èåˆç­–ç•¥
    """
    logger.info("========== èŠ‚ç‚¹ 3: ç»“æœèåˆ ==========")

    # ä» config è·å– db
    db: AsyncSession = config.get("configurable", {}).get("db")  # type: ignore

    es_ids = state.get("es_document_ids", set())
    sql_ids = state.get("sql_document_ids", set())

    logger.info(f"ğŸ“Š ES å¬å›: {len(es_ids)} ç¯‡, SQL å¬å›: {len(sql_ids)} ç¯‡")

    # å†³å®šèåˆç­–ç•¥
    if not es_ids and not sql_ids:
        # ä¸¤è·¯éƒ½æ²¡å¬å›
        logger.warning("âš ï¸ ES å’Œ SQL éƒ½æœªå¬å›ä»»ä½•æ–‡æ¡£")
        state["fusion_strategy"] = "none"
        state["merged_document_ids"] = []
        state["merged_documents"] = []
        return state

    elif not sql_ids:
        # åªæœ‰ ES å¬å›äº†
        logger.info("ğŸ“Œ ç­–ç•¥: ESä¸ºä¸» (SQLæœªå¬å›)")
        state["fusion_strategy"] = "es_only"
        merged_ids = list(es_ids)

    elif not es_ids:
        # åªæœ‰ SQL å¬å›äº†
        logger.info("ğŸ“Œ ç­–ç•¥: SQLä¸ºä¸» (ESæœªå¬å›)")
        state["fusion_strategy"] = "sql_only"
        merged_ids = list(sql_ids)

    else:
        # ä¸¤è·¯éƒ½å¬å›äº†,ä½¿ç”¨æ™ºèƒ½èåˆç­–ç•¥
        intersection = es_ids & sql_ids
        union = es_ids | sql_ids

        if len(intersection) >= 3:
            # äº¤é›†è¶³å¤Ÿå¤š,ä½¿ç”¨äº¤é›† (é«˜ç²¾åº¦)
            logger.info(f"ğŸ“Œ ç­–ç•¥: äº¤é›† (å…± {len(intersection)} ç¯‡æ–‡æ¡£)")
            state["fusion_strategy"] = "intersection"
            merged_ids = list(intersection)

        elif len(intersection) > 0:
            # äº¤é›†è¾ƒå°‘,ESä¸ºä¸»,SQLä¸ºè¾…
            logger.info(f"ğŸ“Œ ç­–ç•¥: ESä¸ºä¸»,SQLè¾…åŠ© (äº¤é›† {len(intersection)} ç¯‡)")
            state["fusion_strategy"] = "es_primary"
            # ES ç»“æœåœ¨å‰,äº¤é›†ä¼˜å…ˆ,ç„¶åæ˜¯ ES ç‹¬æœ‰
            merged_ids = list(intersection) + [
                id for id in es_ids if id not in intersection
            ]

        else:
            # æ²¡æœ‰äº¤é›†,å–å¹¶é›†
            logger.info(f"ğŸ“Œ ç­–ç•¥: å¹¶é›† (ES {len(es_ids)} + SQL {len(sql_ids)})")
            state["fusion_strategy"] = "union"
            merged_ids = list(es_ids) + [id for id in sql_ids if id not in es_ids]

    # é™åˆ¶ç»“æœæ•°é‡ (Top 10)
    merged_ids = merged_ids[:10]
    state["merged_document_ids"] = merged_ids

    # ä»æ•°æ®åº“åŠ è½½æ–‡æ¡£å¯¹è±¡
    if merged_ids:
        try:
            docs_result = await db.execute(
                select(Document).where(Document.id.in_(merged_ids))
            )
            docs = list(docs_result.scalars().all())

            # æŒ‰ merged_ids çš„é¡ºåºæ’åº
            docs_dict = {int(doc.id): doc for doc in docs}  # type: ignore
            state["merged_documents"] = [
                docs_dict[doc_id] for doc_id in merged_ids if doc_id in docs_dict
            ]

            logger.info(f"âœ… èåˆå®Œæˆ,æœ€ç»ˆä¿ç•™ {len(state['merged_documents'])} ç¯‡æ–‡æ¡£")

        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ–‡æ¡£å¯¹è±¡å¤±è´¥: {e}")
            state["merged_documents"] = []
    else:
        state["merged_documents"] = []

    return state


# ==================== èŠ‚ç‚¹ 4: ç²¾ç»†åŒ–ç­›é€‰ ====================
async def refined_filtering(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 4: ç²¾ç»†åŒ–ç­›é€‰

    åŸºäºæ–‡æ¡£ç±»å‹çš„ç‰¹å®šå­—æ®µ (DocumentTypeField),
    ä½¿ç”¨ LLM æå–æ›´ç²¾ç»†çš„æŸ¥è¯¢æ¡ä»¶,åœ¨ ES ä¸­è¿›è¡ŒäºŒæ¬¡ç­›é€‰ã€‚

    è¾“å‡º:
    - document_type_fields: æ–‡æ¡£ç±»å‹ç‰¹å®šå­—æ®µ
    - refined_conditions: ç²¾ç»†åŒ–æ¡ä»¶
    - final_es_query: æœ€ç»ˆ ES æŸ¥è¯¢
    - final_results: æœ€ç»ˆæ£€ç´¢ç»“æœ
    """
    logger.info("========== èŠ‚ç‚¹ 4: ç²¾ç»†åŒ–ç­›é€‰ ==========")

    # ä» config è·å– db å’Œ es_client
    db: AsyncSession = config.get("configurable", {}).get("db")  # type: ignore
    es_client: AsyncElasticsearch = config.get("configurable", {}).get(
        "es"
    )  # type: ignore

    # å¦‚æœæ²¡æœ‰èåˆç»“æœ,ç›´æ¥è·³è¿‡
    if not state.get("merged_documents"):
        logger.warning("âš ï¸ æ— èåˆç»“æœ,è·³è¿‡ç²¾ç»†åŒ–ç­›é€‰")
        state["document_type_fields"] = []
        state["refined_conditions"] = {}
        state["final_es_query"] = None
        state["final_results"] = []
        return state

    # å¦‚æœç±»åˆ«ä¸ºé€šé…ç¬¦,è·³è¿‡ç²¾ç»†åŒ–ç­›é€‰
    category = state.get("category", "*")
    if category == "*":
        logger.info("ğŸ“Œ ç±»åˆ«ä¸ºé€šé…ç¬¦,è·³è¿‡ç²¾ç»†åŒ–ç­›é€‰,ç›´æ¥ä½¿ç”¨èåˆç»“æœ")
        state["document_type_fields"] = []
        state["refined_conditions"] = {}
        state["final_es_query"] = None
        state["final_results"] = _convert_docs_to_results(state["merged_documents"])
        return state

    # 1. è·å– DocumentType å’Œ DocumentTypeField
    try:
        doc_types_result = await db.execute(
            select(DocumentType).where(
                DocumentType.template_id == state["template_id"],
                DocumentType.type_code == category,
            )
        )
        doc_types = doc_types_result.scalars().all()

        if not doc_types:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç±»åˆ« '{category}' çš„ DocumentType,è·³è¿‡ç²¾ç»†åŒ–ç­›é€‰")
            state["document_type_fields"] = []
            state["refined_conditions"] = {}
            state["final_results"] = _convert_docs_to_results(state["merged_documents"])
            return state

        document_type_fields_result = await db.execute(
            select(DocumentTypeField).where(
                DocumentTypeField.doc_type_id.in_([dt.id for dt in doc_types])
            )
        )
        document_type_fields = list(document_type_fields_result.scalars().all())
        state["document_type_fields"] = document_type_fields

        if not document_type_fields:
            logger.info("ğŸ“Œ è¯¥ç±»åˆ«æ— ç‰¹å®šå­—æ®µ,è·³è¿‡ç²¾ç»†åŒ–ç­›é€‰")
            state["refined_conditions"] = {}
            state["final_results"] = _convert_docs_to_results(state["merged_documents"])
            return state

    except Exception as e:
        logger.error(f"âŒ è·å–æ–‡æ¡£ç±»å‹å­—æ®µå¤±è´¥: {e}")
        state["document_type_fields"] = []
        state["refined_conditions"] = {}
        state["final_results"] = _convert_docs_to_results(state["merged_documents"])
        return state

    # 2. ä½¿ç”¨ LLM æå–ç²¾ç»†åŒ–æ¡ä»¶
    field_definitions = ""
    field_map = {}  # å­—æ®µå -> å­—æ®µç±»å‹

    for f in document_type_fields:
        field_definitions += (
            f"- {f.field_name}: {f.description} (ç±»å‹: {f.field_type})\n"
        )
        field_map[f.field_name] = f.field_type

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç²¾ç»†åŒ–æŸ¥è¯¢åŠ©æ‰‹ã€‚
ç”¨æˆ·æ­£åœ¨æŸ¥è¯¢ç±»åˆ«ä¸º '{category}' çš„æ–‡æ¡£,è¯¥ç±»åˆ«æœ‰ä»¥ä¸‹ç‰¹å®šå­—æ®µ:

{field_definitions}

è¯·æ ¹æ®ç”¨æˆ·æŸ¥è¯¢æå–è¿™äº›å­—æ®µçš„å…·ä½“å€¼:

è¦æ±‚:
1. è¾“å‡º JSON å¯¹è±¡: {{"conditions": {{"å­—æ®µå": "å€¼"}}, "missing_fields": ["ç¼ºå¤±å­—æ®µ"]}}
2. åªæå–ç”¨æˆ·æ˜ç¡®æåˆ°çš„å­—æ®µå€¼
3. missing_fields åˆ—å‡ºå¯¹ç²¾ç¡®æ£€ç´¢æœ‰å¸®åŠ©ä½†ç”¨æˆ·æœªæä¾›çš„å­—æ®µ

ç”¨æˆ·æŸ¥è¯¢:
{state['query']}

è¯·ç›´æ¥è¾“å‡º JSON,ä¸è¦è§£é‡Šã€‚
    """
    llm_client = get_llm_client()

    try:
        llm_response = await llm_client.extract_json_response(prompt, db=db)
        logger.info(f"ğŸ¤– LLM æå–çš„ç²¾ç»†åŒ–æ¡ä»¶: {llm_response}")

        conditions = llm_response.get("conditions", {})
        missing_fields = llm_response.get("missing_fields", [])

        state["refined_conditions"] = conditions

        # 3. æ£€æŸ¥æ­§ä¹‰
        if not conditions and missing_fields:
            missing_str = "ã€".join(missing_fields)
            state["ambiguity_message"] = (
                f"æ‚¨çš„é—®é¢˜ä¼¼ä¹æœ‰äº›å®½æ³›ã€‚ä¸ºäº†æ›´ç²¾ç¡®åœ°æŸ¥æ‰¾,èƒ½å¦æä¾›: {missing_str}?"
            )
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ­§ä¹‰,å»ºè®®è¡¥å……: {missing_str}")
            state["final_results"] = _convert_docs_to_results(state["merged_documents"])
            return state

    except Exception as e:
        logger.error(f"âŒ LLM æå–ç²¾ç»†åŒ–æ¡ä»¶å¤±è´¥: {e}")
        state["refined_conditions"] = {}
        state["final_results"] = _convert_docs_to_results(state["merged_documents"])
        return state

    # 4. æ„é€ ç²¾ç»†åŒ– ES æŸ¥è¯¢
    if not state["refined_conditions"]:
        logger.info("ğŸ“Œ æ— ç²¾ç»†åŒ–æ¡ä»¶,ç›´æ¥ä½¿ç”¨èåˆç»“æœ")
        state["final_es_query"] = None
        state["final_results"] = _convert_docs_to_results(state["merged_documents"])
        return state

    # åªåœ¨èåˆåçš„æ–‡æ¡£ä¸­ç­›é€‰
    merged_doc_ids = state["merged_document_ids"]

    must_clauses = []
    for field_name, value in state["refined_conditions"].items():
        if not value or value == "UNKNOWN":
            continue

        field_type = field_map.get(field_name, "text")

        # æ ¹æ®å­—æ®µç±»å‹æ„é€ æŸ¥è¯¢
        if field_type in ["text", "textarea"]:
            must_clauses.append({"match": {f"metadata.{field_name}": value}})
        elif field_type == "number":
            must_clauses.append({"term": {f"metadata.{field_name}": value}})
        elif field_type == "date":
            must_clauses.append({"range": {f"metadata.{field_name}": {"gte": value}}})
        else:
            must_clauses.append({"term": {f"metadata.{field_name}": value}})

    final_es_query = {
        "query": {
            "bool": {
                "must": must_clauses,
                "filter": [{"terms": {"document_id": merged_doc_ids}}],
            }
        },
        "size": 5,
        "_source": ["document_id", "title", "content", "metadata"],
    }

    state["final_es_query"] = final_es_query

    # 5. æ‰§è¡Œç²¾ç»†åŒ– ES æŸ¥è¯¢
    try:
        # ä» config è·å– es_index
        es_index: str = config.get("configurable", {}).get(
            "es_index", "dochive_documents"
        )  # type: ignore
        response = await es_client.search(index=es_index, body=final_es_query)

        hits = response.get("hits", {}).get("hits", [])
        state["final_results"] = [hit["_source"] for hit in hits]

        logger.info(f"âœ… ç²¾ç»†åŒ–ç­›é€‰å®Œæˆ,ä¿ç•™ {len(hits)} ç¯‡æ–‡æ¡£")

    except Exception as e:
        logger.error(f"âŒ ç²¾ç»†åŒ– ES æŸ¥è¯¢å¤±è´¥: {e}")
        # é™çº§: ä½¿ç”¨èåˆç»“æœ
        state["final_results"] = _convert_docs_to_results(state["merged_documents"])

    return state


def _convert_docs_to_results(documents: List[Document]) -> List[Dict[str, Any]]:
    """
    è¾…åŠ©å‡½æ•°: å°† Document å¯¹è±¡åˆ—è¡¨è½¬æ¢ä¸ºç»“æœå­—å…¸åˆ—è¡¨
    """
    results = []
    for doc in documents:
        results.append(
            {
                "document_id": doc.id,
                "title": doc.title,
                "content": doc.content_text or "",
                "metadata": doc.doc_metadata or {},
            }
        )
    return results


# ==================== èŠ‚ç‚¹ 4.5: æ–‡æ¡£å»é‡ ====================
async def deduplicate_documents(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 4.5: æ–‡æ¡£å»é‡

    åŸºäºä¸‰é˜¶æ®µå»é‡ç®—æ³•ï¼Œç§»é™¤é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„æ–‡æ¡£ã€‚

    ä¸‰é˜¶æ®µç­–ç•¥ï¼š
    1. å¼ºå“ˆå¸Œ (SHA256): æ£€æµ‹å®Œå…¨ç›¸åŒçš„æ–‡æ¡£
    2. SimHash + æ±‰æ˜è·ç¦»: æ£€æµ‹é«˜åº¦ç›¸ä¼¼çš„æ–‡æ¡£
    3. Jaccardç›¸ä¼¼åº¦ + difflib: æ£€æµ‹â€œç²˜è´´å¼é‡å¤â€ï¼ˆä¸€ä¸ªæ–‡æ¡£è¢«ç²˜è´´åˆ°å¦ä¸€ä¸ªæ–‡æ¡£ä¸­ï¼‰

    è¾“å‡º:
    - final_results: å»é‡åçš„æ–‡æ¡£åˆ—è¡¨
    """
    logger.info("========== èŠ‚ç‚¹ 4.5: æ–‡æ¡£å»é‡ ===========")

    results = state.get("final_results", [])

    if not results or len(results) <= 1:
        logger.info("æ–‡æ¡£æ•°é‡â‰¤ 1ï¼Œæ— éœ€å»é‡")
        return state

    logger.info(f"å¼€å§‹å»é‡ï¼ŒåŸå§‹æ–‡æ¡£æ•°: {len(results)}")

    # é˜¶æ®µ 0: é¢„å¤„ç† - ä¸ºæ¯ä¸ªæ–‡æ¡£è®¡ç®—ç‰¹å¾
    doc_features = []
    for doc in results:
        content = doc.get("content", "")
        if not content:
            continue

        # æ ‡å‡†åŒ–æ–‡æœ¬
        normalized = normalize_text(content)
        if not normalized:
            continue

        doc_features.append(
            {
                "document_id": doc.get("document_id"),
                "title": doc.get("title", ""),
                "content": content,
                "normalized": normalized,
                "strong_hash": compute_strong_hash(normalized),
                "simhash": compute_simhash(normalized),
                "shingles": compute_shingles(normalized, k=5),
                "original_index": results.index(doc),
            }
        )

    logger.info(f"é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ–‡æ¡£æ•°: {len(doc_features)}")

    if len(doc_features) <= 1:
        return state

    # é˜¶æ®µ 1-4: è¿›è¡Œå»é‡æ¯”å¯¹
    removed_ids = set()

    for i in range(len(doc_features)):
        if doc_features[i]["document_id"] in removed_ids:
            continue

        for j in range(i + 1, len(doc_features)):
            if doc_features[j]["document_id"] in removed_ids:
                continue

            # åˆ¤æ–­æ˜¯å¦é‡å¤
            remove_id = should_remove_duplicate(doc_features[i], doc_features[j])

            if remove_id is not None:
                removed_ids.add(remove_id)
                logger.info(f"âœ–ï¸  æ–‡æ¡£ {remove_id} è¢«æ ‡è®°ä¸ºé‡å¤ï¼Œå°†è¢«ç§»é™¤")

    # è¿‡æ»¤é‡å¤æ–‡æ¡£
    deduplicated_results = [
        doc for doc in results if doc.get("document_id") not in removed_ids
    ]

    logger.info(
        f"âœ… å»é‡å®Œæˆ: åŸå§‹ {len(results)} ç¯‡ â†’ å»é‡å {len(deduplicated_results)} ç¯‡ ï¼ˆç§»é™¤ {len(removed_ids)} ç¯‡ï¼‰"
    )

    # æ›´æ–°çŠ¶æ€
    state["final_results"] = deduplicated_results

    return state


# ==================== èŠ‚ç‚¹ 5: æ­§ä¹‰å¤„ç† ====================
async def handle_ambiguity(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 5: æ­§ä¹‰å¤„ç†

    å¦‚æœæŸ¥è¯¢æœ‰æ­§ä¹‰,æš‚åœæµç¨‹å¹¶å‘ç”¨æˆ·æé—®ã€‚
    è¿™æ˜¯ä¸€ä¸ªç»ˆç«¯èŠ‚ç‚¹ã€‚
    """
    logger.info("========== èŠ‚ç‚¹ 5: æ­§ä¹‰å¤„ç† ==========")
    logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ­§ä¹‰: {state.get('ambiguity_message')}")

    # çŠ¶æ€å·²åŒ…å« ambiguity_message,ç›´æ¥è¿”å›
    # å‰ç«¯ä¼šå±•ç¤ºè¿™ä¸ªæ¶ˆæ¯å¹¶ç­‰å¾…ç”¨æˆ·è¾“å…¥
    return state


# ==================== èŠ‚ç‚¹ 6: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ ====================
async def generate_answer(
    state: RetrievalState, config: RunnableConfig
) -> RetrievalState:
    """
    èŠ‚ç‚¹ 6: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

    åŸºäºæœ€ç»ˆæ£€ç´¢ç»“æœ,ä½¿ç”¨ RAG ç”Ÿæˆç”¨æˆ·é—®é¢˜çš„ç­”æ¡ˆã€‚

    è¾“å‡º:
    - answer: æœ€ç»ˆç­”æ¡ˆ
    """
    logger.info("========== èŠ‚ç‚¹ 6: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ ==========")

    # ä» config è·å– db
    db: AsyncSession = config.get("configurable", {}).get("db")  # type: ignore

    query = state["query"]
    results = state.get("final_results", [])

    if not results:
        logger.warning("âš ï¸ æ— æœ€ç»ˆæ£€ç´¢ç»“æœ,æ— æ³•ç”Ÿæˆç­”æ¡ˆ")
        state["answer"] = (
            "æŠ±æ­‰,æˆ‘æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„æ–‡æ¡£ã€‚å»ºè®®æ‚¨:\n1. å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯\n2. ç®€åŒ–æˆ–æ˜ç¡®æ‚¨çš„é—®é¢˜\n3. æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²ä¸Šä¼ åˆ°ç³»ç»Ÿä¸­"
        )
        return state

    # æ„é€  RAG ä¸Šä¸‹æ–‡
    context_parts = []
    for i, doc in enumerate(results[:5], 1):  # æœ€å¤šä½¿ç”¨ 5 ç¯‡æ–‡æ¡£
        doc_context = f"ã€æ–‡æ¡£ {i}ã€‘\n"
        doc_context += f"æ ‡é¢˜: {doc.get('title', 'æœªçŸ¥æ ‡é¢˜')}\n"

        # æ™ºèƒ½æˆªå–å†…å®¹ç‰‡æ®µ (ä¼˜å…ˆåŒ…å«æŸ¥è¯¢å…³é”®è¯é™„è¿‘çš„å†…å®¹)
        content = doc.get("content", "")
        if len(content) > 800:
            # ç®€å•æˆªå–ç­–ç•¥,å®é™…å¯ä»¥ç”¨æ›´æ™ºèƒ½çš„æ–¹æ³•
            content = content[:800] + "..."
        doc_context += f"å†…å®¹: {content}\n"

        # æ·»åŠ å…ƒæ•°æ®
        metadata = doc.get("metadata", {})
        if metadata:
            doc_context += f"å…ƒæ•°æ®: {json.dumps(metadata, ensure_ascii=False)}\n"

        context_parts.append(doc_context)

    context_str = "\n".join(context_parts)

    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨çš„éƒ¨åˆ†ç­”æ¡ˆï¼ˆç»„åˆæŸ¥è¯¢ï¼‰
    tool_answer_partial = state.get("tool_answer_partial")

    # æ„é€  RAG prompt
    if tool_answer_partial:
        # ç»„åˆæŸ¥è¯¢ï¼šéœ€è¦åˆå¹¶å·¥å…·ç­”æ¡ˆå’Œæ–‡æ¡£ç­”æ¡ˆ
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚ç”¨æˆ·çš„é—®é¢˜åŒ…å«å¤šä¸ªå­ä»»åŠ¡ï¼Œä½ å·²ç»é€šè¿‡å·¥å…·è°ƒç”¨å›ç­”äº†éƒ¨åˆ†é—®é¢˜ï¼Œç°åœ¨éœ€è¦ç»“åˆæ–‡æ¡£å†…å®¹å›ç­”å‰©ä½™éƒ¨åˆ†ã€‚

ã€å·¥å…·è°ƒç”¨ç»“æœï¼ˆå·²å›ç­”çš„éƒ¨åˆ†ï¼‰ã€‘
{tool_answer_partial}

ã€æ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‘
{context_str}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
1. å…ˆç®€è¦åˆ—å‡ºå·¥å…·è°ƒç”¨å·²ç»å›ç­”çš„éƒ¨åˆ†
2. å†åŸºäºæ–‡æ¡£å†…å®¹å›ç­”å‰©ä½™é—®é¢˜
3. å¦‚æœéœ€è¦å¼•ç”¨æ–‡æ¡£ï¼Œè¯·ä½¿ç”¨ "æ ¹æ®æ–‡æ¡£X" çš„æ ¼å¼
4. å›ç­”è¦å…¨é¢ã€å‡†ç¡®ã€æ¸…æ™°
5. å¦‚æœæ–‡æ¡£å†…å®¹ä¸å‰©ä½™é—®é¢˜æ— å…³ï¼Œè¯·å¦‚å®è¯´æ˜

è¯·å¼€å§‹å›ç­”ï¼š
    """
    else:
        # å•çº¯æ–‡æ¡£æ£€ç´¢
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€æ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‘
{context_str}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
1. åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹è¿›è¡Œå›ç­”,å¦‚æœæ–‡æ¡£ä¸­æœ‰æ˜ç¡®ç­”æ¡ˆè¯·ç›´æ¥å¼•ç”¨
2. å¦‚æœéœ€è¦å¼•ç”¨æ–‡æ¡£,è¯·ä½¿ç”¨ "æ ¹æ®æ–‡æ¡£X" çš„æ ¼å¼
3. å¦‚æœæ–‡æ¡£ä¿¡æ¯ä¸è¶³ä»¥å®Œæ•´å›ç­”é—®é¢˜,è¯·æ˜ç¡®è¯´æ˜å“ªäº›éƒ¨åˆ†æ— æ³•ç¡®å®š
4. å›ç­”è¦ç®€æ´ã€å‡†ç¡®ã€ä¸“ä¸š
5. å¦‚æœæ–‡æ¡£å†…å®¹ä¸é—®é¢˜æ— å…³,è¯·å¦‚å®è¯´æ˜

è¯·å¼€å§‹å›ç­”:
    """
    llm_client = get_llm_client()

    try:
        answer = await llm_client.chat_completion(prompt, db=db)
        state["answer"] = answer
        logger.info(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(answer)} å­—ç¬¦)")

    except Exception as e:
        logger.error(f"âŒ LLM ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
        state["answer"] = "æŠ±æ­‰,æˆ‘åœ¨ç”Ÿæˆç­”æ¡ˆæ—¶é‡åˆ°äº†æŠ€æœ¯é—®é¢˜,è¯·ç¨åé‡è¯•ã€‚"

    return state


# ==================== å†³ç­–å‡½æ•° ====================
def should_use_tool(state: RetrievalState) -> str:
    """
    å†³ç­–å‡½æ•°: æ ¹æ®æ‰§è¡Œè®¡åˆ’åˆ¤æ–­è·¯ç”±

    Returns:
        'tool_answer': æ‰§è¡Œè®¡åˆ’åŒ…å«å·¥å…·è°ƒç”¨ï¼ˆä¹‹åå¯èƒ½è¿˜éœ€è¦æ£€ç´¢ï¼‰
        'retrieval': æ‰§è¡Œè®¡åˆ’åªæœ‰æ–‡æ¡£æ£€ç´¢
    """
    execution_plan = state.get("execution_plan", [])

    # æ£€æŸ¥æ‰§è¡Œè®¡åˆ’ä¸­æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
    has_tool_call = any(step.get("action") == "tool_call" for step in execution_plan)

    if has_tool_call:
        logger.info("ğŸ”§ å†³ç­–: æ‰§è¡Œè®¡åˆ’åŒ…å«å·¥å…·è°ƒç”¨ -> tool_answer")
        return "tool_answer"
    else:
        logger.info("ğŸ” å†³ç­–: ä»…æ–‡æ¡£æ£€ç´¢ -> retrieval")
        return "retrieval"


def should_ask_user(state: RetrievalState) -> str:
    """
    å†³ç­–å‡½æ•°: åˆ¤æ–­æ˜¯å¦éœ€è¦å‘ç”¨æˆ·æé—®æ¾„æ¸…

    Returns:
        'ask_user': æœ‰æ­§ä¹‰,éœ€è¦ç”¨æˆ·æ¾„æ¸…
        'generate_answer': æ— æ­§ä¹‰,ç›´æ¥ç”Ÿæˆç­”æ¡ˆ
    """
    if state.get("ambiguity_message"):
        logger.info("ğŸ”€ å†³ç­–: æœ‰æ­§ä¹‰ -> ask_user")
        return "ask_user"
    else:
        logger.info("ğŸ”€ å†³ç­–: æ— æ­§ä¹‰ -> generate_answer")
        return "generate_answer"


# ==================== æ„å»º LangGraph å·¥ä½œæµ ====================
"""
ä¼˜åŒ–åçš„å·¥ä½œæµç¨‹ï¼š

0. ä»»åŠ¡è§„åˆ’ (intent_routing) - LLM è§„åˆ’æ‰§è¡Œæ­¥éª¤
   â†“
   [å†³ç­–] æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨? (should_use_tool)
   â”œâ”€ tool_answer: åŒ…å«å·¥å…·è°ƒç”¨ â†’ æ‰§è¡Œå·¥å…· â†’ [å†³ç­–] æ˜¯å¦éœ€è¦æ£€ç´¢?
   â”‚   â”œâ”€ éœ€è¦ â†’ ç»§ç»­æ–‡æ¡£æ£€ç´¢
   â”‚   â””â”€ ä¸éœ€è¦ â†’ END
   â””â”€ retrieval: åªæœ‰æ–‡æ¡£æ£€ç´¢ â†’ ESå…¨æ–‡æ£€ç´¢...
       â†“
1. ESå…¨æ–‡æ£€ç´¢ (es_fulltext_retrieval)
   â†“
2. SQLç»“æ„åŒ–æ£€ç´¢ (sql_structured_retrieval) 
   â†“
3. ç»“æœèåˆ (merge_retrieval_results)
   â†“
4. ç²¾ç»†åŒ–ç­›é€‰ (refined_filtering)
   â†“
4.5. æ–‡æ¡£å»é‡ (deduplicate_documents)
   â†“
5. [å†³ç­–] æ˜¯å¦æœ‰æ­§ä¹‰? (should_ask_user)
   â”œâ”€ ask_user: æ­§ä¹‰å¤„ç† (handle_ambiguity) â†’ END
   â””â”€ generate_answer: ç”Ÿæˆç­”æ¡ˆ (generate_answer) â†’ END
"""

# 1. åˆå§‹åŒ– StateGraph
workflow = StateGraph(RetrievalState)

# 2. æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
workflow.add_node("intent_routing", intent_routing)  # èŠ‚ç‚¹0: ä»»åŠ¡è§„åˆ’
workflow.add_node("tool_answer", generate_tool_answer)  # å·¥å…·è°ƒç”¨ç­”æ¡ˆç”Ÿæˆ
workflow.add_node("es_fulltext", es_fulltext_retrieval)  # èŠ‚ç‚¹1: ESå…¨æ–‡æ£€ç´¢
workflow.add_node("sql_structured", sql_structured_retrieval)  # èŠ‚ç‚¹2: SQLç»“æ„åŒ–æ£€ç´¢
workflow.add_node("merge_results", merge_retrieval_results)  # èŠ‚ç‚¹3: ç»“æœèåˆ
workflow.add_node("refined_filter", refined_filtering)  # èŠ‚ç‚¹4: ç²¾ç»†åŒ–ç­›é€‰
workflow.add_node("deduplicate", deduplicate_documents)  # èŠ‚ç‚¹4.5: æ–‡æ¡£å»é‡
workflow.add_node("ask_user", handle_ambiguity)  # èŠ‚ç‚¹5a: æ­§ä¹‰å¤„ç†
workflow.add_node("generate_answer", generate_answer)  # èŠ‚ç‚¹5b: ç”Ÿæˆç­”æ¡ˆ

# 3. è®¾ç½®å›¾çš„å…¥å£ç‚¹ï¼ˆä»ä»»åŠ¡è§„åˆ’å¼€å§‹ï¼‰
workflow.set_entry_point("intent_routing")

# 4. æ·»åŠ æ¡ä»¶è¾¹ï¼šä»»åŠ¡è§„åˆ’åå†³å®šèµ°å‘
workflow.add_conditional_edges(
    "intent_routing",  # æºèŠ‚ç‚¹
    should_use_tool,  # å†³ç­–å‡½æ•°
    {
        "tool_answer": "tool_answer",  # åŒ…å«å·¥å…·è°ƒç”¨ â†’ å·¥å…·ç­”æ¡ˆç”Ÿæˆ â†’ [å†³ç­–]
        "retrieval": "es_fulltext",  # ä»…æ–‡æ¡£æ£€ç´¢ â†’ ESå…¨æ–‡æ£€ç´¢
    },
)

# 4.5 å·¥å…·è°ƒç”¨åï¼Œæ ¹æ®æ‰§è¡Œè®¡åˆ’å†³å®šæ˜¯å¦ç»§ç»­æ£€ç´¢
workflow.add_conditional_edges(
    "tool_answer",  # æºèŠ‚ç‚¹
    lambda state: "continue_retrieval" if state.get("need_retrieval", False) else "end",
    {
        "continue_retrieval": "es_fulltext",  # ç»§ç»­æ–‡æ¡£æ£€ç´¢
        "end": END,  # ç›´æ¥ç»“æŸ
    },
)

# 5. æ·»åŠ æ–‡æ¡£æ£€ç´¢æµç¨‹çš„çº¿æ€§è¾¹
workflow.add_edge("es_fulltext", "sql_structured")  # ESå…¨æ–‡ â†’ SQLç»“æ„åŒ–
workflow.add_edge("sql_structured", "merge_results")  # SQLç»“æ„åŒ– â†’ ç»“æœèåˆ
workflow.add_edge("merge_results", "refined_filter")  # ç»“æœèåˆ â†’ ç²¾ç»†åŒ–ç­›é€‰
workflow.add_edge("refined_filter", "deduplicate")  # ç²¾ç»†åŒ–ç­›é€‰ â†’ æ–‡æ¡£å»é‡

# 6. æ·»åŠ æ¡ä»¶è¾¹ï¼šåœ¨å»é‡åï¼Œåˆ¤æ–­æ˜¯å¦æœ‰æ­§ä¹‰
workflow.add_conditional_edges(
    "deduplicate",  # æºèŠ‚ç‚¹
    should_ask_user,  # å†³ç­–å‡½æ•°
    {
        "ask_user": "ask_user",  # æœ‰æ­§ä¹‰ â†’ å‘ç”¨æˆ·æé—®
        "generate_answer": "generate_answer",  # æ— æ­§ä¹‰ â†’ ç”Ÿæˆç­”æ¡ˆ
    },
)

# 7. è®¾ç½®å›¾çš„ç»ˆç‚¹ï¼ˆæ³¨æ„ï¼štool_answer ç°åœ¨æœ‰æ¡ä»¶è¾¹ï¼Œä¸å†ç›´æ¥åˆ° ENDï¼‰
workflow.add_edge("ask_user", END)  # æ­§ä¹‰å¤„ç†åç»“æŸ
workflow.add_edge("generate_answer", END)  # ç”Ÿæˆç­”æ¡ˆåç»“æŸ

# 8. ç¼–è¯‘å›¾
app: CompiledStateGraph = workflow.compile()

logger.info("âœ… LangGraph æ™ºèƒ½ä½“å·¥ä½œæµç¼–è¯‘å®Œæˆ")
logger.info("ğŸ“Š å·¥ä½œæµç¨‹: æ„å›¾è·¯ç”± â†’ [å·¥å…·è°ƒç”¨ | æ–‡æ¡£æ£€ç´¢æµç¨‹] â†’ ç”Ÿæˆç­”æ¡ˆ/æ­§ä¹‰å¤„ç†")
